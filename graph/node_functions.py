"""
é‡å­æ™ºèƒ½åŒ–åŠŸèƒ½ç‚¹ä¼°ç®—ç³»ç»Ÿ - LangGraphèŠ‚ç‚¹å‡½æ•°

å®ç°å·¥ä½œæµä¸­çš„å„ä¸ªèŠ‚ç‚¹é€»è¾‘
"""

import asyncio
import time
from typing import Dict, Any, Optional, List
import logging
from datetime import datetime

from graph.state_definitions import (
    WorkflowGraphState, WorkflowState, ProcessingStage,
    transition_state, update_execution_log, is_retry_needed,
    increment_retry_attempt
)
from models.project_models import EstimationStrategy, ProcessDetails
from agents.standards.standard_recommender import StandardRecommenderAgent
from agents.analysis.requirement_parser import RequirementParserAgent
from agents.analysis.process_identifier import ProcessIdentifierAgent
from agents.analysis.comparison_analyzer import ComparisonAnalyzerAgent
from agents.standards.nesma.function_classifier import NESMAFunctionClassifierAgent
from agents.standards.nesma.complexity_calculator import NESMAComplexityCalculatorAgent
from agents.standards.nesma.ufp_calculator import NESMAUFPCalculatorAgent
from agents.standards.cosmic.functional_user_agent import COSMICFunctionalUserAgent
from agents.standards.cosmic.boundary_analyzer import COSMICBoundaryAnalyzerAgent
from agents.standards.cosmic.data_movement_classifier import COSMICDataMovementClassifierAgent
from agents.standards.cosmic.cfp_calculator import COSMICCFPCalculatorAgent
from agents.knowledge.validator import ValidatorAgent
from agents.output.report_generator import ReportGeneratorAgent

# æ·»åŠ å¿…è¦çš„å¯¼å…¥
from agents.knowledge.rule_retriever import create_rule_retriever_agent
from knowledge_base.vector_stores.pgvector_store import create_pgvector_store
from knowledge_base.embeddings.embedding_models import get_embedding_model
from config.settings import get_settings

logger = logging.getLogger(__name__)

# å…¨å±€æ™ºèƒ½ä½“ç¼“å­˜
_cached_agents = {}

async def get_or_create_rule_retriever():
    """è·å–æˆ–åˆ›å»ºrule_retrieveræ™ºèƒ½ä½“ï¼ˆå…¨å±€ç¼“å­˜ï¼‰"""
    if 'rule_retriever' not in _cached_agents:
        try:
            logger.info("ğŸ”§ åˆå§‹åŒ–rule_retrieveræ™ºèƒ½ä½“...")
            
            # è·å–é…ç½®
            settings = get_settings()
            
            # åˆ›å»ºembeddings
            embeddings = get_embedding_model()
            
            # åˆ›å»ºå‘é‡å­˜å‚¨
            vector_store = await create_pgvector_store(embeddings)
            
            # åˆ›å»ºLLM
            from config.settings import get_llm
            llm = get_llm()
            
            # åˆ›å»ºrule_retriever
            rule_retriever = await create_rule_retriever_agent(
                llm=llm,
                embeddings=embeddings,
                vector_store=vector_store
            )
            
            _cached_agents['rule_retriever'] = rule_retriever
            logger.info("âœ… rule_retrieveræ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ rule_retrieveråˆå§‹åŒ–å¤±è´¥: {e}")
            _cached_agents['rule_retriever'] = None
    
    return _cached_agents.get('rule_retriever')


# åˆå§‹åŒ–èŠ‚ç‚¹
async def start_workflow_node(state: WorkflowGraphState) -> WorkflowGraphState:
    """å·¥ä½œæµå¯åŠ¨èŠ‚ç‚¹"""
    
    logger.info(f"ğŸš€ å¯åŠ¨å·¥ä½œæµï¼Œä¼šè¯ID: {state.session_id}")
    
    try:
        # è®°å½•å¯åŠ¨æ—¥å¿—
        state = update_execution_log(
            state,
            agent_id="workflow_manager",
            action="start_workflow",
            status="success",
            details={
                "project_name": state.project_info.name,
                "user_requirements_length": len(state.user_requirements)
            }
        )
        
        # è½¬æ¢çŠ¶æ€
        state = transition_state(
            state,
            WorkflowState.REQUIREMENT_INPUT_RECEIVED,
            "å·¥ä½œæµå¯åŠ¨å®Œæˆ"
        )
        
        logger.info("âœ… å·¥ä½œæµå¯åŠ¨æˆåŠŸ")
        return state
        
    except Exception as e:
        logger.error(f"âŒ å·¥ä½œæµå¯åŠ¨å¤±è´¥: {str(e)}")
        state.error_message = str(e)
        return transition_state(state, WorkflowState.ERROR_ENCOUNTERED, str(e))


# æ ‡å‡†æ¨èèŠ‚ç‚¹
async def recommend_standard_node(state: WorkflowGraphState) -> WorkflowGraphState:
    """æ ‡å‡†æ¨èèŠ‚ç‚¹"""
    
    logger.info("ğŸ¯ å¼€å§‹æ ‡å‡†æ¨èåˆ†æ...")
    
    start_time = time.time()
    
    try:
        # åˆ›å»ºæ ‡å‡†æ¨èæ™ºèƒ½ä½“
        recommender = StandardRecommenderAgent()
        
        # æ‰§è¡Œæ ‡å‡†æ¨è
        recommendation = await recommender.recommend_standards(
            project_info=state.project_info
        )
        
        # æ›´æ–°çŠ¶æ€ - ä½¿ç”¨æ­£ç¡®çš„StandardRecommendationå±æ€§
        state.standard_recommendation = recommendation
        
        # æ ¹æ®æ¨èçš„æ ‡å‡†è®¾ç½®ç­–ç•¥
        if recommendation.recommended_standard == "NESMA":
            state.selected_strategy = EstimationStrategy.NESMA_ONLY
        elif recommendation.recommended_standard == "COSMIC":
            state.selected_strategy = EstimationStrategy.COSMIC_ONLY
        elif recommendation.recommended_standard == "NESMA+COSMIC":
            state.selected_strategy = EstimationStrategy.DUAL_PARALLEL
        else:
            # é»˜è®¤ç­–ç•¥
            state.selected_strategy = EstimationStrategy.NESMA_ONLY
        
        processing_time = time.time() - start_time
        
        # è®°å½•æ‰§è¡Œæ—¥å¿—
        state = update_execution_log(
            state,
            agent_id="standard_recommender",
            action="recommend_standard",
            status="success",
            duration_ms=int(processing_time * 1000),
            details={
                "recommended_standard": recommendation.recommended_standard,
                "confidence_score": recommendation.confidence_score
            }
        )
        
        # è½¬æ¢çŠ¶æ€
        state = transition_state(
            state,
            WorkflowState.STANDARD_RECOMMENDATION_READY,
            f"æ ‡å‡†æ¨èå®Œæˆ: {recommendation.recommended_standard}"
        )
        
        logger.info(f"âœ… æ ‡å‡†æ¨èå®Œæˆ: {recommendation.recommended_standard}")
        return state
        
    except Exception as e:
        error_msg = f"æ ‡å‡†æ¨èå¤±è´¥: {str(e)}"
        logger.error(f"âŒ {error_msg}")
        
        # è¯¦ç»†çš„é”™è¯¯å¤„ç†
        if "'StandardRecommenderAgent' object has no attribute 'recommend_estimation_standard'" in str(e):
            error_msg = "æ–¹æ³•åä¸åŒ¹é…ï¼šä½¿ç”¨äº†é”™è¯¯çš„æ–¹æ³•å 'recommend_estimation_standard'ï¼Œåº”è¯¥æ˜¯ 'recommend_standards'"
        elif "recommend_standards" in str(e):
            error_msg = f"æ ‡å‡†æ¨èæ™ºèƒ½ä½“æ‰§è¡Œå¤±è´¥: {str(e)}"
        
        state.error_message = error_msg
        
        # è®°å½•é”™è¯¯æ—¥å¿—
        state = update_execution_log(
            state,
            agent_id="standard_recommender",
            action="recommend_standard",
            status="error",
            details={"error": error_msg, "exception_type": type(e).__name__}
        )
        
        return transition_state(state, WorkflowState.ERROR_ENCOUNTERED, error_msg)


# éœ€æ±‚è§£æèŠ‚ç‚¹
async def parse_requirements_node(state: WorkflowGraphState) -> WorkflowGraphState:
    """éœ€æ±‚è§£æèŠ‚ç‚¹"""
    
    logger.info("ğŸ“ å¼€å§‹éœ€æ±‚è§£æ...")
    
    start_time = time.time()
    
    try:
        # åˆ›å»ºéœ€æ±‚è§£ææ™ºèƒ½ä½“
        parser = RequirementParserAgent()
        
        # æ£€æŸ¥ç”¨æˆ·éœ€æ±‚æ˜¯å¦å­˜åœ¨
        user_requirements = state.user_requirements
        if not user_requirements:
            raise ValueError("ç”¨æˆ·éœ€æ±‚ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œéœ€æ±‚è§£æ")
        
        # æ‰§è¡Œéœ€æ±‚è§£æ
        analysis_result = await parser.parse_requirements(user_requirements)
        
        # ğŸ”¥ å¢å¼ºéœ€æ±‚è§£æç»“æœçš„æ£€æŸ¥å’Œå¤„ç†
        if not analysis_result:
            raise ValueError("éœ€æ±‚è§£æè¿”å›ç©ºç»“æœ")
        
        # æ›´æ–°çŠ¶æ€ - åˆ›å»ºRequirementAnalysiså®ä¾‹
        from graph.state_definitions import RequirementAnalysis
        
        # å¤„ç†business_processeså­—æ®µ - ç¡®ä¿æ˜¯ProcessDetailså¯¹è±¡åˆ—è¡¨
        business_processes = []
        raw_business_processes = analysis_result.get("business_processes", [])
        
        for i, process_data in enumerate(raw_business_processes):
            if isinstance(process_data, dict):
                # å¦‚æœæ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸ºProcessDetailså¯¹è±¡
                process = ProcessDetails(
                    id=f"process_{i+1}",
                    name=process_data.get("æµç¨‹åç§°", process_data.get("name", f"æµç¨‹{i+1}")),
                    description=process_data.get("æµç¨‹æè¿°", process_data.get("description", "")),
                    data_groups=process_data.get("æ¶‰åŠçš„æ•°æ®ç»„", process_data.get("data_groups", [])),
                    dependencies=process_data.get("ä¾èµ–å…³ç³»", process_data.get("dependencies", [])),
                    inputs=process_data.get("inputs", []),
                    outputs=process_data.get("outputs", []),
                    business_rules=process_data.get("business_rules", []),
                    complexity_indicators=process_data.get("complexity_indicators", {}),
                    metadata=process_data
                )
                business_processes.append(process)
            elif isinstance(process_data, ProcessDetails):
                # å¦‚æœå·²ç»æ˜¯ProcessDetailså¯¹è±¡ï¼Œç›´æ¥ä½¿ç”¨
                business_processes.append(process_data)
            else:
                # å…¶ä»–æƒ…å†µï¼Œåˆ›å»ºé»˜è®¤å¯¹è±¡
                process = ProcessDetails(
                    id=f"process_{i+1}",
                    name=f"æµç¨‹{i+1}",
                    description=str(process_data),
                    data_groups=[],
                    dependencies=[],
                    inputs=[],
                    outputs=[],
                    business_rules=[],
                    complexity_indicators={},
                    metadata={"source": "fallback", "original_data": process_data}
                )
                business_processes.append(process)
        
        state.requirement_analysis = RequirementAnalysis(
            functional_modules=analysis_result.get("functional_modules", []),
            business_entities=analysis_result.get("business_entities", {}),
            business_processes=business_processes,
            data_groups=analysis_result.get("data_groups", []),
            analysis_confidence=analysis_result.get("parsing_confidence", 0.0),
            parsing_issues=analysis_result.get("parsing_issues", []),
            analysis_metadata=analysis_result.get("metadata", {}),
            original_analysis=analysis_result,
            # è®¾ç½®åŸºæœ¬å­—æ®µ
            functional_requirements=analysis_result.get("functional_requirements", []),
            non_functional_requirements=analysis_result.get("non_functional_requirements", []),
            complexity_factors=analysis_result.get("complexity_factors", []),
            risk_factors=analysis_result.get("risk_factors", []),
            estimated_effort=analysis_result.get("estimated_effort")
        )
        
        # ğŸ”¥ éªŒè¯éœ€æ±‚è§£æç»“æœçš„ç»“æ„
        logger.info(f"ğŸ“‹ éœ€æ±‚è§£æç»“æœæ£€æŸ¥:")
        logger.info(f"  - ç»“æœç±»å‹: {type(state.requirement_analysis)}")
        logger.info(f"  - ç»“æœé”®: {list(state.requirement_analysis.__dict__.keys()) if hasattr(state.requirement_analysis, '__dict__') else 'Not a model'}")
        
        if hasattr(state.requirement_analysis, 'functional_modules'):
            functional_modules = state.requirement_analysis.functional_modules
            business_processes = state.requirement_analysis.business_processes
            logger.info(f"  - åŠŸèƒ½æ¨¡å—æ•°é‡: {len(functional_modules)}")
            logger.info(f"  - ä¸šåŠ¡æµç¨‹æ•°é‡: {len(business_processes)}")
        
        processing_time = time.time() - start_time
        
        # è®°å½•æ‰§è¡Œæ—¥å¿—
        state = update_execution_log(
            state,
            agent_id="requirement_parser",
            action="parse_requirements",
            status="success",
            duration_ms=int(processing_time * 1000),
            details={
                "functional_modules_count": len(state.requirement_analysis.functional_modules),
                "analysis_confidence": state.requirement_analysis.analysis_confidence,
                "has_business_processes": len(state.requirement_analysis.business_processes) > 0
            }
        )
        
        # è½¬æ¢çŠ¶æ€
        state = transition_state(
            state,
            WorkflowState.REQUIREMENT_PARSING_COMPLETED,
            "éœ€æ±‚è§£æå®Œæˆ"
        )
        
        logger.info("âœ… éœ€æ±‚è§£æå®Œæˆ")
        return state
        
    except Exception as e:
        error_msg = f"éœ€æ±‚è§£æå¤±è´¥: {str(e)}"
        logger.error(f"âŒ {error_msg}")
        
        # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
        import traceback
        logger.error(f"âŒ éœ€æ±‚è§£æå¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
        
        state.error_message = error_msg
        
        # è®°å½•é”™è¯¯æ—¥å¿—
        state = update_execution_log(
            state,
            agent_id="requirement_parser",
            action="parse_requirements",
            status="error",
            details={"error": error_msg, "exception_type": type(e).__name__}
        )
        
        return transition_state(state, WorkflowState.ERROR_ENCOUNTERED, error_msg)


# æµç¨‹è¯†åˆ«èŠ‚ç‚¹
async def identify_processes_node(state: WorkflowGraphState) -> WorkflowGraphState:
    """æµç¨‹è¯†åˆ«èŠ‚ç‚¹"""
    
    logger.info("ğŸ” å¼€å§‹æµç¨‹è¯†åˆ«...")
    
    start_time = time.time()
    
    try:
        # ğŸ”¥ å¢å¼ºçŠ¶æ€æ£€æŸ¥
        requirement_analysis = state.requirement_analysis
        if not requirement_analysis:
            error_msg = "éœ€æ±‚è§£æç»“æœä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œæµç¨‹è¯†åˆ«"
            logger.error(f"âŒ {error_msg}")
            
            # å°è¯•ä»çŠ¶æ€ä¸­è·å–æ›´å¤šä¿¡æ¯ç”¨äºè°ƒè¯•
            logger.error(f"ğŸ” çŠ¶æ€æ£€æŸ¥è¯¦æƒ…:")
            logger.error(f"  - å½“å‰çŠ¶æ€: {state.current_state}")
            logger.error(f"  - ç”¨æˆ·éœ€æ±‚æ˜¯å¦å­˜åœ¨: {bool(state.user_requirements)}")
            logger.error(f"  - é¡¹ç›®ä¿¡æ¯æ˜¯å¦å­˜åœ¨: {bool(state.project_info)}")
            logger.error(f"  - éœ€æ±‚åˆ†æç»“æœæ˜¯å¦å­˜åœ¨: {bool(state.requirement_analysis)}")
            
            raise ValueError(error_msg)
        
        # ğŸ”¥ éªŒè¯éœ€æ±‚è§£æç»“æœçš„ç»“æ„
        logger.info(f"ğŸ“‹ éœ€æ±‚è§£æç»“æœæ£€æŸ¥:")
        logger.info(f"  - ç»“æœç±»å‹: {type(requirement_analysis)}")
        
        if hasattr(requirement_analysis, 'functional_modules'):
            functional_modules = requirement_analysis.functional_modules
            business_processes = requirement_analysis.business_processes
            logger.info(f"  - åŠŸèƒ½æ¨¡å—æ•°é‡: {len(functional_modules)}")
            logger.info(f"  - ä¸šåŠ¡æµç¨‹æ•°é‡: {len(business_processes)}")
        
        # åˆ›å»ºæµç¨‹è¯†åˆ«æ™ºèƒ½ä½“
        identifier = ProcessIdentifierAgent()
        
        # æ£€æŸ¥é¡¹ç›®ä¿¡æ¯
        project_info = state.project_info
        if not project_info:
            raise ValueError("é¡¹ç›®ä¿¡æ¯ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œæµç¨‹è¯†åˆ«")
        
        # æ‰§è¡Œæµç¨‹è¯†åˆ«
        processes = await identifier.identify_processes(
            requirement_analysis=requirement_analysis,
            project_info=project_info
        )
        
        # ğŸ”¥ éªŒè¯æµç¨‹è¯†åˆ«ç»“æœ
        if not processes:
            logger.warning("âš ï¸ æµç¨‹è¯†åˆ«è¿”å›ç©ºç»“æœï¼Œåˆ›å»ºé»˜è®¤æµç¨‹")
            # åˆ›å»ºä¸€ä¸ªé»˜è®¤æµç¨‹ä»¥é¿å…å®Œå…¨å¤±è´¥
            from models.project_models import ProcessDetails
            processes = [
                ProcessDetails(
                    id="default_process_001",
                    name="é»˜è®¤åŠŸèƒ½æµç¨‹",
                    description="åŸºäºéœ€æ±‚è§£æç»“æœç”Ÿæˆçš„é»˜è®¤åŠŸèƒ½æµç¨‹",
                    data_groups=["é»˜è®¤æ•°æ®ç»„"],
                    dependencies=[],
                    inputs=[],
                    outputs=[],
                    business_rules=[],
                    complexity_indicators={},
                    metadata={"source": "fallback", "confidence": 0.3}
                )
            ]
        
        # æ›´æ–°çŠ¶æ€ - ç›´æ¥ä½¿ç”¨è¿”å›çš„ProcessDetailså¯¹è±¡
        state.identified_processes = processes
        
        processing_time = time.time() - start_time
        
        # è®°å½•æ‰§è¡Œæ—¥å¿—
        state = update_execution_log(
            state,
            agent_id="process_identifier",
            action="identify_processes",
            status="success",
            duration_ms=int(processing_time * 1000),
            details={
                "processes_count": len(processes),
                "process_names": [p.name for p in processes]
            }
        )
        
        # è½¬æ¢çŠ¶æ€
        state = transition_state(
            state,
            WorkflowState.PROCESSES_IDENTIFIED,
            f"è¯†åˆ«å‡º {len(processes)} ä¸ªåŠŸèƒ½æµç¨‹"
        )
        
        logger.info(f"âœ… æµç¨‹è¯†åˆ«å®Œæˆï¼Œè¯†åˆ«å‡º {len(processes)} ä¸ªæµç¨‹")
        return state
        
    except Exception as e:
        error_msg = f"æµç¨‹è¯†åˆ«å¤±è´¥: {str(e)}"
        logger.error(f"âŒ {error_msg}")
        
        # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
        import traceback
        logger.error(f"âŒ æµç¨‹è¯†åˆ«å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
        
        # è¯¦ç»†çš„é”™è¯¯å¤„ç†
        if "Can't instantiate abstract class ProcessIdentifierAgent" in str(e):
            error_msg = "ProcessIdentifierAgentç±»æ²¡æœ‰å®ç°æŠ½è±¡æ–¹æ³• '_execute_task'"
        elif "_execute_task" in str(e):
            error_msg = f"æµç¨‹è¯†åˆ«æ™ºèƒ½ä½“æ–¹æ³•å®ç°é”™è¯¯: {str(e)}"
        elif "éœ€æ±‚è§£æç»“æœä¸å­˜åœ¨" in str(e):
            # è¿™ç§æƒ…å†µåœ¨ä¸Šé¢å·²ç»å¤„ç†è¿‡äº†ï¼Œç›´æ¥ä¼ é€’é”™è¯¯æ¶ˆæ¯
            pass
        
        state.error_message = error_msg
        
        # è®°å½•é”™è¯¯æ—¥å¿—
        state = update_execution_log(
            state,
            agent_id="process_identifier",
            action="identify_processes", 
            status="error",
            details={"error": error_msg, "exception_type": type(e).__name__}
        )
        
        return transition_state(state, WorkflowState.ERROR_ENCOUNTERED, error_msg)


# NESMAåŠŸèƒ½åˆ†ç±»èŠ‚ç‚¹
async def nesma_classify_functions_node(state: WorkflowGraphState) -> WorkflowGraphState:
    """NESMAåŠŸèƒ½åˆ†ç±»èŠ‚ç‚¹"""
    
    logger.info("ğŸ¯ å¼€å§‹NESMAåŠŸèƒ½åˆ†ç±»...")
    
    start_time = time.time()
    
    try:
        # è·å–æˆ–åˆ›å»ºrule_retriever
        rule_retriever = await get_or_create_rule_retriever()
        
        # åˆ›å»ºLLM
        from config.settings import get_llm
        llm = get_llm()
        
        # åˆ›å»ºNESMAåŠŸèƒ½åˆ†ç±»æ™ºèƒ½ä½“ï¼Œä¼ å…¥rule_retriever
        classifier = NESMAFunctionClassifierAgent(
            rule_retriever=rule_retriever,
            llm=llm
        )
        
        logger.info(f"ğŸ“‹ åŠŸèƒ½åˆ†ç±»å™¨åˆå§‹åŒ–å®Œæˆï¼Œrule_retriever: {'âœ…' if rule_retriever else 'âŒ'}")
        
        classifications = []
        processes = state.identified_processes or []
        
        for process in processes:
            # ä¸ºæ¯ä¸ªæµç¨‹è¿›è¡ŒåŠŸèƒ½åˆ†ç±»
            classification = await classifier.classify_function(
                function_description=process.description,
                process_details=process
            )
            classifications.append(classification)
        
        # åˆå§‹åŒ–NESMAç»“æœ
        if not state.nesma_results:
            state.nesma_results = {
                "function_classifications": [],
                "complexity_results": [],
                "total_ufp": 0,
                "ufp_breakdown": {},
                "estimation_confidence": 0.0,
                "classification_issues": [],
                "calculation_metadata": {}
            }
        
        state.nesma_results["function_classifications"] = classifications
        
        processing_time = time.time() - start_time
        
        # è®°å½•æ‰§è¡Œæ—¥å¿—
        state = update_execution_log(
            state,
            agent_id="nesma_classifier",
            action="classify_functions",
            status="success",
            duration_ms=int(processing_time * 1000),
            details={
                "classifications_count": len(classifications),
                "function_types": [c.function_type for c in classifications]
            }
        )
        
        # è½¬æ¢çŠ¶æ€
        state = transition_state(
            state,
            WorkflowState.NESMA_CLASSIFICATION_COMPLETED,
            f"NESMAåŠŸèƒ½åˆ†ç±»å®Œæˆï¼Œåˆ†ç±» {len(classifications)} ä¸ªåŠŸèƒ½"
        )
        
        logger.info(f"âœ… NESMAåŠŸèƒ½åˆ†ç±»å®Œæˆ")
        return state
        
    except Exception as e:
        error_msg = f"NESMAåŠŸèƒ½åˆ†ç±»å¤±è´¥: {str(e)}"
        logger.error(f"âŒ {error_msg}")
        
        # è¯¦ç»†çš„é”™è¯¯å¤„ç†
        if "'nesma_results'" in str(e):
            error_msg = "çŠ¶æ€å­—æ®µ 'nesma_results' è®¿é—®é”™è¯¯ï¼Œå¯èƒ½æœªæ­£ç¡®åˆå§‹åŒ–"
        elif "classify_function" in str(e):
            error_msg = f"NESMAåŠŸèƒ½åˆ†ç±»æ–¹æ³•æ‰§è¡Œé”™è¯¯: {str(e)}"
        
        state.error_message = error_msg
        
        # è®°å½•é”™è¯¯æ—¥å¿—
        state = update_execution_log(
            state,
            agent_id="nesma_classifier",
            action="classify_functions",
            status="error",
            details={"error": error_msg, "exception_type": type(e).__name__}
        )
        
        return transition_state(state, WorkflowState.ERROR_ENCOUNTERED, error_msg)


# NESMAå¤æ‚åº¦è®¡ç®—èŠ‚ç‚¹
async def nesma_calculate_complexity_node(state: WorkflowGraphState) -> WorkflowGraphState:
    """NESMAå¤æ‚åº¦è®¡ç®—èŠ‚ç‚¹"""
    
    logger.info("âš™ï¸ å¼€å§‹NESMAå¤æ‚åº¦è®¡ç®—...")
    
    start_time = time.time()
    
    try:
        # è·å–æˆ–åˆ›å»ºrule_retriever
        rule_retriever = await get_or_create_rule_retriever()
        
        # åˆ›å»ºLLM
        from config.settings import get_llm
        llm = get_llm()
        
        # åˆ›å»ºNESMAå¤æ‚åº¦è®¡ç®—æ™ºèƒ½ä½“ï¼Œä¼ å…¥rule_retriever
        calculator = NESMAComplexityCalculatorAgent(
            rule_retriever=rule_retriever,
            llm=llm
        )
        
        logger.info(f"ğŸ“‹ å¤æ‚åº¦è®¡ç®—å™¨åˆå§‹åŒ–å®Œæˆï¼Œrule_retriever: {'âœ…' if rule_retriever else 'âŒ'}")
        
        complexity_results = []
        classifications = state.nesma_results["function_classifications"]
        
        for classification in classifications:
            # ä¸ºæ¯ä¸ªåˆ†ç±»è®¡ç®—å¤æ‚åº¦
            complexity = await calculator.execute(
                task_name="calculate_complexity",
                inputs={
                    "classification": classification,
                    "function_description": classification.function_description,
                    "detailed_data": {}
                }
            )
            complexity_results.append(complexity)
        
        state.nesma_results["complexity_results"] = complexity_results
        
        processing_time = time.time() - start_time
        
        # è®°å½•æ‰§è¡Œæ—¥å¿—
        state = update_execution_log(
            state,
            agent_id="nesma_complexity_calculator",
            action="calculate_complexity",
            status="success",
            duration_ms=int(processing_time * 1000),
            details={
                "complexity_results_count": len(complexity_results)
            }
        )
        
        # è½¬æ¢çŠ¶æ€
        state = transition_state(
            state,
            WorkflowState.NESMA_COMPLEXITY_COMPLETED,
            "NESMAå¤æ‚åº¦è®¡ç®—å®Œæˆ"
        )
        
        logger.info("âœ… NESMAå¤æ‚åº¦è®¡ç®—å®Œæˆ")
        return state
        
    except Exception as e:
        logger.error(f"âŒ NESMAå¤æ‚åº¦è®¡ç®—å¤±è´¥: {str(e)}")
        state.error_message = str(e)
        return transition_state(state, WorkflowState.ERROR_ENCOUNTERED, str(e))


# NESMA UFPè®¡ç®—èŠ‚ç‚¹
async def nesma_calculate_ufp_node(state: WorkflowGraphState) -> WorkflowGraphState:
    """NESMA UFPè®¡ç®—èŠ‚ç‚¹"""
    
    logger.info("ğŸ§® å¼€å§‹NESMA UFPè®¡ç®—...")
    
    start_time = time.time()
    
    try:
        # åˆ›å»ºNESMA UFPè®¡ç®—æ™ºèƒ½ä½“
        calculator = NESMAUFPCalculatorAgent()
        
        # è·å–åˆ†ç±»å’Œå¤æ‚åº¦ç»“æœï¼Œç¡®ä¿æ•°æ®å­˜åœ¨
        function_classifications = state.nesma_results.function_classifications or []
        complexity_results = state.nesma_results.complexity_results or []
        
        # å¦‚æœå¤æ‚åº¦ç»“æœä¸è¶³ï¼Œç”¨é»˜è®¤å€¼è¡¥å……
        if len(complexity_results) < len(function_classifications):
            complexity_results.extend([{"complexity": "AVERAGE"}] * (len(function_classifications) - len(complexity_results)))
        
        # å°†å­—å…¸è½¬æ¢ä¸ºNESMAComplexityCalculationå¯¹è±¡
        from models.nesma_models import NESMAComplexityCalculation, NESMAComplexityLevel, NESMAFunctionType
        
        # å¤æ‚åº¦æ˜ å°„å­—å…¸
        complexity_mapping = {
            "Low": NESMAComplexityLevel.LOW,
            "Average": NESMAComplexityLevel.AVERAGE,
            "High": NESMAComplexityLevel.HIGH
        }
        
        complexity_objects = []
        for i, (c, cr) in enumerate(zip(function_classifications, complexity_results)):
            try:
                # è·å–å¤æ‚åº¦å­—ç¬¦ä¸²å¹¶è½¬æ¢ä¸ºæšä¸¾
                complexity_str = cr.get("complexity", "Low")
                complexity_enum = complexity_mapping.get(complexity_str, NESMAComplexityLevel.LOW)
                
                # åˆ›å»ºNESMAComplexityCalculationå¯¹è±¡
                complexity_obj = NESMAComplexityCalculation(
                    function_id=cr.get("function_id", f"func_{i}"),
                    function_type=NESMAFunctionType(c.function_type.value if hasattr(c, 'function_type') else c.get("function_type", "EI")),
                    complexity=complexity_enum,
                    det_count=cr.get("det_count", 0),
                    ret_count=cr.get("ret_count", 0),
                    calculation_details=cr.get("calculation_details", {}),
                    complexity_matrix_used=cr.get("complexity_matrix_used", "default"),
                    calculation_steps=cr.get("calculation_steps", [])
                )
                complexity_objects.append(complexity_obj)
            except Exception as e:
                logger.warning(f"âš ï¸ è½¬æ¢å¤æ‚åº¦å¯¹è±¡å¤±è´¥ {i}: {e}, ä½¿ç”¨é»˜è®¤å€¼")
                # åˆ›å»ºé»˜è®¤å¯¹è±¡
                complexity_obj = NESMAComplexityCalculation(
                    function_id=f"func_{i}",
                    function_type=NESMAFunctionType.EI,
                    complexity=NESMAComplexityLevel.LOW,
                    det_count=1,
                    ret_count=1,
                    calculation_details={},
                    complexity_matrix_used="default",
                    calculation_steps=[]
                )
                complexity_objects.append(complexity_obj)
        
        # æ‰§è¡ŒUFPè®¡ç®—
        ufp_result = await calculator.execute(
            task_name="calculate_ufp",
            inputs={
                "complexity_results": complexity_objects,
                "project_info": state.project_info
            }
        )
        
        # æ›´æ–°NESMAç»“æœ
        state.nesma_results["total_ufp"] = ufp_result.get("total_ufp", 0)
        state.nesma_results["ufp_breakdown"] = ufp_result.get("ufp_breakdown", {})
        state.nesma_results["estimation_confidence"] = ufp_result.get("confidence_score", 0.0)
        state.nesma_results["calculation_metadata"] = ufp_result.get("calculation_details", {})
        
        processing_time = time.time() - start_time
        
        # è®°å½•æ‰§è¡Œæ—¥å¿—
        state = update_execution_log(
            state,
            agent_id="nesma_ufp_calculator",
            action="calculate_ufp",
            status="success",
            duration_ms=int(processing_time * 1000),
            details={
                "total_ufp": ufp_result.get("total_ufp", 0),
                "confidence_score": ufp_result.get("confidence_score", 0.0)
            }
        )
        
        # è½¬æ¢çŠ¶æ€
        state = transition_state(
            state,
            WorkflowState.NESMA_CALCULATION_COMPLETED,
            f"NESMA UFPè®¡ç®—å®Œæˆ: {ufp_result.get('total_ufp', 0)} UFP"
        )
        
        logger.info(f"âœ… NESMA UFPè®¡ç®—å®Œæˆ: {ufp_result.get('total_ufp', 0)} UFP")
        return state
        
    except Exception as e:
        logger.error(f"âŒ NESMA UFPè®¡ç®—å¤±è´¥: {str(e)}")
        state.error_message = str(e)
        return transition_state(state, WorkflowState.ERROR_ENCOUNTERED, str(e))


# COSMICåŠŸèƒ½ç”¨æˆ·è¯†åˆ«èŠ‚ç‚¹
async def cosmic_identify_users_node(state: WorkflowGraphState) -> WorkflowGraphState:
    """COSMICåŠŸèƒ½ç”¨æˆ·è¯†åˆ«èŠ‚ç‚¹"""
    
    logger.info("ğŸ‘¥ å¼€å§‹COSMICåŠŸèƒ½ç”¨æˆ·è¯†åˆ«...")
    
    start_time = time.time()
    
    try:
        # åˆ›å»ºCOSMICåŠŸèƒ½ç”¨æˆ·æ™ºèƒ½ä½“
        user_agent = COSMICFunctionalUserAgent()
        
        # æ‰§è¡ŒåŠŸèƒ½ç”¨æˆ·è¯†åˆ«
        functional_users = await user_agent.execute(
            task_name="identify_functional_users",
            inputs={
                "project_info": state.project_info,
                "requirement_analysis": state.requirement_analysis,
                "identified_processes": state.identified_processes
            }
        )
        
        # åˆå§‹åŒ–COSMICç»“æœ
        if not state.cosmic_results:
            state.cosmic_results = {
                "functional_users": [],
                "boundary_analysis": None,
                "data_movements": [],
                "functional_processes": [],
                "total_cfp": 0,
                "cfp_breakdown": {},
                "estimation_confidence": 0.0,
                "analysis_issues": [],
                "calculation_metadata": {}
            }
        
        state.cosmic_results.functional_users = functional_users if isinstance(functional_users, list) else functional_users.get("functional_users", [])
        
        processing_time = time.time() - start_time
        
        # è®°å½•æ‰§è¡Œæ—¥å¿—
        state = update_execution_log(
            state,
            agent_id="cosmic_functional_user",
            action="identify_users",
            status="success",
            duration_ms=int(processing_time * 1000),
            details={
                "functional_users_count": len(functional_users),
                "user_names": [user.name for user in functional_users]
            }
        )
        
        logger.info(f"âœ… COSMICåŠŸèƒ½ç”¨æˆ·è¯†åˆ«å®Œæˆï¼Œè¯†åˆ«å‡º {len(functional_users)} ä¸ªåŠŸèƒ½ç”¨æˆ·")
        return state
        
    except Exception as e:
        logger.error(f"âŒ COSMICåŠŸèƒ½ç”¨æˆ·è¯†åˆ«å¤±è´¥: {str(e)}")
        state.error_message = str(e)
        return transition_state(state, WorkflowState.ERROR_ENCOUNTERED, str(e))


# COSMICè¾¹ç•Œåˆ†æèŠ‚ç‚¹
async def cosmic_analyze_boundary_node(state: WorkflowGraphState) -> WorkflowGraphState:
    """COSMICè¾¹ç•Œåˆ†æèŠ‚ç‚¹"""
    
    logger.info("ğŸ—ï¸ å¼€å§‹COSMICè¾¹ç•Œåˆ†æ...")
    
    start_time = time.time()
    
    try:
        # åˆ›å»ºCOSMICè¾¹ç•Œåˆ†ææ™ºèƒ½ä½“
        boundary_analyzer = COSMICBoundaryAnalyzerAgent()
        
        # æ‰§è¡Œè¾¹ç•Œåˆ†æ
        boundary_analysis = await boundary_analyzer.analyze_system_boundary(
            project_info=state.project_info,
            functional_users=state.cosmic_results.functional_users,
            requirement_analysis=state.requirement_analysis
        )
        
        state.cosmic_results.boundary_analysis= boundary_analysis
        
        processing_time = time.time() - start_time
        
        # è®°å½•æ‰§è¡Œæ—¥å¿—
        state = update_execution_log(
            state,
            agent_id="cosmic_boundary_analyzer",
            action="analyze_boundary",
            status="success",
            duration_ms=int(processing_time * 1000),
            details={
                "software_boundary": boundary_analysis.software_boundary,
                "storage_boundary": boundary_analysis.persistent_storage_boundary
            }
        )
        
        logger.info("âœ… COSMICè¾¹ç•Œåˆ†æå®Œæˆ")
        return state
        
    except Exception as e:
        logger.error(f"âŒ COSMICè¾¹ç•Œåˆ†æå¤±è´¥: {str(e)}")
        state.error_message = str(e)
        return transition_state(state, WorkflowState.ERROR_ENCOUNTERED, str(e))


# COSMICæ•°æ®ç§»åŠ¨åˆ†ç±»èŠ‚ç‚¹
async def cosmic_classify_movements_node(state: WorkflowGraphState) -> WorkflowGraphState:
    """COSMICæ•°æ®ç§»åŠ¨åˆ†ç±»èŠ‚ç‚¹"""
    
    logger.info("ğŸ”„ å¼€å§‹COSMICæ•°æ®ç§»åŠ¨åˆ†ç±»...")
    
    start_time = time.time()
    
    try:
        # åˆ›å»ºCOSMICæ•°æ®ç§»åŠ¨åˆ†ç±»æ™ºèƒ½ä½“
        movement_classifier = COSMICDataMovementClassifierAgent()
        
        # æ‰§è¡Œæ•°æ®ç§»åŠ¨åˆ†ç±»
        data_movements = await movement_classifier.classify_data_movements(
            identified_processes=state.identified_processes,
            functional_users=state.cosmic_results.functional_users,
            boundary_analysis=state.cosmic_results.boundary_analysis
        )
        
        state.cosmic_results.data_movements= data_movements
        
        processing_time = time.time() - start_time
        
        # è®°å½•æ‰§è¡Œæ—¥å¿—
        state = update_execution_log(
            state,
            agent_id="cosmic_movement_classifier",
            action="classify_movements",
            status="success",
            duration_ms=int(processing_time * 1000),
            details={
                "data_movements_count": len(data_movements),
                "movement_types": [dm.type for dm in data_movements]
            }
        )
        
        # è½¬æ¢çŠ¶æ€
        state = transition_state(
            state,
            WorkflowState.COSMIC_ANALYSIS_COMPLETED,
            f"COSMICæ•°æ®ç§»åŠ¨åˆ†ç±»å®Œæˆï¼Œè¯†åˆ«å‡º {len(data_movements)} ä¸ªæ•°æ®ç§»åŠ¨"
        )
        
        logger.info(f"âœ… COSMICæ•°æ®ç§»åŠ¨åˆ†ç±»å®Œæˆ")
        return state
        
    except Exception as e:
        logger.error(f"âŒ COSMICæ•°æ®ç§»åŠ¨åˆ†ç±»å¤±è´¥: {str(e)}")
        state.error_message = str(e)
        return transition_state(state, WorkflowState.ERROR_ENCOUNTERED, str(e))


# COSMIC CFPè®¡ç®—èŠ‚ç‚¹
async def cosmic_calculate_cfp_node(state: WorkflowGraphState) -> WorkflowGraphState:
    """COSMIC CFPè®¡ç®—èŠ‚ç‚¹"""
    
    logger.info("ğŸ§® å¼€å§‹COSMIC CFPè®¡ç®—...")
    
    start_time = time.time()
    
    try:
        # åˆ›å»ºCOSMIC CFPè®¡ç®—æ™ºèƒ½ä½“
        cfp_calculator = COSMICCFPCalculatorAgent()
        
        # æ‰§è¡ŒCFPè®¡ç®—
        cfp_result = await cfp_calculator.calculate_cosmic_function_points(
            data_movements=state.cosmic_results.data_movements,
            functional_processes=state.identified_processes,
            boundary_analysis=state.cosmic_results.boundary_analysis
        )
        
        # æ›´æ–°COSMICç»“æœ
        state.cosmic_results.total_cfp= cfp_result.total_cfp
        state.cosmic_results.cfp_breakdown = cfp_result.cfp_breakdown
        state.cosmic_results.estimation_confidence = cfp_result.confidence_score
        state.cosmic_results.functional_processes = cfp_result.functional_processes
        state.cosmic_results.calculation_metadata = cfp_result.calculation_details
        
        processing_time = time.time() - start_time
        
        # è®°å½•æ‰§è¡Œæ—¥å¿—
        state = update_execution_log(
            state,
            agent_id="cosmic_cfp_calculator",
            action="calculate_cfp",
            status="success",
            duration_ms=int(processing_time * 1000),
            details={
                "total_cfp": cfp_result.total_cfp,
                "confidence_score": cfp_result.confidence_score
            }
        )
        
        # è½¬æ¢çŠ¶æ€
        state = transition_state(
            state,
            WorkflowState.COSMIC_CALCULATION_COMPLETED,
            f"COSMIC CFPè®¡ç®—å®Œæˆ: {cfp_result.total_cfp} CFP"
        )
        
        logger.info(f"âœ… COSMIC CFPè®¡ç®—å®Œæˆ: {cfp_result.total_cfp} CFP")
        return state
        
    except Exception as e:
        logger.error(f"âŒ COSMIC CFPè®¡ç®—å¤±è´¥: {str(e)}")
        state.error_message = str(e)
        return transition_state(state, WorkflowState.ERROR_ENCOUNTERED, str(e))


# ç»“æœéªŒè¯èŠ‚ç‚¹
async def validate_results_node(state: WorkflowGraphState) -> WorkflowGraphState:
    """ç»“æœéªŒè¯èŠ‚ç‚¹"""
    
    logger.info("âœ… å¼€å§‹ç»“æœéªŒè¯...")
    
    start_time = time.time()
    
    try:
        # åˆ›å»ºéªŒè¯æ™ºèƒ½ä½“
        validator = ValidatorAgent()
        
        # åˆå§‹åŒ–éªŒè¯ç»“æœ
        nesma_validation = None
        cosmic_validation = None
        
        # éªŒè¯NESMAç»“æœ
        if state.nesma_results:
            nesma_validation = await validator.validate_analysis_result(
                analysis_type="NESMA_estimation",
                analysis_result=state.nesma_results,
                input_data=state.requirement_analysis
            )
        
        # éªŒè¯COSMICç»“æœ
        if state.cosmic_results:
            cosmic_validation = await validator.validate_analysis_result(
                analysis_type="COSMIC_estimation",
                analysis_result=state.cosmic_results,
                input_data=state.requirement_analysis
            )
        
        # è®¡ç®—æ•´ä½“éªŒè¯åˆ†æ•°
        overall_validation = _calculate_overall_validation({
            "nesma_validation": nesma_validation,
            "cosmic_validation": cosmic_validation
        })
        
        # åˆ›å»ºValidationResultså¯¹è±¡
        from graph.state_definitions import ValidationResults
        
        validation_results = ValidationResults(
            nesma_validation=nesma_validation,
            cosmic_validation=cosmic_validation,
            overall_validation=overall_validation.model_dump() if overall_validation else None,
            validation_status="completed",
            validation_score=overall_validation.confidence_score if overall_validation else 0.0,
            validation_issues=[],
            recommendations=[]
        )
        
        state.validation_results = validation_results
        
        processing_time = time.time() - start_time
        
        # è®°å½•æ‰§è¡Œæ—¥å¿—
        state = update_execution_log(
            state,
            agent_id="validator",
            action="validate_results",
            status="success",
            duration_ms=int(processing_time * 1000),
            details={
                "overall_validation_score": validation_results.validation_score
            }
        )
        
        # è½¬æ¢çŠ¶æ€
        state = transition_state(
            state,
            WorkflowState.VALIDATION_COMPLETED,
            "ç»“æœéªŒè¯å®Œæˆ"
        )
        
        logger.info("âœ… ç»“æœéªŒè¯å®Œæˆ")
        return state
        
    except Exception as e:
        logger.error(f"âŒ ç»“æœéªŒè¯å¤±è´¥: {str(e)}")
        state.error_message = str(e)
        return transition_state(state, WorkflowState.ERROR_ENCOUNTERED, str(e))


# å¯¹æ¯”åˆ†æèŠ‚ç‚¹
async def compare_standards_node(state: WorkflowGraphState) -> WorkflowGraphState:
    """è·¨æ ‡å‡†å¯¹æ¯”åˆ†æèŠ‚ç‚¹"""
    
    logger.info("ğŸ“Š å¼€å§‹è·¨æ ‡å‡†å¯¹æ¯”åˆ†æ...")
    
    start_time = time.time()
    
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸¤ç§æ ‡å‡†çš„ç»“æœ
        if not state.nesma_results or not state.cosmic_results:
            logger.warning("ç¼ºå°‘å®Œæ•´çš„ä¼°ç®—ç»“æœï¼Œè·³è¿‡å¯¹æ¯”åˆ†æ")
            return transition_state(
                state,
                WorkflowState.REPORT_GENERATION_PENDING,
                "è·³è¿‡å¯¹æ¯”åˆ†æï¼Œç¼ºå°‘å®Œæ•´ç»“æœ"
            )
        
        # åˆ›å»ºå¯¹æ¯”åˆ†ææ™ºèƒ½ä½“
        analyzer = ComparisonAnalyzerAgent()
        
        # æ‰§è¡Œå¯¹æ¯”åˆ†æ
        comparison_result = await analyzer.analyze_cross_standard_comparison(
            nesma_results=state.nesma_results,
            cosmic_results=state.cosmic_results,
            project_info=state.project_info.__dict__
        )
        
        state.comparison_analysis = comparison_result
        
        processing_time = time.time() - start_time
        
        # è®°å½•æ‰§è¡Œæ—¥å¿—
        state = update_execution_log(
            state,
            agent_id="comparison_analyzer",
            action="compare_standards",
            status="success",
            duration_ms=int(processing_time * 1000),
            details={
                "difference_percentage": comparison_result.difference_percentage,
                "recommended_standard": comparison_result.recommendation.get("recommended_standard")
            }
        )
        
        # è½¬æ¢çŠ¶æ€
        state = transition_state(
            state,
            WorkflowState.COMPARISON_ANALYSIS_COMPLETED,
            "è·¨æ ‡å‡†å¯¹æ¯”åˆ†æå®Œæˆ"
        )
        
        logger.info("âœ… è·¨æ ‡å‡†å¯¹æ¯”åˆ†æå®Œæˆ")
        return state
        
    except Exception as e:
        logger.error(f"âŒ è·¨æ ‡å‡†å¯¹æ¯”åˆ†æå¤±è´¥: {str(e)}")
        state.error_message = str(e)
        return transition_state(state, WorkflowState.ERROR_ENCOUNTERED, str(e))


# æŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹
async def generate_report_node(state: WorkflowGraphState) -> WorkflowGraphState:
    """æŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹"""
    
    logger.info("ğŸ“„ å¼€å§‹ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")
    
    start_time = time.time()
    
    try:
        # åˆ›å»ºæŠ¥å‘Šç”Ÿæˆæ™ºèƒ½ä½“
        generator = ReportGeneratorAgent()
        
        # ğŸ”¥ æ”¯æŒå¤šç§æŠ¥å‘Šæ ¼å¼ - é»˜è®¤ç”ŸæˆExcelå’ŒWordæ ¼å¼
        report_formats = ["excel", "word", "markdown"]
        generated_reports = {}
        
        for format_type in report_formats:
            try:
                # ç”ŸæˆæŠ¥å‘Š
                report_data = await generator.generate_estimation_report(
                    project_info=state.project_info,
                    estimation_results={
                        "nesma_results": state.nesma_results.model_dump() if state.nesma_results else None,
                        "cosmic_results": state.cosmic_results.model_dump() if state.cosmic_results else None,
                        "comparison_result": state.comparison_analysis.model_dump() if state.comparison_analysis else None,
                        "validation_results": state.validation_results.model_dump() if state.validation_results else None,
                        "execution_log": [log.dict() for log in state.execution_log] if state.execution_log else []
                    },
                    format=format_type
                )
                
                # å°†ReportContentè½¬æ¢ä¸ºPydanticæ¨¡å‹
                from graph.state_definitions import ReportContent
                if format_type == "markdown":
                    generated_reports["markdown"] = ReportContent(
                        content=report_data.get("content"),
                        file_path=report_data.get("file_path"),
                        error=report_data.get("error")
                    )
                elif format_type == "excel":
                    generated_reports["excel"] = ReportContent(
                        content=report_data.get("content"),
                        file_path=report_data.get("file_path"),
                        error=report_data.get("error")
                    )
                elif format_type == "word":
                    generated_reports["word"] = ReportContent(
                        content=report_data.get("content"),
                        file_path=report_data.get("file_path"),
                        error=report_data.get("error")
                    )
                
                logger.info(f"âœ… {format_type.upper()}æ ¼å¼æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
                
            except Exception as e:
                logger.error(f"âŒ {format_type.upper()}æ ¼å¼æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")
                # åˆ›å»ºé”™è¯¯æŠ¥å‘Š
                from graph.state_definitions import ReportContent
                generated_reports[format_type] = ReportContent(
                    content=None,
                    file_path=None,
                    error=str(e)
                )
        
        # åˆ›å»ºFinalReport Pydanticæ¨¡å‹
        from graph.state_definitions import FinalReport
        final_report = FinalReport(
            markdown=generated_reports.get("markdown"),
            excel=generated_reports.get("excel"),
            word=generated_reports.get("word")
        )
        
        # æ›´æ–°çŠ¶æ€
        state.final_report = final_report
        
        # è®°å½•æ‰§è¡Œæ—¶é—´
        execution_time = int((time.time() - start_time) * 1000)
        state = update_execution_log(
            state,
            "report_generator",
            "generate_final_report",
            "success",
            execution_time,
            {"formats_generated": list(generated_reports.keys())}
        )
        
        logger.info(f"ğŸ“„ æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {execution_time}ms")
        
        return state
        
    except Exception as e:
        logger.error(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")
        
        # åˆ›å»ºé”™è¯¯æŠ¥å‘Š
        from graph.state_definitions import FinalReport, ReportContent
        error_report = ReportContent(
            content=None,
            file_path=None,
            error=str(e)
        )
        
        state.final_report = FinalReport(
            markdown=error_report,
            excel=error_report,
            word=error_report
        )
        
        # è®°å½•é”™è¯¯
        execution_time = int((time.time() - start_time) * 1000)
        state = update_execution_log(
            state,
            "report_generator",
            "generate_final_report",
            "error",
            execution_time,
            {"error": str(e)}
        )
        
        return state


# å®ŒæˆèŠ‚ç‚¹
async def complete_workflow_node(state: WorkflowGraphState) -> WorkflowGraphState:
    """å·¥ä½œæµå®ŒæˆèŠ‚ç‚¹"""
    
    logger.info("ğŸ‰ å·¥ä½œæµæ‰§è¡Œå®Œæˆ")
    
    try:
        # è®¡ç®—æœ€ç»ˆè´¨é‡æŒ‡æ ‡
        state.quality_metrics.accuracy_score = _calculate_overall_confidence(state)
        
        # è®°å½•å®Œæˆæ—¥å¿—
        state = update_execution_log(
            state,
            agent_id="workflow_manager",
            action="complete_workflow",
            status="success",
            details={
                "total_duration": state.processing_stats.total_duration_ms,
                "overall_confidence": state.quality_metrics.accuracy_score,
                "nesma_ufp": state.nesma_results.total_ufp if state.nesma_results else None,
                "cosmic_cfp": state.cosmic_results.total_cfp if state.cosmic_results else None
            }
        )
        
        # è½¬æ¢åˆ°æœ€ç»ˆçŠ¶æ€
        state = transition_state(
            state,
            WorkflowState.COMPLETED,
            "å·¥ä½œæµæ‰§è¡ŒæˆåŠŸå®Œæˆ"
        )
        
        logger.info("âœ… å·¥ä½œæµæ‰§è¡ŒæˆåŠŸå®Œæˆ")
        return state
        
    except Exception as e:
        logger.error(f"âŒ å·¥ä½œæµå®Œæˆå¤±è´¥: {str(e)}")
        state.error_message = str(e)
        return transition_state(state, WorkflowState.ERROR_ENCOUNTERED, str(e))


# é”™è¯¯å¤„ç†èŠ‚ç‚¹
async def handle_error_node(state: WorkflowGraphState) -> WorkflowGraphState:
    """é”™è¯¯å¤„ç†èŠ‚ç‚¹"""
    
    # ğŸ”¥ æ”¹è¿›é”™è¯¯æ¶ˆæ¯å¤„ç†ï¼Œé¿å…None
    error_msg = state.error_message
    if not error_msg or error_msg == "None":
        # å°è¯•ä»çŠ¶æ€ä¸­æå–æ›´å¤šé”™è¯¯ä¿¡æ¯
        last_log = state.execution_log or []
        if last_log:
            last_entry = last_log[-1]
            if isinstance(last_entry, dict) and last_entry.get("status") == "error":
                error_msg = last_entry.get("details", {}).get("error", "å·¥ä½œæµæ‰§è¡Œé‡åˆ°æœªçŸ¥é”™è¯¯")
            else:
                error_msg = "å·¥ä½œæµæ‰§è¡Œé‡åˆ°æœªçŸ¥é”™è¯¯"
        else:
            error_msg = "å·¥ä½œæµæ‰§è¡Œé‡åˆ°æœªçŸ¥é”™è¯¯"
    
    logger.error(f"âŒ å¤„ç†å·¥ä½œæµé”™è¯¯: {error_msg}")
    
    # ğŸ”¥ è®°å½•é”™è¯¯è¯¦æƒ…ç”¨äºè°ƒè¯•
    logger.error(f"ğŸ” é”™è¯¯çŠ¶æ€è¯¦æƒ…:")
    logger.error(f"  - å½“å‰çŠ¶æ€: {state.current_state}")
    logger.error(f"  - é‡è¯•æ¬¡æ•°: {state.retry_count}")
    logger.error(f"  - æœ€å¤§é‡è¯•: {state.max_retries}")
    logger.error(f"  - é”™è¯¯æ¶ˆæ¯: {repr(error_msg)}")
    
    try:
        # ğŸ”¥ ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœå½“å‰çŠ¶æ€æ˜¯VALIDATION_COMPLETEDä¸”é”™è¯¯ä¿¡æ¯æ¶‰åŠæµç¨‹è¯†åˆ«
        # è¯´æ˜æ˜¯çŠ¶æ€æµè½¬å‡ºç°é—®é¢˜ï¼Œåº”è¯¥ç›´æ¥ç»§ç»­åˆ°ä¸‹ä¸€æ­¥è€Œä¸æ˜¯é‡æ–°æ‰§è¡Œæµç¨‹è¯†åˆ«
        current_state = state.current_state
        
        # ğŸ”¥ å¢åŠ æ›´å¤šç‰¹æ®Šå¤„ç†æƒ…å†µ
        if (current_state == WorkflowState.VALIDATION_COMPLETED and 
            ("æµç¨‹è¯†åˆ«å¤±è´¥" in error_msg and "éœ€æ±‚è§£æç»“æœä¸å­˜åœ¨" in error_msg) or 
            error_msg == "å·¥ä½œæµæ‰§è¡Œé‡åˆ°æœªçŸ¥é”™è¯¯"):
            
            logger.info("ğŸ”§ æ£€æµ‹åˆ°çŠ¶æ€æµè½¬é”™è¯¯ï¼Œç›´æ¥è·³è½¬åˆ°æŠ¥å‘Šç”Ÿæˆé˜¶æ®µ")
            
            # ğŸ”¥ ä¿®å¤çŠ¶æ€è®¿é—®é—®é¢˜ - æ­£ç¡®å¤„ç†Pydanticæ¨¡å‹
            try:
                # ç›´æ¥è®¿é—®Pydanticæ¨¡å‹å±æ€§ï¼Œä¸ä½¿ç”¨å­—å…¸è®¿é—®
                has_nesma_results = bool(state.nesma_results and state.nesma_results.total_ufp)
                has_cosmic_results = bool(state.cosmic_results and state.cosmic_results.total_cfp)
                
                logger.info(f"ğŸ” ç»“æœæ£€æŸ¥: NESMA={has_nesma_results}, COSMIC={has_cosmic_results}")
                logger.info(f"ğŸ” NESMAç»“æœ: {state.nesma_results}")
                logger.info(f"ğŸ” COSMICç»“æœ: {state.cosmic_results}")
                
            except Exception as e:
                logger.error(f"âŒ æ£€æŸ¥ç»“æœæ—¶å‡ºé”™: {e}")
                has_nesma_results = False
                has_cosmic_results = False
            
            if has_nesma_results or has_cosmic_results:
                # æœ‰ç»“æœï¼Œç›´æ¥ç”ŸæˆæŠ¥å‘Š
                return transition_state(
                    state,
                    WorkflowState.REPORT_GENERATION_PENDING,
                    "è·³è¿‡é”™è¯¯çŠ¶æ€ï¼Œç›´æ¥ç”ŸæˆæŠ¥å‘Š"
                )
            else:
                # æ²¡æœ‰æœ‰æ•ˆç»“æœï¼Œç»ˆæ­¢å·¥ä½œæµ
                logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„ä¼°ç®—ç»“æœï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
                return transition_state(
                    state,
                    WorkflowState.TERMINATED,
                    f"å·¥ä½œæµç»ˆæ­¢: æ²¡æœ‰æœ‰æ•ˆçš„ä¼°ç®—ç»“æœ"
                )
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è¯•
        if is_retry_needed(state):
            # å¢åŠ é‡è¯•æ¬¡æ•°
            state = increment_retry_attempt(state)
            
            logger.info(f"ğŸ”„ å‡†å¤‡é‡è¯•ï¼Œç¬¬ {state.retry_count} æ¬¡")
            
            # è½¬æ¢åˆ°é‡è¯•çŠ¶æ€
            return transition_state(
                state,
                WorkflowState.STARTING,
                f"å‡†å¤‡é‡è¯•: {error_msg}"
            )
        else:
            # é‡è¯•æ¬¡æ•°å·²è¾¾ä¸Šé™ï¼Œç»ˆæ­¢å·¥ä½œæµ
            logger.error("âŒ é‡è¯•æ¬¡æ•°å·²è¾¾ä¸Šé™ï¼Œç»ˆæ­¢å·¥ä½œæµ")
            
            return transition_state(
                state,
                WorkflowState.TERMINATED,
                f"å·¥ä½œæµç»ˆæ­¢: {error_msg}"
            )
            
    except Exception as e:
        logger.error(f"âŒ é”™è¯¯å¤„ç†å¤±è´¥: {str(e)}")
        import traceback
        logger.error(f"âŒ é”™è¯¯å¤„ç†å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
        return transition_state(
            state,
            WorkflowState.TERMINATED,
            f"é”™è¯¯å¤„ç†å¤±è´¥: {str(e)}"
        )


# è¾…åŠ©å‡½æ•°
def _calculate_overall_validation(validation_results: Dict[str, Any]) -> Optional[Any]:
    """è®¡ç®—æ•´ä½“éªŒè¯ç»“æœ"""
    
    scores = []
    
    if "nesma_validation" in validation_results:
        scores.append(validation_results["nesma_validation"].confidence_score)
    
    if "cosmic_validation" in validation_results:
        scores.append(validation_results["cosmic_validation"].confidence_score)
    
    if not scores:
        return None
    
    # åˆ›å»ºè™šæ‹Ÿçš„æ•´ä½“éªŒè¯ç»“æœ
    from models.common_models import ValidationResult
    
    overall_score = sum(scores) / len(scores)
    
    return ValidationResult(
        is_valid=overall_score >= 0.5,
        confidence_score=overall_score,
        confidence_level=_score_to_confidence_level(overall_score),
        errors=[],
        warnings=[],
        suggestions=[],
        metadata={"component_scores": scores}
    )


def _score_to_confidence_level(score: float):
    """å°†åˆ†æ•°è½¬æ¢ä¸ºç½®ä¿¡åº¦ç­‰çº§"""
    from models.common_models import ConfidenceLevel
    if score >= 0.8:
        return ConfidenceLevel.VERY_HIGH
    elif score >= 0.6:
        return ConfidenceLevel.HIGH
    elif score >= 0.4:
        return ConfidenceLevel.MEDIUM
    elif score >= 0.2:
        return ConfidenceLevel.LOW
    else:
        return ConfidenceLevel.VERY_LOW


def _calculate_overall_confidence(state: WorkflowGraphState) -> float:
    """è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦"""
    
    confidence_scores = []
    
    # NESMAç½®ä¿¡åº¦
    if state.nesma_results and hasattr(state.nesma_results, 'confidence_level'):
        # å°†ç½®ä¿¡åº¦ç­‰çº§è½¬æ¢ä¸ºåˆ†æ•°
        confidence_level = state.nesma_results.confidence_level
        if confidence_level == "HIGH":
            confidence_scores.append(0.8)
        elif confidence_level == "MEDIUM":
            confidence_scores.append(0.6)
        elif confidence_level == "LOW":
            confidence_scores.append(0.4)
        else:
            confidence_scores.append(0.5)  # é»˜è®¤ä¸­ç­‰ç½®ä¿¡åº¦
    
    # COSMICç½®ä¿¡åº¦
    if state.cosmic_results and hasattr(state.cosmic_results, 'confidence_level'):
        # å°†ç½®ä¿¡åº¦ç­‰çº§è½¬æ¢ä¸ºåˆ†æ•°
        confidence_level = state.cosmic_results.confidence_level
        if confidence_level == "HIGH":
            confidence_scores.append(0.8)
        elif confidence_level == "MEDIUM":
            confidence_scores.append(0.6)
        elif confidence_level == "LOW":
            confidence_scores.append(0.4)
        else:
            confidence_scores.append(0.5)  # é»˜è®¤ä¸­ç­‰ç½®ä¿¡åº¦
    
    # éªŒè¯ç½®ä¿¡åº¦
    if state.validation_results and hasattr(state.validation_results, 'validation_score'):
        validation_score = state.validation_results.validation_score
        if validation_score is not None:
            confidence_scores.append(validation_score)
    
    if not confidence_scores:
        return 0.0
    
    return sum(confidence_scores) / len(confidence_scores) 