"""
é‡å­æ™ºèƒ½åŒ–åŠŸèƒ½ç‚¹ä¼°ç®—ç³»ç»Ÿ - è§„åˆ™æ£€ç´¢æ™ºèƒ½ä½“

åŸºäºPgVectorå®ç°æ™ºèƒ½çŸ¥è¯†æ£€ç´¢å’ŒéªŒè¯
"""

import asyncio
import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_core.language_models import BaseLanguageModel

from agents.base.base_agent import BaseAgent
from models.common_models import EstimationStandard, KnowledgeQuery, KnowledgeResult, ValidationResult, ConfidenceLevel
from knowledge_base.vector_stores.pgvector_store import PgVectorStore
from knowledge_base.embeddings.embedding_models import get_embedding_model

logger = logging.getLogger(__name__)


class RuleRetrieverAgent(BaseAgent):
    """è§„åˆ™æ£€ç´¢æ™ºèƒ½ä½“ - åŸºäºPgVector"""
    
    def __init__(
        self,
        llm: BaseLanguageModel,
        embeddings: Embeddings,
        vector_store: PgVectorStore,
        agent_name: str = "RuleRetriever"
    ):
        super().__init__(llm, agent_name)
        self.embeddings = embeddings
        self.vector_store = vector_store
        
        # æ£€ç´¢é…ç½®
        self.retrieval_config = {
            "default_k": 5,
            "relevance_threshold": 0.7,
            "max_retries": 3
        }

    async def initialize(self):
        """åˆå§‹åŒ–æ™ºèƒ½ä½“"""
        await super().initialize()
        
        logger.info("âœ… è§„åˆ™æ£€ç´¢æ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
    
    async def retrieve_rules(
        self,
        query: str,
        standard: Optional[EstimationStandard] = None,
        use_cache: bool = True,
        min_chunks: int = 3,
        max_retries: int = 2
    ) -> KnowledgeResult:
        """æ£€ç´¢ç›¸å…³è§„åˆ™"""
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = f"{query}_{standard}_{min_chunks}"
        
        # æ£€æŸ¥ç¼“å­˜
        if use_cache and cache_key in self.retrieval_cache:
            cached_result = self.retrieval_cache[cache_key]
            logger.info(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜ç»“æœ: {query}")
            return cached_result
        
        # ç¡®å®šæ£€ç´¢ç­–ç•¥
        if standard == EstimationStandard.NESMA:
            preferred_source = "nesma"
            fallback_sources = ["common"]
        elif standard == EstimationStandard.COSMIC:
            preferred_source = "cosmic"
            fallback_sources = ["common"]
        else:
            preferred_source = None
            fallback_sources = None
        
        # æ‰§è¡Œæ£€ç´¢
        retrieval_result = None
        for attempt in range(max_retries + 1):
            try:
                if attempt == 0:
                    # é¦–æ¬¡å°è¯•ï¼šè‡ªé€‚åº”æ£€ç´¢
                    retrieval_result = await self.multi_source_retriever.adaptive_retrieve(
                        query=query,
                        preferred_source=preferred_source,
                        fallback_sources=fallback_sources,
                        min_chunks=min_chunks
                    )
                else:
                    # é‡è¯•ï¼šé™ä½è¦æ±‚
                    retrieval_result = await self.multi_source_retriever.adaptive_retrieve(
                        query=query,
                        preferred_source=None,
                        fallback_sources=None,
                        min_chunks=max(1, min_chunks - attempt)
                    )
                
                # æ£€æŸ¥ç»“æœè´¨é‡
                if retrieval_result.retrieved_chunks:
                    break
                    
            except Exception as e:
                logger.warning(f"âš ï¸ æ£€ç´¢å°è¯• {attempt + 1} å¤±è´¥: {str(e)}")
                if attempt == max_retries:
                    # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ
                    retrieval_result = KnowledgeResult(
                        query=query,
                        source_type=standard or EstimationStandard.BOTH,
                        retrieved_chunks=[],
                        total_chunks=0,
                        retrieval_time_ms=0
                    )
        
        # éªŒè¯æ£€ç´¢ç»“æœ
        if retrieval_result and retrieval_result.retrieved_chunks:
            validation_result = await self._validate_retrieval_result(retrieval_result)
            retrieval_result.validation_result = validation_result
        
        # ç¼“å­˜ç»“æœ
        if use_cache and retrieval_result:
            self.retrieval_cache[cache_key] = retrieval_result
        
        # è®°å½•æ£€ç´¢å†å²
        if retrieval_result:
            self.retrieval_history.append(retrieval_result)
        
        return retrieval_result
    
    async def _validate_retrieval_result(self, retrieval_result: KnowledgeResult) -> ValidationResult:
        """éªŒè¯æ£€ç´¢ç»“æœçš„è´¨é‡"""
        
        if not retrieval_result.retrieved_chunks:
            return ValidationResult(
                is_valid=False,
                confidence_level=ConfidenceLevel.LOW,
                issues=["æ£€ç´¢ç»“æœä¸ºç©º"],
                suggestions=["å°è¯•ä¸åŒçš„æŸ¥è¯¢è¯", "æ£€æŸ¥çŸ¥è¯†åº“å†…å®¹"]
            )
        
        # åŸºç¡€è´¨é‡æŒ‡æ ‡
        total_chunks = len(retrieval_result.retrieved_chunks)
        high_relevance_chunks = [
            chunk for chunk in retrieval_result.retrieved_chunks 
            if chunk.relevance_score > 0.8
        ]
        medium_relevance_chunks = [
            chunk for chunk in retrieval_result.retrieved_chunks 
            if 0.6 <= chunk.relevance_score <= 0.8
        ]
        
        # è®¡ç®—æ€»ä½“è´¨é‡åˆ†æ•°
        quality_score = 0.0
        if total_chunks > 0:
            quality_score = (
                len(high_relevance_chunks) * 1.0 + 
                len(medium_relevance_chunks) * 0.7
            ) / total_chunks
        
        # ä½¿ç”¨LLMè¿›è¡Œæ·±åº¦éªŒè¯
        llm_validation = await self._llm_validate_relevance(retrieval_result)
        
        # ç»¼åˆåˆ¤æ–­
        issues = []
        suggestions = []
        
        if total_chunks < 2:
            issues.append("æ£€ç´¢ç»“æœæ•°é‡è¾ƒå°‘")
            suggestions.append("æ‰©å¤§æ£€ç´¢èŒƒå›´æˆ–è°ƒæ•´æŸ¥è¯¢ç­–ç•¥")
        
        if len(high_relevance_chunks) == 0:
            issues.append("ç¼ºä¹é«˜ç›¸å…³æ€§å†…å®¹")
            suggestions.append("ä¼˜åŒ–æŸ¥è¯¢è¡¨è¾¾æˆ–æ£€æŸ¥çŸ¥è¯†åº“å®Œæ•´æ€§")
        
        # ç¡®å®šç½®ä¿¡åº¦
        if quality_score >= 0.8 and llm_validation.get("relevant", False):
            confidence_level = ConfidenceLevel.HIGH
            is_valid = True
        elif quality_score >= 0.6 and llm_validation.get("partially_relevant", False):
            confidence_level = ConfidenceLevel.MEDIUM
            is_valid = True
        else:
            confidence_level = ConfidenceLevel.LOW
            is_valid = len(retrieval_result.retrieved_chunks) > 0
        
        return ValidationResult(
            is_valid=is_valid,
            confidence_level=confidence_level,
            validation_score=quality_score,
            issues=issues,
            suggestions=suggestions,
            details={
                "total_chunks": total_chunks,
                "high_relevance_count": len(high_relevance_chunks),
                "medium_relevance_count": len(medium_relevance_chunks),
                "llm_validation": llm_validation
            }
        )
    
    async def _llm_validate_relevance(self, retrieval_result: KnowledgeResult) -> Dict[str, Any]:
        """ä½¿ç”¨LLMéªŒè¯æ£€ç´¢ç»“æœçš„ç›¸å…³æ€§"""
        
        if not self.llm or not retrieval_result.retrieved_chunks:
            return {"relevant": False, "reasoning": "æ— æ³•è¿›è¡ŒLLMéªŒè¯"}
        
        # å‡†å¤‡éªŒè¯å†…å®¹
        query = retrieval_result.query
        top_chunks = retrieval_result.retrieved_chunks[:3]  # åªéªŒè¯å‰3ä¸ªç»“æœ
        
        chunks_text = "\n\n---\n\n".join([
            f"æ–‡æ¡£{i+1}: {chunk.content[:300]}..."
            for i, chunk in enumerate(top_chunks)
        ])
        
        validation_prompt = f"""
        è¯·åˆ†æä»¥ä¸‹æ£€ç´¢ç»“æœä¸æŸ¥è¯¢çš„ç›¸å…³æ€§ï¼š
        
        æŸ¥è¯¢: {query}
        
        æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹:
        {chunks_text}
        
        è¯·è¯„ä¼°ï¼š
        1. è¿™äº›æ–‡æ¡£å†…å®¹æ˜¯å¦ä¸æŸ¥è¯¢ç›¸å…³ï¼Ÿ
        2. ç›¸å…³æ€§ç¨‹åº¦å¦‚ä½•ï¼Ÿ
        3. æ˜¯å¦åŒ…å«å¯ä»¥å›ç­”æŸ¥è¯¢çš„ä¿¡æ¯ï¼Ÿ
        
        è¯·ç”¨JSONæ ¼å¼å›ç­”ï¼š
        {{
            "relevant": true/false,
            "partially_relevant": true/false,
            "relevance_score": 0-1ä¹‹é—´çš„åˆ†æ•°,
            "reasoning": "è¯¦ç»†è¯´æ˜"
        }}
        """
        
        try:
            response = await self.llm.ainvoke(validation_prompt)
            
            # è§£æLLMå“åº”
            import json
            result = json.loads(response.content)
            
            return result
            
        except Exception as e:
            logger.warning(f"âš ï¸ LLMéªŒè¯å¤±è´¥: {str(e)}")
            return {
                "relevant": False,
                "reasoning": f"LLMéªŒè¯å‡ºé”™: {str(e)}"
            }
    
    async def retrieve_by_context(
        self,
        context: Dict[str, Any],
        specific_queries: Optional[List[str]] = None
    ) -> Dict[str, KnowledgeResult]:
        """æ ¹æ®ä¸Šä¸‹æ–‡è¿›è¡Œå¤šæ–¹é¢æ£€ç´¢"""
        
        # æå–æ£€ç´¢æŸ¥è¯¢
        queries_to_search = []
        
        if specific_queries:
            queries_to_search.extend(specific_queries)
        
        # æ ¹æ®ä¸Šä¸‹æ–‡ç”ŸæˆæŸ¥è¯¢
        if "function_type" in context:
            function_type = context["function_type"]
            queries_to_search.append(f"{function_type} åŠŸèƒ½åˆ†ç±»è§„åˆ™")
            queries_to_search.append(f"{function_type} å®šä¹‰å’Œç‰¹å¾")
        
        if "complexity" in context:
            queries_to_search.append("å¤æ‚åº¦è®¡ç®—æ–¹æ³•")
            queries_to_search.append("æ•°æ®å…ƒç´ è®¡ç®—è§„åˆ™")
        
        if "data_elements" in context:
            queries_to_search.append("æ•°æ®å…ƒç´ è¯†åˆ«è§„åˆ™")
            queries_to_search.append("è®°å½•å…ƒç´ ç±»å‹åˆ¤å®š")
        
        # å¹¶è¡Œæ£€ç´¢
        results = {}
        tasks = []
        
        for query in queries_to_search:
            task = self.retrieve_rules(
                query=query,
                standard=context.get("standard"),
                min_chunks=2
            )
            tasks.append((query, task))
        
        if tasks:
            completed_tasks = await asyncio.gather(
                *[task for _, task in tasks],
                return_exceptions=True
            )
            
            for (query, _), result in zip(tasks, completed_tasks):
                if isinstance(result, Exception):
                    logger.error(f"âŒ ä¸Šä¸‹æ–‡æ£€ç´¢å¤±è´¥ '{query}': {str(result)}")
                else:
                    results[query] = result
        
        return results
    
    async def get_retrieval_statistics(self) -> Dict[str, Any]:
        """è·å–æ£€ç´¢ç»Ÿè®¡ä¿¡æ¯"""
        
        if not self.retrieval_history:
            return {
                "total_retrievals": 0,
                "average_chunks": 0,
                "average_time_ms": 0,
                "success_rate": 0.0
            }
        
        total_retrievals = len(self.retrieval_history)
        total_chunks = sum(len(r.retrieved_chunks) for r in self.retrieval_history)
        total_time = sum(r.retrieval_time_ms for r in self.retrieval_history)
        successful_retrievals = sum(
            1 for r in self.retrieval_history 
            if r.retrieved_chunks and len(r.retrieved_chunks) > 0
        )
        
        return {
            "total_retrievals": total_retrievals,
            "average_chunks": total_chunks / total_retrievals,
            "average_time_ms": total_time / total_retrievals,
            "success_rate": successful_retrievals / total_retrievals,
            "cache_hits": len(self.retrieval_cache),
            "standards_distribution": self._get_standards_distribution()
        }
    
    def _get_standards_distribution(self) -> Dict[str, int]:
        """è·å–æ ‡å‡†åˆ†å¸ƒç»Ÿè®¡"""
        distribution = {}
        for result in self.retrieval_history:
            standard = result.source_type.value
            distribution[standard] = distribution.get(standard, 0) + 1
        return distribution
    
    async def clear_cache(self):
        """æ¸…é™¤æ£€ç´¢ç¼“å­˜"""
        self.retrieval_cache.clear()
        logger.info("ğŸ—‘ï¸ æ£€ç´¢ç¼“å­˜å·²æ¸…é™¤")
    
    async def export_retrieval_history(self) -> List[Dict[str, Any]]:
        """å¯¼å‡ºæ£€ç´¢å†å²"""
        return [
            {
                "query": result.query,
                "source_type": result.source_type.value,
                "chunks_count": len(result.retrieved_chunks),
                "retrieval_time_ms": result.retrieval_time_ms,
                "timestamp": datetime.now().isoformat(),
                "validation_score": (
                    result.validation_result.validation_score 
                    if result.validation_result else None
                )
            }
            for result in self.retrieval_history
        ]


async def create_rule_retriever_agent(
    llm: BaseLanguageModel,
    embeddings: Embeddings,
    vector_store: PgVectorStore
) -> RuleRetrieverAgent:
    """åˆ›å»ºè§„åˆ™æ£€ç´¢æ™ºèƒ½ä½“"""
    
    agent = RuleRetrieverAgent(
        llm=llm,
        embeddings=embeddings,
        vector_store=vector_store
    )
    
    await agent.initialize()
    return agent


if __name__ == "__main__":
    async def main():
        # æµ‹è¯•è§„åˆ™æ£€ç´¢æ™ºèƒ½ä½“
        from knowledge_base.vector_stores.pgvector_store import PgVectorStore
        
        # åˆå§‹åŒ–å‘é‡ç®¡ç†å™¨
        vector_store = PgVectorStore()
        await vector_store.initialize()
        
        # åˆ›å»ºè§„åˆ™æ£€ç´¢æ™ºèƒ½ä½“
        agent = await create_rule_retriever_agent(vector_store.llm, vector_store.embeddings, vector_store)
        
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "NESMA å†…éƒ¨é€»è¾‘æ–‡ä»¶åˆ†ç±»æ ‡å‡†",
            "COSMIC æ•°æ®ç§»åŠ¨è¯†åˆ«è§„åˆ™",
            "åŠŸèƒ½å¤æ‚åº¦è®¡ç®—å…¬å¼"
        ]
        
        for query in test_queries:
            print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢: {query}")
            
            # æ‰§è¡Œæ£€ç´¢
            result = await agent.retrieve_rules(
                query=query,
                standard=EstimationStandard.BOTH,
                min_chunks=2
            )
            
            print(f"ğŸ“Š æ£€ç´¢ç»“æœ: {len(result.retrieved_chunks)} ä¸ªæ–‡æ¡£å—")
            print(f"â±ï¸ æ£€ç´¢è€—æ—¶: {result.retrieval_time_ms}ms")
            
            if result.validation_result:
                print(f"âœ… éªŒè¯ç»“æœ: {result.validation_result.confidence_level.value}")
                print(f"ğŸ“ˆ è´¨é‡åˆ†æ•°: {result.validation_result.validation_score:.3f}")
            
            if result.retrieved_chunks:
                best_chunk = result.retrieved_chunks[0]
                print(f"ğŸ¯ æœ€ä½³åŒ¹é… (åˆ†æ•°: {best_chunk.relevance_score:.3f})")
                print(f"   å†…å®¹é¢„è§ˆ: {best_chunk.content[:200]}...")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = await agent.get_retrieval_statistics()
        print(f"\nğŸ“Š æ£€ç´¢ç»Ÿè®¡:")
        for key, value in stats.items():
            print(f"  {key}: {value}")
        
        await vector_store.close()
    
    asyncio.run(main()) 