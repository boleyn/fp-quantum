"""
é‡å­æ™ºèƒ½åŒ–åŠŸèƒ½ç‚¹ä¼°ç®—ç³»ç»Ÿ - è§„åˆ™æ£€ç´¢æ™ºèƒ½ä½“

åŸºäºPgVectorå®ç°æ™ºèƒ½çŸ¥è¯†æ£€ç´¢å’ŒéªŒè¯
"""

import asyncio
import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_core.language_models import BaseLanguageModel
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.tools import tool
from pydantic import BaseModel, Field

from agents.base.base_agent import BaseAgent
from models.common_models import EstimationStandard, KnowledgeQuery, KnowledgeResult, ValidationResult, ConfidenceLevel
from knowledge_base.vector_stores.pgvector_store import PgVectorStore
from knowledge_base.embeddings.embedding_models import get_embedding_model
from knowledge_base.auto_setup import ensure_knowledge_base_ready
import logging

logger = logging.getLogger(__name__)

# ç»“æ„åŒ–è¾“å‡ºæ¨¡å‹
class RelevanceValidationResult(BaseModel):
    """ç›¸å…³æ€§éªŒè¯ç»“æœæ¨¡å‹"""
    relevant: bool = Field(description="å†…å®¹æ˜¯å¦ç›¸å…³")
    partially_relevant: bool = Field(description="å†…å®¹æ˜¯å¦éƒ¨åˆ†ç›¸å…³", default=False)
    reasoning: str = Field(description="è¯¦ç»†è¯´æ˜")


class RuleRetrieverAgent(BaseAgent):
    """è§„åˆ™æ£€ç´¢æ™ºèƒ½ä½“ - åŸºäºPgVector"""
    
    def __init__(
        self,
        llm: BaseLanguageModel,
        embeddings: Embeddings,
        vector_store: PgVectorStore,
        agent_name: str = "RuleRetriever"
    ):
        super().__init__(agent_name, llm)  # ä¿®å¤ï¼šå‚æ•°é¡ºåºåº”è¯¥æ˜¯agent_id, llm
        self.embeddings = embeddings
        self.vector_store = vector_store
        
        # æ£€ç´¢é…ç½®
        self.retrieval_config = {
            "default_k": 5,
            "relevance_threshold": 0.7,
            "max_retries": 3
        }

    async def initialize(self):
        """åˆå§‹åŒ–æ™ºèƒ½ä½“"""
        await super().initialize()
        
        # æ£€æŸ¥çŸ¥è¯†åº“çŠ¶æ€
        logger.info("ğŸ” æ£€æŸ¥çŸ¥è¯†åº“çŠ¶æ€...")
        try:
            kb_ready = await ensure_knowledge_base_ready()
            if kb_ready:
                logger.info("âœ… çŸ¥è¯†åº“å·²å°±ç»ª")
            else:
                logger.warning("âš ï¸ çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥ï¼Œä½†ç»§ç»­è¿è¡Œ")
        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†åº“æ£€æŸ¥å¤±è´¥: {e}")
        
        logger.info("âœ… è§„åˆ™æ£€ç´¢æ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
    
    async def retrieve_rules(
        self,
        query: str,
        standard: Optional[EstimationStandard] = None,
        use_cache: bool = True,
        min_chunks: int = 3,
        max_retries: int = 2
    ) -> KnowledgeResult:
        """æ£€ç´¢ç›¸å…³è§„åˆ™"""
        
        start_time = datetime.now()
        
        # ğŸ”¥ å¢åŠ è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—
        logger.info(f"ğŸ” å¼€å§‹æ£€ç´¢è§„åˆ™:")
        logger.info(f"  - æŸ¥è¯¢: {query}")
        logger.info(f"  - æ ‡å‡†: {standard}")
        logger.info(f"  - ä½¿ç”¨ç¼“å­˜: {use_cache}")
        logger.info(f"  - æœ€å°å—æ•°: {min_chunks}")
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = f"{query}_{standard}_{min_chunks}"
        
        # æ£€æŸ¥ç¼“å­˜
        if use_cache and hasattr(self, 'retrieval_cache') and cache_key in self.retrieval_cache:
            cached_result = self.retrieval_cache[cache_key]
            logger.info(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜ç»“æœ: {query}")
            return cached_result
        
        # åˆå§‹åŒ–ç¼“å­˜å’Œå†å²è®°å½•
        if not hasattr(self, 'retrieval_cache'):
            self.retrieval_cache = {}
        if not hasattr(self, 'retrieval_history'):
            self.retrieval_history = []
        
        try:
            # ğŸ”¥ ä¼˜åŒ–ï¼šé¦–å…ˆå°è¯•ä»ç®€å•çŸ¥è¯†åº“æ£€ç´¢ï¼Œå¢åŠ è¯¦ç»†æ—¥å¿—
            logger.info("ğŸ“š å°è¯•ä»ç®€å•çŸ¥è¯†åº“æ£€ç´¢...")
            retrieval_result = await self._retrieve_from_simple_kb(query, standard)
            
            logger.info(f"ğŸ“Š ç®€å•çŸ¥è¯†åº“æ£€ç´¢ç»“æœ: {retrieval_result.total_chunks} ä¸ªå—")
            if retrieval_result.retrieved_chunks:
                for i, chunk in enumerate(retrieval_result.retrieved_chunks):
                    logger.info(f"  å—{i+1}: ç›¸å…³æ€§={chunk.get('relevance_score', 0):.3f}, å†…å®¹é•¿åº¦={len(chunk.get('content', ''))}")
            
            # å¦‚æœç®€å•çŸ¥è¯†åº“æ²¡æœ‰ç»“æœï¼Œå°è¯•å‘é‡æ£€ç´¢
            if not retrieval_result.retrieved_chunks and hasattr(self, 'vector_store'):
                logger.info("ğŸ” ç®€å•çŸ¥è¯†åº“æ— ç»“æœï¼Œå°è¯•å‘é‡æ£€ç´¢...")
                retrieval_result = await self._retrieve_from_vector_store(query, standard, min_chunks)
                logger.info(f"ğŸ“Š å‘é‡æ£€ç´¢ç»“æœ: {retrieval_result.total_chunks} ä¸ªå—")
            
        except Exception as e:
            logger.error(f"âŒ æ£€ç´¢å¤±è´¥: {str(e)}")
            import traceback
            logger.error(f"âŒ æ£€ç´¢å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")
            # è¿”å›ç©ºç»“æœ
            retrieval_result = KnowledgeResult(
                query=query,
                source_type=standard or EstimationStandard.BOTH,
                retrieved_chunks=[],
                total_chunks=0,
                processing_time_ms=int((datetime.now() - start_time).total_seconds() * 1000)
            )
        
        # éªŒè¯æ£€ç´¢ç»“æœ
        validation_result = None
        if retrieval_result and retrieval_result.retrieved_chunks:
            validation_result = await self._validate_retrieval_result(retrieval_result)
            # ä¸è®¾ç½®validation_resultåˆ°å¯¹è±¡ä¸Šï¼Œå› ä¸ºKnowledgeResultæ¨¡å‹æ²¡æœ‰è¿™ä¸ªå­—æ®µ
        
        # ç¼“å­˜ç»“æœ
        if use_cache and retrieval_result:
            self.retrieval_cache[cache_key] = retrieval_result
        
        # è®°å½•æ£€ç´¢å†å²
        if retrieval_result:
            self.retrieval_history.append(retrieval_result)
        
        # ğŸ”¥ è®°å½•æœ€ç»ˆç»“æœç»Ÿè®¡
        logger.info(f"âœ… è§„åˆ™æ£€ç´¢å®Œæˆ:")
        logger.info(f"  - æ€»å—æ•°: {retrieval_result.total_chunks}")
        logger.info(f"  - æ£€ç´¢ç”¨æ—¶: {getattr(retrieval_result, 'processing_time_ms', 0)}ms")
        if validation_result:
            logger.info(f"  - éªŒè¯åˆ†æ•°: {validation_result.confidence_score:.3f}")
        
        return retrieval_result
    
    async def _retrieve_from_simple_kb(
        self,
        query: str,
        standard: Optional[EstimationStandard] = None
    ) -> KnowledgeResult:
        """ä»ç®€å•çŸ¥è¯†åº“JSONæ–‡ä»¶æ£€ç´¢"""
        
        import json
        from pathlib import Path
        
        start_time = datetime.now()
        
        try:
            # åŠ è½½ç®€å•çŸ¥è¯†åº“
            kb_file = Path("knowledge_base/simple_kb.json")
            if not kb_file.exists():
                logger.warning("âš ï¸ ç®€å•çŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨")
                return KnowledgeResult(
                    query=query,
                    source_type=standard or EstimationStandard.BOTH,
                    retrieved_chunks=[],
                    total_chunks=0,
                    processing_time_ms=0
                )
            
            with open(kb_file, 'r', encoding='utf-8') as f:
                knowledge_entries = json.load(f)
            
            logger.info(f"ğŸ“š åŠ è½½äº† {len(knowledge_entries)} æ¡çŸ¥è¯†åº“æ¡ç›®")
            
            # ğŸ”¥ è®°å½•æœç´¢å‚æ•°
            logger.info(f"ğŸ” æœç´¢å‚æ•°è¯¦æƒ…:")
            logger.info(f"  - æŸ¥è¯¢è¯: '{query}'")
            logger.info(f"  - æ ‡å‡†è¿‡æ»¤: {standard}")
            
            # ç®€å•çš„å…³é”®è¯åŒ¹é…æ£€ç´¢
            matched_chunks = []
            query_lower = query.lower()
            
            # ğŸ”¥ åˆ†ææŸ¥è¯¢å…³é”®è¯
            query_keywords = query_lower.split()
            logger.info(f"  - å…³é”®è¯åˆ—è¡¨: {query_keywords}")
            
            for i, entry in enumerate(knowledge_entries):
                content = entry.get('content', '')
                metadata = entry.get('metadata', {})
                
                # æ£€æŸ¥æ ‡å‡†è¿‡æ»¤
                if standard == EstimationStandard.NESMA and metadata.get('type') != 'nesma_rules':
                    continue
                elif standard == EstimationStandard.COSMIC and metadata.get('type') != 'cosmic_rules':
                    continue
                
                # ç®€å•çš„ç›¸å…³æ€§è¯„åˆ†
                relevance_score = 0.0
                match_details = []
                
                # å…³é”®è¯åŒ¹é…
                content_lower = content.lower()
                keyword_matches = []
                for keyword in query_keywords:
                    if keyword in content_lower:
                        keyword_matches.append(keyword)
                        relevance_score += 0.5 / len(query_keywords)
                
                if keyword_matches:
                    match_details.append(f"å…³é”®è¯åŒ¹é…: {keyword_matches}")
                
                # åŠŸèƒ½ç±»å‹åŒ¹é…
                if 'function_type' in metadata:
                    if metadata['function_type'].lower() in query_lower:
                        relevance_score += 0.4
                        match_details.append(f"åŠŸèƒ½ç±»å‹åŒ¹é…: {metadata['function_type']}")
                
                # ç±»å‹åŒ¹é…
                if metadata.get('type') == 'nesma_rules' and 'nesma' in query_lower:
                    relevance_score += 0.3
                    match_details.append("NESMAç±»å‹åŒ¹é…")
                
                # ğŸ”¥ è®°å½•åŒ¹é…åˆ†æè¿‡ç¨‹
                if relevance_score > 0:
                    logger.info(f"  æ¡ç›®{i+1} åŒ¹é…å¾—åˆ†: {relevance_score:.3f}")
                    logger.info(f"    åŒ¹é…è¯¦æƒ…: {match_details}")
                    logger.info(f"    å†…å®¹é¢„è§ˆ: {content[:100]}...")
                    
                    matched_chunks.append({
                        'content': content,
                        'metadata': metadata,
                        'relevance_score': min(1.0, relevance_score),
                        'chunk_id': entry.get('id', f'kb_entry_{i}'),
                        'match_details': match_details
                    })
            
            # æŒ‰ç›¸å…³æ€§æ’åº
            matched_chunks.sort(key=lambda x: x['relevance_score'], reverse=True)
            
            # é™åˆ¶ç»“æœæ•°é‡
            matched_chunks = matched_chunks[:5]
            
            processing_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)
            
            logger.info(f"ğŸ” ç®€å•çŸ¥è¯†åº“æ£€ç´¢å®Œæˆ: æ‰¾åˆ° {len(matched_chunks)} ä¸ªåŒ¹é…ç»“æœ")
            
            # ğŸ”¥ è®°å½•æœ€ç»ˆåŒ¹é…ç»“æœè¯¦æƒ…
            for i, chunk in enumerate(matched_chunks):
                logger.info(f"  æœ€ç»ˆç»“æœ{i+1}: åˆ†æ•°={chunk['relevance_score']:.3f}, ID={chunk['chunk_id']}")
            
            return KnowledgeResult(
                query=query,
                source_type=standard or EstimationStandard.BOTH,
                retrieved_chunks=matched_chunks,
                total_chunks=len(matched_chunks),
                processing_time_ms=processing_time_ms
            )
            
        except Exception as e:
            logger.error(f"âŒ ç®€å•çŸ¥è¯†åº“æ£€ç´¢å¤±è´¥: {str(e)}")
            import traceback
            logger.error(f"âŒ å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")
            return KnowledgeResult(
                query=query,
                source_type=standard or EstimationStandard.BOTH,
                retrieved_chunks=[],
                total_chunks=0,
                processing_time_ms=int((datetime.now() - start_time).total_seconds() * 1000)
            )
    
    async def _retrieve_from_vector_store(
        self,
        query: str,
        standard: Optional[EstimationStandard] = None,
        min_chunks: int = 3
    ) -> KnowledgeResult:
        """ä»å‘é‡å­˜å‚¨æ£€ç´¢ï¼ˆLangChain vector as retrieverï¼‰"""
        
        start_time = datetime.now()
        
        try:
            logger.info("ğŸ” å¼€å§‹å‘é‡å­˜å‚¨æ£€ç´¢...")
            logger.info(f"  - æŸ¥è¯¢: {query}")
            logger.info(f"  - æ ‡å‡†: {standard}")
            logger.info(f"  - æœ€å°å—æ•°: {min_chunks}")
            
            # æ£€æŸ¥å‘é‡å­˜å‚¨æ˜¯å¦å¯ç”¨
            if not self.vector_store:
                logger.warning("âš ï¸ å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–")
                return KnowledgeResult(
                    query=query,
                    source_type=standard or EstimationStandard.BOTH,
                    retrieved_chunks=[],
                    total_chunks=0,
                    processing_time_ms=0
                )
            
            # ç¡®å®šæœç´¢æºç±»å‹
            source_type = None
            if standard == EstimationStandard.NESMA:
                source_type = "nesma"
            elif standard == EstimationStandard.COSMIC:
                source_type = "cosmic"
            
            logger.info(f"ğŸ“‹ å‘é‡æ£€ç´¢æºç±»å‹: {source_type}")
            
            # ä½¿ç”¨å‘é‡å­˜å‚¨çš„similarity_search_with_scoreæ–¹æ³•
            results_with_scores = await self.vector_store.similarity_search_with_score(
                query=query,
                k=max(min_chunks, 5),
                filter={"source_type": standard.value} if standard else None
            )
            
            logger.info(f"ğŸ“Š å‘é‡æ£€ç´¢è¿”å›: {len(results_with_scores)} ä¸ªæ–‡æ¡£")
            
            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            retrieved_chunks = []
            for i, (doc, score) in enumerate(results_with_scores):
                chunk = {
                    'content': doc.page_content,
                    'metadata': doc.metadata or {},
                    'relevance_score': float(1.0 - score),  # è½¬æ¢è·ç¦»ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
                    'chunk_id': doc.metadata.get('id', f'vector_chunk_{i}'),
                    'source': 'vector_store'
                }
                retrieved_chunks.append(chunk)
                
                logger.info(f"  å‘é‡å—{i+1}: ID={chunk['chunk_id']}, åˆ†æ•°={chunk['relevance_score']:.3f}, é•¿åº¦={len(chunk['content'])}")
            
            processing_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)
            
            logger.info(f"âœ… å‘é‡æ£€ç´¢å®Œæˆ: {len(retrieved_chunks)} ä¸ªç»“æœï¼Œè€—æ—¶ {processing_time_ms}ms")
            
            return KnowledgeResult(
                query=query,
                source_type=standard or EstimationStandard.BOTH,
                retrieved_chunks=retrieved_chunks,
                total_chunks=len(retrieved_chunks),
                processing_time_ms=processing_time_ms
            )
            
        except Exception as e:
            logger.error(f"âŒ å‘é‡å­˜å‚¨æ£€ç´¢å¤±è´¥: {str(e)}")
            import traceback
            logger.error(f"âŒ å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")
            
            # å¦‚æœå‘é‡æ£€ç´¢å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨retrieveræ¥å£
            try:
                logger.info("ğŸ”„ å°è¯•ä½¿ç”¨retrieveræ¥å£...")
                
                # ä½¿ç”¨LangChainçš„vector as retrieverè¿›è¡Œæ£€ç´¢
                search_kwargs = {"k": max(min_chunks, 5)}
                if standard:
                    search_kwargs["filter"] = {"source_type": standard.value}
                
                retriever = self.vector_store.as_retriever(
                    source_type=source_type,
                    search_kwargs=search_kwargs
                )
                
                # æ‰§è¡Œå¼‚æ­¥æ£€ç´¢
                documents = await retriever.ainvoke(query)
                
                logger.info(f"ğŸ“Š Retrieveræ¥å£è¿”å›: {len(documents)} ä¸ªæ–‡æ¡£")
                
                # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                retrieved_chunks = []
                for i, doc in enumerate(documents):
                    chunk = {
                        'content': doc.page_content,
                        'metadata': doc.metadata or {},
                        'relevance_score': 0.8,  # é»˜è®¤ç›¸å…³æ€§åˆ†æ•°
                        'chunk_id': doc.metadata.get('id', f'retriever_chunk_{i}'),
                        'source': 'retriever'
                    }
                    retrieved_chunks.append(chunk)
                    
                    logger.info(f"  æ£€ç´¢å—{i+1}: ID={chunk['chunk_id']}, é•¿åº¦={len(chunk['content'])}")
                
                processing_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)
                
                logger.info(f"âœ… Retrieveræ£€ç´¢å®Œæˆ: {len(retrieved_chunks)} ä¸ªç»“æœï¼Œè€—æ—¶ {processing_time_ms}ms")
                
                return KnowledgeResult(
                    query=query,
                    source_type=standard or EstimationStandard.BOTH,
                    retrieved_chunks=retrieved_chunks,
                    total_chunks=len(retrieved_chunks),
                    processing_time_ms=processing_time_ms
                )
                
            except Exception as retriever_error:
                logger.error(f"âŒ Retrieveræ¥å£ä¹Ÿå¤±è´¥: {str(retriever_error)}")
                
                return KnowledgeResult(
                    query=query,
                    source_type=standard or EstimationStandard.BOTH,
                    retrieved_chunks=[],
                    total_chunks=0,
                    processing_time_ms=int((datetime.now() - start_time).total_seconds() * 1000)
                )
    
    async def _validate_retrieval_result(self, retrieval_result: KnowledgeResult) -> ValidationResult:
        """éªŒè¯æ£€ç´¢ç»“æœçš„è´¨é‡"""
        
        if not retrieval_result.retrieved_chunks:
                    return ValidationResult(
            is_valid=False,
            confidence_score=0.0,
            confidence_level=ConfidenceLevel.LOW,
            errors=["æ£€ç´¢ç»“æœä¸ºç©º"],
            warnings=[],
            suggestions=["å°è¯•ä¸åŒçš„æŸ¥è¯¢è¯", "æ£€æŸ¥çŸ¥è¯†åº“å†…å®¹"],
            metadata={}
        )
        
        # åŸºç¡€è´¨é‡æŒ‡æ ‡
        total_chunks = len(retrieval_result.retrieved_chunks)
        
        # å¤„ç†ä¸åŒç±»å‹çš„chunkæ ¼å¼ï¼ˆå­—å…¸æˆ–å¯¹è±¡ï¼‰
        def get_relevance_score(chunk):
            if isinstance(chunk, dict):
                return chunk.get('relevance_score', 0.0)
            else:
                return getattr(chunk, 'relevance_score', 0.0)
        
        high_relevance_chunks = [
            chunk for chunk in retrieval_result.retrieved_chunks 
            if get_relevance_score(chunk) > 0.8
        ]
        medium_relevance_chunks = [
            chunk for chunk in retrieval_result.retrieved_chunks 
            if 0.6 <= get_relevance_score(chunk) <= 0.8
        ]
        
        # è®¡ç®—æ€»ä½“è´¨é‡åˆ†æ•°
        quality_score = 0.0
        if total_chunks > 0:
            quality_score = (
                len(high_relevance_chunks) * 1.0 + 
                len(medium_relevance_chunks) * 0.7
            ) / total_chunks
        
        # ä½¿ç”¨LLMè¿›è¡Œæ·±åº¦éªŒè¯
        llm_validation = await self._llm_validate_relevance(retrieval_result)
        
        # ç»¼åˆåˆ¤æ–­
        issues = []
        suggestions = []
        
        if total_chunks < 2:
            issues.append("æ£€ç´¢ç»“æœæ•°é‡è¾ƒå°‘")
            suggestions.append("æ‰©å¤§æ£€ç´¢èŒƒå›´æˆ–è°ƒæ•´æŸ¥è¯¢ç­–ç•¥")
        
        if len(high_relevance_chunks) == 0:
            issues.append("ç¼ºä¹é«˜ç›¸å…³æ€§å†…å®¹")
            suggestions.append("ä¼˜åŒ–æŸ¥è¯¢è¡¨è¾¾æˆ–æ£€æŸ¥çŸ¥è¯†åº“å®Œæ•´æ€§")
        
        # ç¡®å®šç½®ä¿¡åº¦
        if quality_score >= 0.8 and llm_validation.get("relevant", False):
            confidence_level = ConfidenceLevel.HIGH
            is_valid = True
        elif quality_score >= 0.6 and llm_validation.get("partially_relevant", False):
            confidence_level = ConfidenceLevel.MEDIUM
            is_valid = True
        else:
            confidence_level = ConfidenceLevel.LOW
            is_valid = len(retrieval_result.retrieved_chunks) > 0
        
        return ValidationResult(
            is_valid=is_valid,
            confidence_score=quality_score,
            confidence_level=confidence_level,
            errors=issues,
            warnings=[],
            suggestions=suggestions,
            metadata={
                "total_chunks": total_chunks,
                "high_relevance_count": len(high_relevance_chunks),
                "medium_relevance_count": len(medium_relevance_chunks),
                "llm_validation": llm_validation
            }
        )
    
    async def _llm_validate_relevance(self, retrieval_result: KnowledgeResult) -> Dict[str, Any]:
        """ä½¿ç”¨LLMéªŒè¯æ£€ç´¢ç»“æœçš„ç›¸å…³æ€§"""
        
        if not self.llm or not retrieval_result.retrieved_chunks:
            return {"relevant": False, "reasoning": "LLMæœªåˆå§‹åŒ–æˆ–æ— æ£€ç´¢ç»“æœ"}
        
        # å®šä¹‰éªŒè¯å·¥å…·
        @tool
        def validate_relevance(
            relevant: bool,
            partially_relevant: bool,
            reasoning: str
        ) -> dict:
            """éªŒè¯æ£€ç´¢å†…å®¹çš„ç›¸å…³æ€§
            
            Args:
                relevant: å†…å®¹æ˜¯å¦ä¸æŸ¥è¯¢ç›¸å…³
                partially_relevant: å†…å®¹æ˜¯å¦éƒ¨åˆ†ç›¸å…³
                reasoning: è¯¦ç»†çš„éªŒè¯è¯´æ˜
            """
            return {
                "relevant": relevant,
                "partially_relevant": partially_relevant,
                "reasoning": reasoning
            }
        
        # åˆ›å»ºå¸¦å·¥å…·çš„LLM
        llm_with_tools = self.llm.bind_tools([validate_relevance])
        
        # å¤„ç†ä¸åŒç±»å‹çš„chunkæ ¼å¼ï¼ˆå­—å…¸æˆ–å¯¹è±¡ï¼‰
        def get_chunk_content(chunk):
            if isinstance(chunk, dict):
                return chunk.get('content', '')
            else:
                return getattr(chunk, 'content', '')
        
        # è·å–å‰3ä¸ªæœ€ç›¸å…³çš„å—
        top_chunks = retrieval_result.retrieved_chunks[:3]
        chunks_text = "\n\n---\n\n".join([
            f"æ–‡æ¡£{i+1}: {get_chunk_content(chunk)[:300]}..."
            for i, chunk in enumerate(top_chunks)
        ])
        
        validation_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯çŸ¥è¯†æ£€ç´¢è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹æ˜¯å¦ä¸ç”¨æˆ·æŸ¥è¯¢ç›¸å…³ã€‚

è¯„ä¼°æ ‡å‡†ï¼š
1. å†…å®¹æ˜¯å¦ç›´æ¥å›ç­”äº†æŸ¥è¯¢é—®é¢˜
2. ä¿¡æ¯æ˜¯å¦å‡†ç¡®å’Œæœ‰ç”¨
3. æ˜¯å¦åŒ…å«æŸ¥è¯¢æ‰€éœ€çš„å…³é”®ä¿¡æ¯

è¯·ä½¿ç”¨validate_relevanceå·¥å…·è¿”å›è¯„ä¼°ç»“æœã€‚"""),
            ("human", """æŸ¥è¯¢: {query}

æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹:
{chunks_text}

è¯·è¯„ä¼°è¿™äº›å†…å®¹çš„ç›¸å…³æ€§ã€‚""")
        ])
        
        try:
            response = await llm_with_tools.ainvoke(
                validation_prompt.format_messages(
                    query=retrieval_result.query,
                    chunks_text=chunks_text
                )
            )
            
            # è§£æå·¥å…·è°ƒç”¨ç»“æœ
            if response.tool_calls:
                tool_call = response.tool_calls[0]
                return tool_call["args"]
            else:
                logger.warning("LLMæœªä½¿ç”¨å·¥å…·è°ƒç”¨ï¼Œè¿”å›é»˜è®¤ç»“æœ")
                return {
                    "relevant": True,
                    "reasoning": "LLMæœªä½¿ç”¨å·¥å…·è°ƒç”¨ï¼Œé»˜è®¤ä¸ºç›¸å…³"
                }
            
        except Exception as e:
            logger.warning(f"âš ï¸ LLMéªŒè¯å¤±è´¥: {str(e)}")
            return {
                "relevant": False,
                "reasoning": f"LLMéªŒè¯å‡ºé”™: {str(e)}"
            }
    
    async def retrieve_by_context(
        self,
        context: Dict[str, Any],
        specific_queries: Optional[List[str]] = None
    ) -> Dict[str, KnowledgeResult]:
        """æ ¹æ®ä¸Šä¸‹æ–‡è¿›è¡Œå¤šæ–¹é¢æ£€ç´¢"""
        
        # æå–æ£€ç´¢æŸ¥è¯¢
        queries_to_search = []
        
        if specific_queries:
            queries_to_search.extend(specific_queries)
        
        # æ ¹æ®ä¸Šä¸‹æ–‡ç”ŸæˆæŸ¥è¯¢
        if "function_type" in context:
            function_type = context["function_type"]
            queries_to_search.append(f"{function_type} åŠŸèƒ½åˆ†ç±»è§„åˆ™")
            queries_to_search.append(f"{function_type} å®šä¹‰å’Œç‰¹å¾")
        
        if "complexity" in context:
            queries_to_search.append("å¤æ‚åº¦è®¡ç®—æ–¹æ³•")
            queries_to_search.append("æ•°æ®å…ƒç´ è®¡ç®—è§„åˆ™")
        
        if "data_elements" in context:
            queries_to_search.append("æ•°æ®å…ƒç´ è¯†åˆ«è§„åˆ™")
            queries_to_search.append("è®°å½•å…ƒç´ ç±»å‹åˆ¤å®š")
        
        # å¹¶è¡Œæ£€ç´¢
        results = {}
        tasks = []
        
        for query in queries_to_search:
            task = self.retrieve_rules(
                query=query,
                standard=context.get("standard"),
                min_chunks=2
            )
            tasks.append((query, task))
        
        if tasks:
            completed_tasks = await asyncio.gather(
                *[task for _, task in tasks],
                return_exceptions=True
            )
            
            for (query, _), result in zip(tasks, completed_tasks):
                if isinstance(result, Exception):
                    logger.error(f"âŒ ä¸Šä¸‹æ–‡æ£€ç´¢å¤±è´¥ '{query}': {str(result)}")
                else:
                    results[query] = result
        
        return results
    
    async def get_retrieval_statistics(self) -> Dict[str, Any]:
        """è·å–æ£€ç´¢ç»Ÿè®¡ä¿¡æ¯"""
        
        if not self.retrieval_history:
            return {
                "total_retrievals": 0,
                "average_chunks": 0,
                "average_time_ms": 0,
                "success_rate": 0.0
            }
        
        total_retrievals = len(self.retrieval_history)
        total_chunks = sum(len(r.retrieved_chunks) for r in self.retrieval_history)
        total_time = sum(r.processing_time_ms for r in self.retrieval_history)
        successful_retrievals = sum(
            1 for r in self.retrieval_history 
            if r.retrieved_chunks and len(r.retrieved_chunks) > 0
        )
        
        return {
            "total_retrievals": total_retrievals,
            "average_chunks": total_chunks / total_retrievals,
            "average_time_ms": total_time / total_retrievals,
            "success_rate": successful_retrievals / total_retrievals,
            "cache_hits": len(self.retrieval_cache),
            "standards_distribution": self._get_standards_distribution()
        }
    
    def _get_standards_distribution(self) -> Dict[str, int]:
        """è·å–æ ‡å‡†åˆ†å¸ƒç»Ÿè®¡"""
        distribution = {}
        for result in self.retrieval_history:
            standard = result.source_type.value
            distribution[standard] = distribution.get(standard, 0) + 1
        return distribution
    
    async def clear_cache(self):
        """æ¸…é™¤æ£€ç´¢ç¼“å­˜"""
        self.retrieval_cache.clear()
        logger.info("ğŸ—‘ï¸ æ£€ç´¢ç¼“å­˜å·²æ¸…é™¤")
    
    async def export_retrieval_history(self) -> List[Dict[str, Any]]:
        """å¯¼å‡ºæ£€ç´¢å†å²"""
        return [
            {
                "query": result.query,
                "source_type": result.source_type.value,
                "chunks_count": len(result.retrieved_chunks),
                "retrieval_time_ms": result.processing_time_ms,
                "timestamp": datetime.now().isoformat(),
                "validation_score": (
                    result.validation_result.validation_score 
                    if result.validation_result else None
                )
            }
            for result in self.retrieval_history
        ]

    def _get_capabilities(self) -> List[str]:
        """è·å–æ™ºèƒ½ä½“èƒ½åŠ›åˆ—è¡¨"""
        return [
            "è§„åˆ™æ£€ç´¢ä¸éªŒè¯",
            "çŸ¥è¯†åº“æœç´¢",
            "ä¸Šä¸‹æ–‡åŒ–æ£€ç´¢",
            "æ£€ç´¢ç»“æœè´¨é‡è¯„ä¼°",
            "å¤šæ ‡å‡†çŸ¥è¯†æºç®¡ç†"
        ]
    
    async def _execute_task(self, task_name: str, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œè§„åˆ™æ£€ç´¢ä»»åŠ¡"""
        if task_name == "retrieve_rules":
            result = await self.retrieve_rules(
                query=inputs["query"],
                standard=inputs.get("standard"),
                use_cache=inputs.get("use_cache", True),
                min_chunks=inputs.get("min_chunks", 3),
                max_retries=inputs.get("max_retries", 2)
            )
            # å°†KnowledgeResultå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
            return {
                "query": result.query,
                "source_type": result.source_type.value,
                "retrieved_chunks": result.retrieved_chunks,
                "total_chunks": result.total_chunks,
                "retrieval_time_ms": result.processing_time_ms,
                "validation_result": result.validation_result.__dict__ if result.validation_result else None
            }
        elif task_name == "retrieve_by_context":
            return await self.retrieve_by_context(
                context=inputs["context"],
                specific_queries=inputs.get("specific_queries")
            )
        elif task_name == "clear_cache":
            await self.clear_cache()
            return {"success": True, "message": "æ£€ç´¢ç¼“å­˜å·²æ¸…é™¤"}
        elif task_name == "get_statistics":
            return await self.get_retrieval_statistics()
        elif task_name == "export_history":
            return {"history": await self.export_retrieval_history()}
        else:
            raise ValueError(f"æœªçŸ¥ä»»åŠ¡: {task_name}")


async def create_rule_retriever_agent(
    llm: BaseLanguageModel,
    embeddings: Embeddings,
    vector_store: PgVectorStore
) -> RuleRetrieverAgent:
    """åˆ›å»ºè§„åˆ™æ£€ç´¢æ™ºèƒ½ä½“"""
    
    agent = RuleRetrieverAgent(
        llm=llm,
        embeddings=embeddings,
        vector_store=vector_store
    )
    
    await agent.initialize()
    return agent


if __name__ == "__main__":
    async def main():
        # æµ‹è¯•è§„åˆ™æ£€ç´¢æ™ºèƒ½ä½“
        from knowledge_base.vector_stores.pgvector_store import PgVectorStore
        
        # åˆå§‹åŒ–å‘é‡ç®¡ç†å™¨
        vector_store = PgVectorStore()
        await vector_store.initialize()
        
        # åˆ›å»ºè§„åˆ™æ£€ç´¢æ™ºèƒ½ä½“
        agent = await create_rule_retriever_agent(vector_store.llm, vector_store.embeddings, vector_store)
        
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "NESMA å†…éƒ¨é€»è¾‘æ–‡ä»¶åˆ†ç±»æ ‡å‡†",
            "COSMIC æ•°æ®ç§»åŠ¨è¯†åˆ«è§„åˆ™",
            "åŠŸèƒ½å¤æ‚åº¦è®¡ç®—å…¬å¼"
        ]
        
        for query in test_queries:
            print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢: {query}")
            
            # æ‰§è¡Œæ£€ç´¢
            result = await agent.retrieve_rules(
                query=query,
                standard=EstimationStandard.BOTH,
                min_chunks=2
            )
            
            print(f"ğŸ“Š æ£€ç´¢ç»“æœ: {len(result.retrieved_chunks)} ä¸ªæ–‡æ¡£å—")
            print(f"â±ï¸ æ£€ç´¢è€—æ—¶: {result.processing_time_ms}ms")
            
            if result.validation_result:
                print(f"âœ… éªŒè¯ç»“æœ: {result.validation_result.confidence_level.value}")
                print(f"ğŸ“ˆ è´¨é‡åˆ†æ•°: {result.validation_result.validation_score:.3f}")
            
            if result.retrieved_chunks:
                best_chunk = result.retrieved_chunks[0]
                # å¤„ç†ä¸åŒç±»å‹çš„chunkæ ¼å¼ï¼ˆå­—å…¸æˆ–å¯¹è±¡ï¼‰
                def get_chunk_content(chunk):
                    if isinstance(chunk, dict):
                        return chunk.get('content', '')
                    else:
                        return getattr(chunk, 'content', '')
                
                def get_relevance_score(chunk):
                    if isinstance(chunk, dict):
                        return chunk.get('relevance_score', 0.0)
                    else:
                        return getattr(chunk, 'relevance_score', 0.0)
                
                print(f"ğŸ¯ æœ€ä½³åŒ¹é… (åˆ†æ•°: {get_relevance_score(best_chunk):.3f})")
                print(f"   å†…å®¹é¢„è§ˆ: {get_chunk_content(best_chunk)[:200]}...")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = await agent.get_retrieval_statistics()
        print(f"\nğŸ“Š æ£€ç´¢ç»Ÿè®¡:")
        for key, value in stats.items():
            print(f"  {key}: {value}")
        
        await vector_store.close()
    
    asyncio.run(main()) 