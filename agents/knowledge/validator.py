"""
é‡å­æ™ºèƒ½åŒ–åŠŸèƒ½ç‚¹ä¼°ç®—ç³»ç»Ÿ - è´¨é‡éªŒè¯æ™ºèƒ½ä½“

è´Ÿè´£éªŒè¯æ£€ç´¢åˆ°çš„çŸ¥è¯†å’Œåˆ†æç»“æœçš„è´¨é‡
"""

import asyncio
import time
from typing import Dict, Any, Optional, List, Tuple
import logging
from datetime import datetime

from langchain_core.language_models import BaseLanguageModel
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document

from agents.base.base_agent import SpecializedAgent
from models.common_models import ValidationResult, ConfidenceLevel
from config.settings import get_settings

logger = logging.getLogger(__name__)


class ValidatorAgent(SpecializedAgent):
    """è´¨é‡éªŒè¯æ™ºèƒ½ä½“"""
    
    def __init__(self, llm: Optional[BaseLanguageModel] = None):
        super().__init__(
            agent_id="validator",
            specialty="quality_assurance",
            llm=llm
        )
        
        self.settings = get_settings()
        
        # éªŒè¯è§„åˆ™å’Œæ ‡å‡†
        self.validation_criteria = self._load_validation_criteria()
        self.quality_thresholds = self._load_quality_thresholds()
        
    def _load_validation_criteria(self) -> Dict[str, Any]:
        """åŠ è½½éªŒè¯æ ‡å‡†"""
        return {
            "ç›¸å…³æ€§éªŒè¯": {
                "æ ‡å‡†": [
                    "å†…å®¹ä¸æŸ¥è¯¢ä¸»é¢˜åŒ¹é…",
                    "æ¶µç›–æŸ¥è¯¢çš„å…³é”®æ¦‚å¿µ",
                    "æä¾›ç›´æ¥ç›¸å…³çš„ä¿¡æ¯",
                    "é¿å…æ— å…³æˆ–åç¦»ä¸»é¢˜çš„å†…å®¹"
                ],
                "è¯„åˆ†è¦ç´ ": [
                    "ä¸»é¢˜åŒ¹é…åº¦",
                    "æ¦‚å¿µè¦†ç›–åº¦",
                    "ä¿¡æ¯ç²¾ç¡®åº¦",
                    "ä¸Šä¸‹æ–‡é€‚é…åº¦"
                ]
            },
            "å……åˆ†æ€§éªŒè¯": {
                "æ ‡å‡†": [
                    "ä¿¡æ¯é‡è¶³å¤Ÿå›ç­”é—®é¢˜",
                    "æ¶µç›–é—®é¢˜çš„ä¸»è¦æ–¹é¢",
                    "æä¾›è¶³å¤Ÿçš„ç»†èŠ‚å’Œä¾‹å­",
                    "åŒ…å«å¿…è¦çš„æ“ä½œæŒ‡å¯¼"
                ],
                "è¯„åˆ†è¦ç´ ": [
                    "ä¿¡æ¯å®Œæ•´åº¦",
                    "ç»†èŠ‚ä¸°å¯Œåº¦",
                    "è¦†ç›–å¹¿åº¦",
                    "å®ç”¨æ€§"
                ]
            },
            "ä¸€è‡´æ€§éªŒè¯": {
                "æ ‡å‡†": [
                    "ä¸åŒæºä¹‹é—´ä¿¡æ¯ä¸€è‡´",
                    "æ— è‡ªç›¸çŸ›ç›¾çš„é™ˆè¿°",
                    "æœ¯è¯­ä½¿ç”¨ä¸€è‡´",
                    "è§„åˆ™åº”ç”¨ä¸€è‡´"
                ],
                "è¯„åˆ†è¦ç´ ": [
                    "å†…éƒ¨ä¸€è‡´æ€§",
                    "è·¨æºä¸€è‡´æ€§",
                    "æœ¯è¯­ä¸€è‡´æ€§",
                    "é€»è¾‘ä¸€è‡´æ€§"
                ]
            },
            "å‡†ç¡®æ€§éªŒè¯": {
                "æ ‡å‡†": [
                    "äº‹å®ä¿¡æ¯å‡†ç¡®",
                    "å¼•ç”¨æ¥æºå¯é ",
                    "æ•°æ®è®¡ç®—æ­£ç¡®",
                    "è§„åˆ™è§£é‡Šå‡†ç¡®"
                ],
                "è¯„åˆ†è¦ç´ ": [
                    "äº‹å®å‡†ç¡®åº¦",
                    "æ¥æºå¯é åº¦",
                    "è®¡ç®—æ­£ç¡®æ€§",
                    "ä¸“ä¸šæ°´å‡†"
                ]
            }
        }
    
    def _load_quality_thresholds(self) -> Dict[str, float]:
        """åŠ è½½è´¨é‡é˜ˆå€¼"""
        return {
            "excellent": 0.9,
            "good": 0.7,
            "acceptable": 0.5,
            "poor": 0.3
        }
    
    def _get_capabilities(self) -> List[str]:
        """è·å–æ™ºèƒ½ä½“èƒ½åŠ›åˆ—è¡¨"""
        return [
            "çŸ¥è¯†æ£€ç´¢ç»“æœéªŒè¯",
            "åˆ†æç»“æœè´¨é‡è¯„ä¼°",
            "ç›¸å…³æ€§å’Œå……åˆ†æ€§æ£€æŸ¥",
            "ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§éªŒè¯",
            "æ”¹è¿›å»ºè®®ç”Ÿæˆ"
        ]
    
    async def validate_retrieved_knowledge(
        self,
        query: str,
        retrieved_documents: List[Document],
        knowledge_source: str = "unknown"
    ) -> ValidationResult:
        """éªŒè¯æ£€ç´¢åˆ°çš„çŸ¥è¯†"""
        
        logger.info(f"ğŸ” å¼€å§‹éªŒè¯æ£€ç´¢çŸ¥è¯†ï¼Œæ–‡æ¡£æ•°: {len(retrieved_documents)}")
        
        start_time = time.time()
        
        try:
            # 1. ç›¸å…³æ€§éªŒè¯
            relevance_score = await self._validate_relevance(query, retrieved_documents)
            
            # 2. å……åˆ†æ€§éªŒè¯
            sufficiency_score = await self._validate_sufficiency(query, retrieved_documents)
            
            # 3. ä¸€è‡´æ€§éªŒè¯
            consistency_score = await self._validate_consistency(retrieved_documents)
            
            # 4. å‡†ç¡®æ€§éªŒè¯
            accuracy_score = await self._validate_accuracy(retrieved_documents, knowledge_source)
            
            # 5. è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°
            overall_quality = self._calculate_overall_quality(
                relevance_score, sufficiency_score, consistency_score, accuracy_score
            )
            
            # 6. ç”ŸæˆéªŒè¯æŠ¥å‘Š
            validation_report = await self._generate_validation_report(
                query, retrieved_documents, {
                    "relevance": relevance_score,
                    "sufficiency": sufficiency_score,
                    "consistency": consistency_score,
                    "accuracy": accuracy_score,
                    "overall": overall_quality
                }
            )
            
            processing_time = time.time() - start_time
            
            # åˆ¤æ–­æ˜¯å¦é€šè¿‡éªŒè¯
            is_valid = overall_quality >= self.quality_thresholds["acceptable"]
            
            result = ValidationResult(
                is_valid=is_valid,
                confidence_score=overall_quality,
                validation_details={
                    "relevance_score": relevance_score,
                    "sufficiency_score": sufficiency_score,
                    "consistency_score": consistency_score,
                    "accuracy_score": accuracy_score,
                    "overall_quality": overall_quality,
                    "quality_level": self._determine_quality_level(overall_quality),
                    "document_count": len(retrieved_documents),
                    "processing_time": processing_time
                },
                issues=validation_report.get("issues", []),
                suggestions=validation_report.get("suggestions", [])
            )
            
            logger.info(f"âœ… çŸ¥è¯†éªŒè¯å®Œæˆï¼Œè´¨é‡åˆ†æ•°: {overall_quality:.3f}ï¼Œè€—æ—¶ {processing_time:.2f} ç§’")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†éªŒè¯å¤±è´¥: {str(e)}")
            raise
    
    async def _validate_relevance(
        self,
        query: str,
        documents: List[Document]
    ) -> float:
        """éªŒè¯ç›¸å…³æ€§"""
        
        if not documents:
            return 0.0
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯çŸ¥è¯†è´¨é‡è¯„ä¼°ä¸“å®¶ï¼Œéœ€è¦è¯„ä¼°æ£€ç´¢æ–‡æ¡£ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§ã€‚

ç›¸å…³æ€§è¯„ä¼°æ ‡å‡†ï¼š
1. æ–‡æ¡£å†…å®¹æ˜¯å¦ç›´æ¥å›ç­”æŸ¥è¯¢é—®é¢˜
2. æ–‡æ¡£æ˜¯å¦åŒ…å«æŸ¥è¯¢çš„å…³é”®æ¦‚å¿µ
3. æ–‡æ¡£ä¿¡æ¯æ˜¯å¦é’ˆå¯¹æŸ¥è¯¢åœºæ™¯
4. æ–‡æ¡£æ˜¯å¦æä¾›æœ‰ç”¨çš„ç›¸å…³ä¿¡æ¯

è¯„åˆ†èŒƒå›´ï¼š0.0-1.0ï¼ˆ1.0è¡¨ç¤ºå®Œå…¨ç›¸å…³ï¼‰"""),
            ("human", """æŸ¥è¯¢ï¼š{query}

æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼š
{documents}

è¯·è¯„ä¼°è¿™äº›æ–‡æ¡£ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§ï¼Œè¿”å›0.0-1.0çš„åˆ†æ•°ï¼Œå¹¶è¯´æ˜ç†ç”±ã€‚""")
        ])
        
        documents_text = self._format_documents_for_validation(documents)
        
        response = await self.llm.ainvoke(
            prompt.format_messages(
                query=query,
                documents=documents_text
            )
        )
        
        # è§£æç›¸å…³æ€§åˆ†æ•°
        relevance_score = self._extract_score_from_response(response.content)
        
        # è¡¥å……åŸºäºå…³é”®è¯åŒ¹é…çš„ç›¸å…³æ€§è¯„ä¼°
        keyword_relevance = self._calculate_keyword_relevance(query, documents)
        
        # ç»¼åˆè¯„ä¼°
        final_relevance = (relevance_score * 0.7 + keyword_relevance * 0.3)
        
        return min(1.0, max(0.0, final_relevance))
    
    async def _validate_sufficiency(
        self,
        query: str,
        documents: List[Document]
    ) -> float:
        """éªŒè¯å……åˆ†æ€§"""
        
        if not documents:
            return 0.0
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯çŸ¥è¯†å®Œæ•´æ€§è¯„ä¼°ä¸“å®¶ï¼Œéœ€è¦è¯„ä¼°æ£€ç´¢æ–‡æ¡£æ˜¯å¦å……åˆ†å›ç­”æŸ¥è¯¢ã€‚

å……åˆ†æ€§è¯„ä¼°æ ‡å‡†ï¼š
1. ä¿¡æ¯é‡æ˜¯å¦è¶³å¤Ÿå®Œæ•´
2. æ˜¯å¦æ¶µç›–é—®é¢˜çš„ä¸»è¦æ–¹é¢
3. æ˜¯å¦æä¾›è¶³å¤Ÿçš„ç»†èŠ‚å’Œç¤ºä¾‹
4. æ˜¯å¦åŒ…å«å®ç”¨çš„æ“ä½œæŒ‡å¯¼

è¯„åˆ†èŒƒå›´ï¼š0.0-1.0ï¼ˆ1.0è¡¨ç¤ºå®Œå…¨å……åˆ†ï¼‰"""),
            ("human", """æŸ¥è¯¢ï¼š{query}

æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼š
{documents}

è¯·è¯„ä¼°è¿™äº›æ–‡æ¡£æ˜¯å¦å……åˆ†å›ç­”äº†æŸ¥è¯¢ï¼Œè¿”å›0.0-1.0çš„åˆ†æ•°ï¼Œå¹¶è¯´æ˜ç†ç”±ã€‚""")
        ])
        
        documents_text = self._format_documents_for_validation(documents)
        
        response = await self.llm.ainvoke(
            prompt.format_messages(
                query=query,
                documents=documents_text
            )
        )
        
        # è§£æå……åˆ†æ€§åˆ†æ•°
        sufficiency_score = self._extract_score_from_response(response.content)
        
        # åŸºäºæ–‡æ¡£æ•°é‡å’Œé•¿åº¦çš„è¡¥å……è¯„ä¼°
        quantity_score = self._calculate_quantity_sufficiency(documents)
        
        # ç»¼åˆè¯„ä¼°
        final_sufficiency = (sufficiency_score * 0.8 + quantity_score * 0.2)
        
        return min(1.0, max(0.0, final_sufficiency))
    
    async def _validate_consistency(self, documents: List[Document]) -> float:
        """éªŒè¯ä¸€è‡´æ€§"""
        
        if len(documents) < 2:
            return 1.0  # å•ä¸ªæ–‡æ¡£é»˜è®¤ä¸€è‡´
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¿¡æ¯ä¸€è‡´æ€§è¯„ä¼°ä¸“å®¶ï¼Œéœ€è¦æ£€æŸ¥å¤šä¸ªæ–‡æ¡£ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚

ä¸€è‡´æ€§è¯„ä¼°æ ‡å‡†ï¼š
1. ä¸åŒæ–‡æ¡£é—´ä¿¡æ¯æ˜¯å¦ä¸€è‡´
2. æ˜¯å¦å­˜åœ¨ç›¸äº’çŸ›ç›¾çš„é™ˆè¿°
3. æœ¯è¯­å’Œæ¦‚å¿µä½¿ç”¨æ˜¯å¦ä¸€è‡´
4. è§„åˆ™å’Œæ ‡å‡†åº”ç”¨æ˜¯å¦ä¸€è‡´

è¯„åˆ†èŒƒå›´ï¼š0.0-1.0ï¼ˆ1.0è¡¨ç¤ºå®Œå…¨ä¸€è‡´ï¼‰"""),
            ("human", """æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼š
{documents}

è¯·è¯„ä¼°è¿™äº›æ–‡æ¡£ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œè¿”å›0.0-1.0çš„åˆ†æ•°ï¼Œå¹¶æŒ‡å‡ºä»»ä½•ä¸ä¸€è‡´ä¹‹å¤„ã€‚""")
        ])
        
        documents_text = self._format_documents_for_validation(documents)
        
        response = await self.llm.ainvoke(
            prompt.format_messages(documents=documents_text)
        )
        
        # è§£æä¸€è‡´æ€§åˆ†æ•°
        consistency_score = self._extract_score_from_response(response.content)
        
        # åŸºäºé‡å¤ä¿¡æ¯çš„è¡¥å……è¯„ä¼°
        redundancy_penalty = self._calculate_redundancy_penalty(documents)
        
        # ç»¼åˆè¯„ä¼°
        final_consistency = consistency_score * (1 - redundancy_penalty * 0.1)
        
        return min(1.0, max(0.0, final_consistency))
    
    async def _validate_accuracy(
        self,
        documents: List[Document],
        knowledge_source: str
    ) -> float:
        """éªŒè¯å‡†ç¡®æ€§"""
        
        if not documents:
            return 0.0
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¿¡æ¯å‡†ç¡®æ€§è¯„ä¼°ä¸“å®¶ï¼Œç‰¹åˆ«æ“…é•¿{knowledge_source}æ ‡å‡†ã€‚

å‡†ç¡®æ€§è¯„ä¼°æ ‡å‡†ï¼š
1. äº‹å®ä¿¡æ¯æ˜¯å¦å‡†ç¡®
2. å¼•ç”¨å’Œæ¥æºæ˜¯å¦å¯é 
3. æŠ€æœ¯ç»†èŠ‚æ˜¯å¦æ­£ç¡®
4. æ ‡å‡†è§„åˆ™æ˜¯å¦å‡†ç¡®è§£é‡Š

è¯„åˆ†èŒƒå›´ï¼š0.0-1.0ï¼ˆ1.0è¡¨ç¤ºå®Œå…¨å‡†ç¡®ï¼‰"""),
            ("human", """æ–‡æ¡£å†…å®¹ï¼š
{documents}

çŸ¥è¯†æ¥æºï¼š{knowledge_source}

è¯·è¯„ä¼°è¿™äº›æ–‡æ¡£çš„å‡†ç¡®æ€§ï¼Œè¿”å›0.0-1.0çš„åˆ†æ•°ï¼Œå¹¶æŒ‡å‡ºä»»ä½•å‡†ç¡®æ€§é—®é¢˜ã€‚""")
        ])
        
        documents_text = self._format_documents_for_validation(documents)
        
        response = await self.llm.ainvoke(
            prompt.format_messages(
                documents=documents_text,
                knowledge_source=knowledge_source
            )
        )
        
        # è§£æå‡†ç¡®æ€§åˆ†æ•°
        accuracy_score = self._extract_score_from_response(response.content)
        
        # åŸºäºæ¥æºå¯é æ€§çš„è°ƒæ•´
        source_reliability = self._assess_source_reliability(documents, knowledge_source)
        
        # ç»¼åˆè¯„ä¼°
        final_accuracy = accuracy_score * source_reliability
        
        return min(1.0, max(0.0, final_accuracy))
    
    def _calculate_keyword_relevance(
        self,
        query: str,
        documents: List[Document]
    ) -> float:
        """è®¡ç®—å…³é”®è¯ç›¸å…³æ€§"""
        
        query_words = set(query.lower().split())
        if not query_words:
            return 0.0
        
        total_relevance = 0.0
        
        for doc in documents:
            doc_words = set(doc.page_content.lower().split())
            
            # è®¡ç®—å…³é”®è¯é‡å ç‡
            overlap = query_words & doc_words
            relevance = len(overlap) / len(query_words)
            
            total_relevance += relevance
        
        return total_relevance / len(documents) if documents else 0.0
    
    def _calculate_quantity_sufficiency(self, documents: List[Document]) -> float:
        """è®¡ç®—æ•°é‡å……åˆ†æ€§"""
        
        if not documents:
            return 0.0
        
        # åŸºäºæ–‡æ¡£æ•°é‡
        doc_count_score = min(1.0, len(documents) / 5)  # 5ä¸ªæ–‡æ¡£ä¸ºæ»¡åˆ†
        
        # åŸºäºå†…å®¹é•¿åº¦
        total_length = sum(len(doc.page_content) for doc in documents)
        length_score = min(1.0, total_length / 2000)  # 2000å­—ç¬¦ä¸ºæ»¡åˆ†
        
        return (doc_count_score + length_score) / 2
    
    def _calculate_redundancy_penalty(self, documents: List[Document]) -> float:
        """è®¡ç®—å†—ä½™æƒ©ç½š"""
        
        if len(documents) < 2:
            return 0.0
        
        redundancy_score = 0.0
        
        for i, doc1 in enumerate(documents):
            for j, doc2 in enumerate(documents):
                if i >= j:
                    continue
                
                # ç®€å•çš„é‡å¤å†…å®¹æ£€æµ‹
                words1 = set(doc1.page_content.lower().split())
                words2 = set(doc2.page_content.lower().split())
                
                if words1 and words2:
                    overlap = len(words1 & words2) / len(words1 | words2)
                    if overlap > 0.8:  # 80%é‡å è®¤ä¸ºæ˜¯å†—ä½™
                        redundancy_score += overlap
        
        # å½’ä¸€åŒ–
        max_comparisons = len(documents) * (len(documents) - 1) / 2
        return redundancy_score / max_comparisons if max_comparisons > 0 else 0.0
    
    def _assess_source_reliability(
        self,
        documents: List[Document],
        knowledge_source: str
    ) -> float:
        """è¯„ä¼°æ¥æºå¯é æ€§"""
        
        base_reliability = 0.8  # åŸºç¡€å¯é æ€§
        
        # åŸºäºçŸ¥è¯†æºç±»å‹è°ƒæ•´
        source_reliability_map = {
            "NESMA": 0.95,  # å®˜æ–¹æ ‡å‡†
            "COSMIC": 0.95,  # å®˜æ–¹æ ‡å‡†
            "academic": 0.85,  # å­¦æœ¯æ¥æº
            "industry": 0.75,  # è¡Œä¸šæŠ¥å‘Š
            "blog": 0.6,  # åšå®¢æ–‡ç« 
            "unknown": 0.5  # æœªçŸ¥æ¥æº
        }
        
        source_type = knowledge_source.upper()
        if source_type in source_reliability_map:
            base_reliability = source_reliability_map[source_type]
        
        # åŸºäºæ–‡æ¡£å…ƒæ•°æ®è°ƒæ•´
        for doc in documents:
            metadata = doc.metadata
            
            # æ£€æŸ¥æ¥æºä¿¡æ¯
            if metadata.get("author"):
                base_reliability += 0.05
            
            if metadata.get("publication_date"):
                base_reliability += 0.05
            
            if metadata.get("official_source"):
                base_reliability += 0.1
        
        return min(1.0, base_reliability)
    
    def _calculate_overall_quality(
        self,
        relevance: float,
        sufficiency: float,
        consistency: float,
        accuracy: float
    ) -> float:
        """è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°"""
        
        # åŠ æƒå¹³å‡
        weights = {
            "relevance": 0.3,
            "sufficiency": 0.25,
            "consistency": 0.2,
            "accuracy": 0.25
        }
        
        overall = (
            relevance * weights["relevance"] +
            sufficiency * weights["sufficiency"] +
            consistency * weights["consistency"] +
            accuracy * weights["accuracy"]
        )
        
        return overall
    
    def _determine_quality_level(self, score: float) -> str:
        """ç¡®å®šè´¨é‡ç­‰çº§"""
        
        if score >= self.quality_thresholds["excellent"]:
            return "ä¼˜ç§€"
        elif score >= self.quality_thresholds["good"]:
            return "è‰¯å¥½"
        elif score >= self.quality_thresholds["acceptable"]:
            return "å¯æ¥å—"
        else:
            return "éœ€è¦æ”¹è¿›"
    
    async def _generate_validation_report(
        self,
        query: str,
        documents: List[Document],
        scores: Dict[str, float]
    ) -> Dict[str, Any]:
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        
        issues = []
        suggestions = []
        
        # åŸºäºåˆ†æ•°è¯†åˆ«é—®é¢˜
        if scores["relevance"] < self.quality_thresholds["acceptable"]:
            issues.append("æ–‡æ¡£ç›¸å…³æ€§ä¸è¶³")
            suggestions.append("é‡æ–°ä¼˜åŒ–æŸ¥è¯¢è¯æˆ–æ‰©å¤§æ£€ç´¢èŒƒå›´")
        
        if scores["sufficiency"] < self.quality_thresholds["acceptable"]:
            issues.append("ä¿¡æ¯å……åˆ†æ€§ä¸è¶³")
            suggestions.append("å¢åŠ æ£€ç´¢æ–‡æ¡£æ•°é‡æˆ–ä½¿ç”¨å¤šç§æ£€ç´¢ç­–ç•¥")
        
        if scores["consistency"] < self.quality_thresholds["acceptable"]:
            issues.append("æ–‡æ¡£é—´å­˜åœ¨ä¸ä¸€è‡´")
            suggestions.append("æ£€æŸ¥æ–‡æ¡£æ¥æºï¼Œç­›é€‰æƒå¨èµ„æ–™")
        
        if scores["accuracy"] < self.quality_thresholds["acceptable"]:
            issues.append("ä¿¡æ¯å‡†ç¡®æ€§å­˜ç–‘")
            suggestions.append("éªŒè¯ä¿¡æ¯æ¥æºï¼Œå‚è€ƒå®˜æ–¹æ–‡æ¡£")
        
        # ç”Ÿæˆå…·ä½“å»ºè®®
        if scores["overall"] < self.quality_thresholds["good"]:
            suggestions.extend([
                "è€ƒè™‘ä½¿ç”¨å¤šä¸ªçŸ¥è¯†æºè¿›è¡Œäº¤å‰éªŒè¯",
                "ä¼˜åŒ–æ£€ç´¢ç­–ç•¥ä»¥è·å¾—æ›´å¥½çš„ç»“æœ",
                "äººå·¥å®¡æ ¸å…³é”®ä¿¡æ¯çš„å‡†ç¡®æ€§"
            ])
        
        return {
            "query": query,
            "document_count": len(documents),
            "scores": scores,
            "quality_assessment": {
                "level": self._determine_quality_level(scores["overall"]),
                "strengths": self._identify_strengths(scores),
                "weaknesses": self._identify_weaknesses(scores)
            },
            "issues": issues,
            "suggestions": suggestions,
            "validation_timestamp": datetime.now().isoformat()
        }
    
    def _identify_strengths(self, scores: Dict[str, float]) -> List[str]:
        """è¯†åˆ«ä¼˜åŠ¿"""
        
        strengths = []
        threshold = self.quality_thresholds["good"]
        
        if scores["relevance"] >= threshold:
            strengths.append("æ–‡æ¡£ç›¸å…³æ€§è‰¯å¥½")
        
        if scores["sufficiency"] >= threshold:
            strengths.append("ä¿¡æ¯å……åˆ†æ€§è‰¯å¥½")
        
        if scores["consistency"] >= threshold:
            strengths.append("æ–‡æ¡£ä¸€è‡´æ€§è‰¯å¥½")
        
        if scores["accuracy"] >= threshold:
            strengths.append("ä¿¡æ¯å‡†ç¡®æ€§è‰¯å¥½")
        
        return strengths if strengths else ["æ•´ä½“è´¨é‡éœ€è¦æå‡"]
    
    def _identify_weaknesses(self, scores: Dict[str, float]) -> List[str]:
        """è¯†åˆ«å¼±ç‚¹"""
        
        weaknesses = []
        threshold = self.quality_thresholds["acceptable"]
        
        if scores["relevance"] < threshold:
            weaknesses.append("æ–‡æ¡£ç›¸å…³æ€§ä¸è¶³")
        
        if scores["sufficiency"] < threshold:
            weaknesses.append("ä¿¡æ¯ä¸å¤Ÿå……åˆ†")
        
        if scores["consistency"] < threshold:
            weaknesses.append("å­˜åœ¨ä¿¡æ¯ä¸ä¸€è‡´")
        
        if scores["accuracy"] < threshold:
            weaknesses.append("å‡†ç¡®æ€§æœ‰å¾…éªŒè¯")
        
        return weaknesses
    
    def _format_documents_for_validation(self, documents: List[Document]) -> str:
        """æ ¼å¼åŒ–æ–‡æ¡£ç”¨äºéªŒè¯"""
        
        formatted_docs = []
        
        for i, doc in enumerate(documents):
            doc_text = f"æ–‡æ¡£ {i+1}:\n"
            doc_text += f"å†…å®¹: {doc.page_content[:500]}{'...' if len(doc.page_content) > 500 else ''}\n"
            
            if doc.metadata:
                doc_text += f"å…ƒæ•°æ®: {doc.metadata}\n"
            
            formatted_docs.append(doc_text)
        
        return "\n\n".join(formatted_docs)
    
    def _extract_score_from_response(self, response_content: str) -> float:
        """ä»å“åº”ä¸­æå–åˆ†æ•°"""
        
        import re
        
        # æŸ¥æ‰¾æ•°å­—åˆ†æ•°ï¼ˆ0.0-1.0æ ¼å¼ï¼‰
        score_patterns = [
            r'(\d+\.\d+)',  # å°æ•°æ ¼å¼
            r'(\d+)%',      # ç™¾åˆ†æ¯”æ ¼å¼
            r'(\d+)/10',    # åˆ†æ•°æ ¼å¼
        ]
        
        for pattern in score_patterns:
            matches = re.findall(pattern, response_content)
            if matches:
                score = float(matches[0])
                
                # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
                if score > 1:
                    if score <= 10:
                        score = score / 10
                    elif score <= 100:
                        score = score / 100
                
                return min(1.0, max(0.0, score))
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ•°å­—åˆ†æ•°ï¼ŒåŸºäºå…³é”®è¯ä¼°ç®—
        content_lower = response_content.lower()
        
        if any(word in content_lower for word in ["ä¼˜ç§€", "excellent", "å¾ˆå¥½", "perfect"]):
            return 0.9
        elif any(word in content_lower for word in ["è‰¯å¥½", "good", "ä¸é”™", "satisfactory"]):
            return 0.7
        elif any(word in content_lower for word in ["ä¸€èˆ¬", "average", "å¯ä»¥", "acceptable"]):
            return 0.5
        elif any(word in content_lower for word in ["è¾ƒå·®", "poor", "ä¸å¥½", "inadequate"]):
            return 0.3
        else:
            return 0.5  # é»˜è®¤ä¸­ç­‰åˆ†æ•°
    
    async def validate_analysis_result(
        self,
        analysis_type: str,
        analysis_result: Dict[str, Any],
        input_data: Dict[str, Any]
    ) -> ValidationResult:
        """éªŒè¯åˆ†æç»“æœ"""
        
        logger.info(f"ğŸ” å¼€å§‹éªŒè¯åˆ†æç»“æœï¼Œç±»å‹: {analysis_type}")
        
        try:
            # 1. ç»“æœå®Œæ•´æ€§æ£€æŸ¥
            completeness_score = self._check_result_completeness(analysis_type, analysis_result)
            
            # 2. ç»“æœåˆç†æ€§æ£€æŸ¥
            rationality_score = await self._check_result_rationality(
                analysis_type, analysis_result, input_data
            )
            
            # 3. ç»“æœä¸€è‡´æ€§æ£€æŸ¥
            consistency_score = self._check_result_consistency(analysis_result)
            
            # 4. è®¡ç®—ç»¼åˆè´¨é‡
            overall_quality = (completeness_score + rationality_score + consistency_score) / 3
            
            # 5. ç”ŸæˆéªŒè¯ç»“æœ
            is_valid = overall_quality >= self.quality_thresholds["acceptable"]
            
            result = ValidationResult(
                is_valid=is_valid,
                confidence_score=overall_quality,
                validation_details={
                    "analysis_type": analysis_type,
                    "completeness_score": completeness_score,
                    "rationality_score": rationality_score,
                    "consistency_score": consistency_score,
                    "overall_quality": overall_quality
                },
                issues=self._identify_analysis_issues(analysis_type, analysis_result, {
                    "completeness": completeness_score,
                    "rationality": rationality_score,
                    "consistency": consistency_score
                }),
                suggestions=self._generate_analysis_suggestions(analysis_type, overall_quality)
            )
            
            logger.info(f"âœ… åˆ†æç»“æœéªŒè¯å®Œæˆï¼Œè´¨é‡åˆ†æ•°: {overall_quality:.3f}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æç»“æœéªŒè¯å¤±è´¥: {str(e)}")
            raise
    
    def _check_result_completeness(
        self,
        analysis_type: str,
        analysis_result: Dict[str, Any]
    ) -> float:
        """æ£€æŸ¥ç»“æœå®Œæ•´æ€§"""
        
        required_fields_map = {
            "NESMA_classification": ["function_type", "confidence_score", "justification"],
            "COSMIC_analysis": ["data_movements", "functional_processes", "cfp_total"],
            "process_identification": ["processes", "dependencies", "data_groups"],
            "comparison_analysis": ["nesma_total", "cosmic_total", "difference_analysis"]
        }
        
        required_fields = required_fields_map.get(analysis_type, [])
        if not required_fields:
            return 1.0  # æœªçŸ¥ç±»å‹é»˜è®¤å®Œæ•´
        
        present_fields = sum(1 for field in required_fields if field in analysis_result)
        completeness = present_fields / len(required_fields)
        
        return completeness
    
    async def _check_result_rationality(
        self,
        analysis_type: str,
        analysis_result: Dict[str, Any],
        input_data: Dict[str, Any]
    ) -> float:
        """æ£€æŸ¥ç»“æœåˆç†æ€§"""
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯{analysis_type}åˆ†æä¸“å®¶ï¼Œéœ€è¦è¯„ä¼°åˆ†æç»“æœçš„åˆç†æ€§ã€‚

åˆç†æ€§è¯„ä¼°æ ‡å‡†ï¼š
1. ç»“æœä¸è¾“å…¥æ•°æ®æ˜¯å¦åŒ¹é…
2. åˆ†æé€»è¾‘æ˜¯å¦åˆç†
3. æ•°å€¼æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
4. ç»“è®ºæ˜¯å¦ç¬¦åˆä¸“ä¸šæ ‡å‡†

è¯„åˆ†èŒƒå›´ï¼š0.0-1.0ï¼ˆ1.0è¡¨ç¤ºå®Œå…¨åˆç†ï¼‰"""),
            ("human", """åˆ†æç±»å‹ï¼š{analysis_type}

è¾“å…¥æ•°æ®ï¼š
{input_data}

åˆ†æç»“æœï¼š
{analysis_result}

è¯·è¯„ä¼°è¿™ä¸ªåˆ†æç»“æœçš„åˆç†æ€§ï¼Œè¿”å›0.0-1.0çš„åˆ†æ•°ã€‚""")
        ])
        
        response = await self.llm.ainvoke(
            prompt.format_messages(
                analysis_type=analysis_type,
                input_data=str(input_data),
                analysis_result=str(analysis_result)
            )
        )
        
        return self._extract_score_from_response(response.content)
    
    def _check_result_consistency(self, analysis_result: Dict[str, Any]) -> float:
        """æ£€æŸ¥ç»“æœä¸€è‡´æ€§"""
        
        consistency_score = 1.0
        
        # æ£€æŸ¥æ•°å€¼ä¸€è‡´æ€§
        if "total" in analysis_result and "details" in analysis_result:
            details = analysis_result["details"]
            if isinstance(details, list):
                calculated_total = sum(item.get("value", 0) for item in details if isinstance(item, dict))
                reported_total = analysis_result["total"]
                
                if reported_total > 0:
                    difference_ratio = abs(calculated_total - reported_total) / reported_total
                    if difference_ratio > 0.1:  # 10%ä»¥ä¸Šå·®å¼‚è®¤ä¸ºä¸ä¸€è‡´
                        consistency_score -= 0.3
        
        # æ£€æŸ¥ç½®ä¿¡åº¦ä¸€è‡´æ€§
        confidence_scores = []
        if isinstance(analysis_result, dict):
            for key, value in analysis_result.items():
                if "confidence" in key and isinstance(value, (int, float)):
                    confidence_scores.append(value)
        
        if len(confidence_scores) > 1:
            score_variance = max(confidence_scores) - min(confidence_scores)
            if score_variance > 0.5:  # ç½®ä¿¡åº¦å·®å¼‚è¿‡å¤§
                consistency_score -= 0.2
        
        return max(0.0, consistency_score)
    
    def _identify_analysis_issues(
        self,
        analysis_type: str,
        analysis_result: Dict[str, Any],
        scores: Dict[str, float]
    ) -> List[str]:
        """è¯†åˆ«åˆ†æé—®é¢˜"""
        
        issues = []
        threshold = self.quality_thresholds["acceptable"]
        
        if scores["completeness"] < threshold:
            issues.append(f"{analysis_type}åˆ†æç»“æœä¸å®Œæ•´")
        
        if scores["rationality"] < threshold:
            issues.append(f"{analysis_type}åˆ†æç»“æœä¸åˆç†")
        
        if scores["consistency"] < threshold:
            issues.append(f"{analysis_type}åˆ†æç»“æœå­˜åœ¨å†…éƒ¨ä¸ä¸€è‡´")
        
        return issues
    
    def _generate_analysis_suggestions(
        self,
        analysis_type: str,
        overall_quality: float
    ) -> List[str]:
        """ç”Ÿæˆåˆ†æå»ºè®®"""
        
        suggestions = []
        
        if overall_quality < self.quality_thresholds["good"]:
            suggestions.extend([
                f"é‡æ–°æ£€æŸ¥{analysis_type}çš„åˆ†æé€»è¾‘",
                "éªŒè¯è¾“å…¥æ•°æ®çš„å‡†ç¡®æ€§",
                "å‚è€ƒç›¸å…³æ ‡å‡†å’Œæœ€ä½³å®è·µ",
                "è€ƒè™‘ä½¿ç”¨å¤šç§æ–¹æ³•è¿›è¡Œäº¤å‰éªŒè¯"
            ])
        
        return suggestions


if __name__ == "__main__":
    # æµ‹è¯•è´¨é‡éªŒè¯æ™ºèƒ½ä½“
    async def test_validator():
        agent = ValidatorAgent()
        
        # æµ‹è¯•çŸ¥è¯†éªŒè¯
        test_documents = [
            Document(
                page_content="ILFæ˜¯å†…éƒ¨é€»è¾‘æ–‡ä»¶ï¼Œç”±åº”ç”¨ç¨‹åºå†…éƒ¨ç»´æŠ¤çš„æ•°æ®ç»„æˆã€‚",
                metadata={"source": "NESMAå®˜æ–¹æ–‡æ¡£"}
            ),
            Document(
                page_content="å†…éƒ¨é€»è¾‘æ–‡ä»¶åŒ…å«ç”¨æˆ·å¯è¯†åˆ«çš„æ•°æ®ï¼Œå¹¶é€šè¿‡åº”ç”¨ç¨‹åºçš„åŠŸèƒ½è¿‡ç¨‹ç»´æŠ¤ã€‚",
                metadata={"source": "NESMAæŒ‡å—"}
            )
        ]
        
        result = await agent.validate_retrieved_knowledge(
            query="ä»€ä¹ˆæ˜¯NESMAä¸­çš„ILF",
            retrieved_documents=test_documents,
            knowledge_source="NESMA"
        )
        
        print(f"çŸ¥è¯†éªŒè¯ç»“æœï¼š")
        print(f"- æ˜¯å¦æœ‰æ•ˆï¼š{result.is_valid}")
        print(f"- ç½®ä¿¡åº¦ï¼š{result.confidence_score:.3f}")
        print(f"- è´¨é‡ç­‰çº§ï¼š{result.validation_details.get('quality_level')}")
        if result.issues:
            print(f"- é—®é¢˜ï¼š{result.issues}")
        if result.suggestions:
            print(f"- å»ºè®®ï¼š{result.suggestions}")
    
    asyncio.run(test_validator()) 