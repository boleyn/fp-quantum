"""
é‡å­æ™ºèƒ½åŒ–åŠŸèƒ½ç‚¹ä¼°ç®—ç³»ç»Ÿ - æµç¨‹è¯†åˆ«æ™ºèƒ½ä½“

ä»éœ€æ±‚è§£æç»“æœä¸­è¯†åˆ«ç‹¬ç«‹çš„åŠŸèƒ½æµç¨‹
"""

import asyncio
from typing import Dict, Any, Optional, List, Tuple
import logging
from datetime import datetime
import time
import re

from langchain_core.language_models import BaseLanguageModel
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

from agents.base.base_agent import SpecializedAgent
from models.project_models import ProjectInfo, ProcessDetails
from models.common_models import ConfidenceLevel, ValidationResult
from config.settings import get_settings

logger = logging.getLogger(__name__)


class ProcessIdentifierAgent(SpecializedAgent):
    """æµç¨‹è¯†åˆ«æ™ºèƒ½ä½“"""
    
    def __init__(self, llm: Optional[BaseLanguageModel] = None):
        super().__init__(
            agent_id="process_identifier",
            specialty="business_process_analysis",
            llm=llm
        )
        
        self.settings = get_settings()
        
        # æµç¨‹è¯†åˆ«æ¨¡å¼å’Œè§„åˆ™
        self.process_patterns = self._load_process_patterns()
        self.boundary_rules = self._load_boundary_rules()
        
    def _load_process_patterns(self) -> Dict[str, List[str]]:
        """åŠ è½½æµç¨‹è¯†åˆ«æ¨¡å¼"""
        return {
            "è§¦å‘è¯": [
                "å½“", "å¦‚æœ", "åœ¨...æƒ…å†µä¸‹", "éœ€è¦", "è¦æ±‚",
                "ç”¨æˆ·", "ç³»ç»Ÿ", "ç®¡ç†å‘˜", "æ“ä½œå‘˜"
            ],
            "åŠ¨ä½œè¯": [
                "åˆ›å»º", "æ–°å¢", "æ·»åŠ ", "å½•å…¥", "è¾“å…¥",
                "æŸ¥è¯¢", "æœç´¢", "æ£€ç´¢", "æµè§ˆ", "æŸ¥çœ‹",
                "ä¿®æ”¹", "ç¼–è¾‘", "æ›´æ–°", "å˜æ›´", "è°ƒæ•´",
                "åˆ é™¤", "ç§»é™¤", "å–æ¶ˆ", "æ’¤é”€", "æ¸…é™¤",
                "å®¡æ ¸", "å®¡æ‰¹", "ç¡®è®¤", "éªŒè¯", "æ£€æŸ¥",
                "ç”Ÿæˆ", "äº§ç”Ÿ", "åˆ›å»º", "è¾“å‡º", "å¯¼å‡º",
                "å‘é€", "ä¼ è¾“", "é€šçŸ¥", "æé†’", "æ¨é€"
            ],
            "ç»“æœè¯": [
                "å®Œæˆ", "æˆåŠŸ", "å¤±è´¥", "é”™è¯¯",
                "ä¿å­˜", "å­˜å‚¨", "è®°å½•", "æ›´æ–°",
                "é€šçŸ¥", "æé†’", "åé¦ˆ", "ç¡®è®¤"
            ],
            "æ•°æ®å¯¹è±¡": [
                "ä¿¡æ¯", "æ•°æ®", "è®°å½•", "æ–‡ä»¶", "æŠ¥è¡¨",
                "è®¢å•", "ç”¨æˆ·", "äº§å“", "å®¢æˆ·", "é¡¹ç›®",
                "è´¦æˆ·", "æƒé™", "é…ç½®", "å‚æ•°", "è®¾ç½®"
            ]
        }
    
    def _load_boundary_rules(self) -> Dict[str, Any]:
        """åŠ è½½æµç¨‹è¾¹ç•Œè¯†åˆ«è§„åˆ™"""
        return {
            "ç‹¬ç«‹æ€§åŸåˆ™": [
                "æ¯ä¸ªæµç¨‹æœ‰æ˜ç¡®çš„å¼€å§‹å’Œç»“æŸ",
                "æµç¨‹å†…éƒ¨é€»è¾‘å®Œæ•´ï¼Œå¯ç‹¬ç«‹æ‰§è¡Œ",
                "æµç¨‹æœ‰æ˜ç¡®çš„è¾“å…¥å’Œè¾“å‡º",
                "æµç¨‹å¤„ç†ç‰¹å®šçš„ä¸šåŠ¡åœºæ™¯"
            ],
            "è¾¹ç•Œæ ‡è¯†": [
                "ç”¨æˆ·æ“ä½œè§¦å‘ç‚¹",
                "ç³»ç»Ÿå“åº”ç»“æŸç‚¹",
                "æ•°æ®çŠ¶æ€å˜åŒ–ç‚¹",
                "ä¸šåŠ¡è§„åˆ™æ£€æŸ¥ç‚¹"
            ],
            "åˆ†å‰²æ ‡å‡†": [
                "ä¸åŒçš„ä¸šåŠ¡ç›®æ ‡",
                "ä¸åŒçš„ç”¨æˆ·è§’è‰²",
                "ä¸åŒçš„æ•°æ®ä¸»ä½“",
                "ä¸åŒçš„å¤„ç†é€»è¾‘"
            ],
            "åˆå¹¶æ¡ä»¶": [
                "ç›¸åŒçš„ä¸šåŠ¡ç›®æ ‡",
                "è¿ç»­çš„æ“ä½œåºåˆ—",
                "ç´§å¯†çš„æ•°æ®å…³è”",
                "ä¸å¯åˆ†å‰²çš„äº‹åŠ¡"
            ]
        }
    
    def _get_capabilities(self) -> List[str]:
        """è·å–æ™ºèƒ½ä½“èƒ½åŠ›åˆ—è¡¨"""
        return [
            "åŠŸèƒ½æµç¨‹è¾¹ç•Œè¯†åˆ«",
            "ä¸šåŠ¡æµç¨‹åˆ†è§£",
            "æµç¨‹ä¾èµ–å…³ç³»åˆ†æ",
            "æ•°æ®æµåˆ†æ",
            "æµç¨‹å®Œæ•´æ€§éªŒè¯"
        ]
    
    async def identify_processes(
        self,
        requirement_analysis: Any,
        project_info: ProjectInfo
    ) -> List[ProcessDetails]:
        """è¯†åˆ«åŠŸèƒ½æµç¨‹"""
        
        logger.info(f"ğŸ” å¼€å§‹è¯†åˆ«åŠŸèƒ½æµç¨‹...")
        
        start_time = time.time()
        
        try:
            # 1. ä»éœ€æ±‚åˆ†æç»“æœæå–æµç¨‹ä¿¡æ¯
            raw_processes = await self._extract_process_candidates(requirement_analysis)
            
            # 2. åº”ç”¨æµç¨‹è¾¹ç•Œè§„åˆ™
            bounded_processes = await self._apply_boundary_rules(raw_processes, project_info)
            
            # 3. ä¼˜åŒ–å’Œåˆå¹¶æµç¨‹
            optimized_processes = await self._optimize_processes(bounded_processes)
            
            # 4. éªŒè¯æµç¨‹å®Œæ•´æ€§
            validated_processes = await self._validate_processes(optimized_processes, requirement_analysis)
            
            # 5. ç”Ÿæˆæµç¨‹è¯¦æƒ…
            detailed_processes = await self._generate_process_details(validated_processes)
            
            processing_time = time.time() - start_time
            
            logger.info(f"âœ… æµç¨‹è¯†åˆ«å®Œæˆï¼Œè¯†åˆ«å‡º {len(detailed_processes)} ä¸ªæµç¨‹ï¼Œè€—æ—¶ {processing_time:.2f} ç§’")
            
            return detailed_processes
            
        except Exception as e:
            logger.error(f"âŒ æµç¨‹è¯†åˆ«å¤±è´¥: {str(e)}")
            raise
    
    async def _extract_process_candidates(
        self, 
        requirement_analysis: Any
    ) -> List[Dict[str, Any]]:
        """ä»éœ€æ±‚åˆ†æç»“æœæå–æµç¨‹å€™é€‰"""
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸šåŠ¡æµç¨‹åˆ†æä¸“å®¶ï¼Œéœ€è¦ä»éœ€æ±‚åˆ†æç»“æœä¸­è¯†åˆ«ç‹¬ç«‹çš„åŠŸèƒ½æµç¨‹ã€‚

ä¸€ä¸ªç‹¬ç«‹çš„åŠŸèƒ½æµç¨‹åº”è¯¥å…·å¤‡ï¼š
1. æ˜ç¡®çš„ä¸šåŠ¡ç›®æ ‡
2. æ¸…æ™°çš„è§¦å‘æ¡ä»¶
3. å®Œæ•´çš„å¤„ç†æ­¥éª¤
4. æ˜ç¡®çš„è¾“å‡ºç»“æœ
5. æ¶‰åŠçš„æ•°æ®ç»„

è¯·è¯†åˆ«æ¯ä¸ªå€™é€‰æµç¨‹ï¼Œå¹¶æä¾›è¯¦ç»†åˆ†æã€‚

é‡è¦ï¼šè¾“å‡ºå¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ•°ç»„æ ¼å¼ï¼Œæ¯ä¸ªæµç¨‹å¯¹è±¡åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- name: æµç¨‹åç§°ï¼ˆä¸èƒ½ä¸ºç©ºæˆ–"æœªçŸ¥"ï¼‰
- business_goal: ä¸šåŠ¡ç›®æ ‡
- trigger_conditions: è§¦å‘æ¡ä»¶åˆ—è¡¨
- main_steps: ä¸»è¦æ­¥éª¤åˆ—è¡¨
- input_data: è¾“å…¥æ•°æ®åˆ—è¡¨
- output_results: è¾“å‡ºç»“æœåˆ—è¡¨
- involved_roles: æ¶‰åŠè§’è‰²åˆ—è¡¨"""),
            ("human", """éœ€æ±‚åˆ†æç»“æœï¼š
{requirement_analysis}

è¯·è¯†åˆ«å…¶ä¸­çš„åŠŸèƒ½æµç¨‹å€™é€‰ï¼Œä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼š

```json
[
  {{
    "name": "å…·ä½“çš„æµç¨‹åç§°",
    "business_goal": "æ˜ç¡®çš„ä¸šåŠ¡ç›®æ ‡æè¿°",
    "trigger_conditions": ["è§¦å‘æ¡ä»¶1", "è§¦å‘æ¡ä»¶2"],
    "main_steps": ["æ­¥éª¤1", "æ­¥éª¤2", "æ­¥éª¤3"],
    "input_data": ["è¾“å…¥æ•°æ®1", "è¾“å…¥æ•°æ®2"],
    "output_results": ["è¾“å‡ºç»“æœ1", "è¾“å‡ºç»“æœ2"],
    "involved_roles": ["è§’è‰²1", "è§’è‰²2"]
  }}
]
```

æ³¨æ„ï¼š
1. ç¡®ä¿JSONæ ¼å¼æ­£ç¡®
2. æµç¨‹åç§°å¿…é¡»å…·ä½“ä¸”æœ‰æ„ä¹‰ï¼Œä¸èƒ½æ˜¯"æœªçŸ¥"æˆ–ç©ºå€¼
3. æ¯ä¸ªå­—æ®µéƒ½è¦æœ‰å…·ä½“å†…å®¹ï¼Œä¸èƒ½ä¸ºç©º""")
        ])
        
        response = await self.llm.ainvoke(
            prompt.format_messages(
                requirement_analysis=self._format_requirement_analysis(requirement_analysis)
            )
        )
        
        return await self._parse_process_candidates(response.content)
    
    async def _apply_boundary_rules(
        self,
        raw_processes: List[Dict[str, Any]],
        project_info: ProjectInfo
    ) -> List[Dict[str, Any]]:
        """åº”ç”¨æµç¨‹è¾¹ç•Œè§„åˆ™"""
        
        logger.info(f"ğŸ” å¼€å§‹åº”ç”¨è¾¹ç•Œè§„åˆ™ï¼Œå€™é€‰æµç¨‹æ•°é‡: {len(raw_processes)}")
        
        bounded_processes = []
        
        for i, process in enumerate(raw_processes):
            logger.debug(f"å¤„ç†ç¬¬ {i+1} ä¸ªæµç¨‹: {process.get('name', 'æœªçŸ¥')}")
            
            # æ£€æŸ¥æµç¨‹ç‹¬ç«‹æ€§
            if self._check_process_independence(process):
                # åº”ç”¨è¾¹ç•Œæ ‡è¯†è§„åˆ™
                bounded_process = await self._identify_process_boundaries(process, project_info)
                bounded_processes.append(bounded_process)
                logger.info(f"âœ… æµç¨‹ '{process.get('name', 'æœªçŸ¥')}' é€šè¿‡è¾¹ç•Œè§„åˆ™æ£€æŸ¥")
            else:
                logger.warning(f"âŒ æµç¨‹ '{process.get('name', 'æœªçŸ¥')}' ä¸æ»¡è¶³ç‹¬ç«‹æ€§åŸåˆ™ï¼Œè·³è¿‡")
        
        logger.info(f"âœ… è¾¹ç•Œè§„åˆ™åº”ç”¨å®Œæˆï¼Œæœ‰æ•ˆæµç¨‹æ•°é‡: {len(bounded_processes)}")
        return bounded_processes
    
    def _check_process_independence(self, process: Dict[str, Any]) -> bool:
        """æ£€æŸ¥æµç¨‹ç‹¬ç«‹æ€§"""
        
        logger.debug(f"ğŸ” æ£€æŸ¥æµç¨‹ç‹¬ç«‹æ€§: {process}")
        
        # æ£€æŸ¥æµç¨‹åç§°
        name = process.get("name") or process.get("æµç¨‹åç§°") or process.get("process_name")
        if not name or name.strip() == "" or name == "æœªçŸ¥":
            logger.warning(f"æµç¨‹åç§°ä¸ºç©ºæˆ–æœªçŸ¥: {name}")
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®çš„ä¸šåŠ¡ç›®æ ‡ï¼ˆå…¼å®¹å¤šç§å­—æ®µåï¼‰
        business_goal = (process.get("business_goal") or 
                        process.get("ä¸šåŠ¡ç›®æ ‡") or 
                        process.get("description") or
                        process.get("åŠŸèƒ½æè¿°") or
                        process.get("ç›®æ ‡"))
        if not business_goal:
            logger.warning(f"æµç¨‹ '{name}' ç¼ºå°‘ä¸šåŠ¡ç›®æ ‡")
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è§¦å‘æ¡ä»¶ï¼ˆå…¼å®¹å¤šç§å­—æ®µåï¼‰
        trigger_conditions = (process.get("trigger_conditions") or 
                             process.get("è§¦å‘æ¡ä»¶") or 
                             process.get("triggers") or
                             process.get("input") or
                             process.get("è¾“å…¥æ¡ä»¶"))
        if not trigger_conditions:
            logger.warning(f"æµç¨‹ '{name}' ç¼ºå°‘è§¦å‘æ¡ä»¶")
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¤„ç†æ­¥éª¤ï¼ˆå…¼å®¹å¤šç§å­—æ®µåï¼‰
        main_steps = (process.get("main_steps") or 
                     process.get("ä¸»è¦æ­¥éª¤") or 
                     process.get("steps") or
                     process.get("å¤„ç†æ­¥éª¤") or
                     process.get("æµç¨‹"))
        if not main_steps:
            logger.warning(f"æµç¨‹ '{name}' ç¼ºå°‘å¤„ç†æ­¥éª¤")
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¾“å‡ºç»“æœï¼ˆå…¼å®¹å¤šç§å­—æ®µåï¼‰
        output_results = (process.get("output_results") or 
                         process.get("è¾“å‡ºç»“æœ") or 
                         process.get("output") or
                         process.get("ç»“æœ") or
                         process.get("outputs"))
        if not output_results:
            logger.warning(f"æµç¨‹ '{name}' ç¼ºå°‘è¾“å‡ºç»“æœ")
            return False
        
        logger.info(f"âœ… æµç¨‹ '{name}' é€šè¿‡ç‹¬ç«‹æ€§æ£€æŸ¥")
        return True
    
    async def _identify_process_boundaries(
        self,
        process: Dict[str, Any],
        project_info: ProjectInfo
    ) -> Dict[str, Any]:
        """è¯†åˆ«æµç¨‹è¾¹ç•Œ"""
        
        # è¯†åˆ«æµç¨‹å¼€å§‹è¾¹ç•Œ
        start_boundary = self._identify_start_boundary(process)
        
        # è¯†åˆ«æµç¨‹ç»“æŸè¾¹ç•Œ
        end_boundary = self._identify_end_boundary(process)
        
        # è¯†åˆ«æ•°æ®è¾¹ç•Œ
        data_boundary = await self._identify_data_boundary(process, project_info)
        
        process["boundaries"] = {
            "start": start_boundary,
            "end": end_boundary,
            "data": data_boundary
        }
        
        return process
    
    def _identify_start_boundary(self, process: Dict[str, Any]) -> Dict[str, Any]:
        """è¯†åˆ«æµç¨‹å¼€å§‹è¾¹ç•Œ"""
        
        trigger_conditions = process.get("trigger_conditions", [])
        
        return {
            "type": "user_action",
            "description": "ç”¨æˆ·æ“ä½œè§¦å‘",
            "conditions": trigger_conditions,
            "entry_point": "ç”¨æˆ·ç•Œé¢æˆ–APIè°ƒç”¨"
        }
    
    def _identify_end_boundary(self, process: Dict[str, Any]) -> Dict[str, Any]:
        """è¯†åˆ«æµç¨‹ç»“æŸè¾¹ç•Œ"""
        
        output_results = process.get("output_results", [])
        
        return {
            "type": "system_response",
            "description": "ç³»ç»Ÿå“åº”å®Œæˆ",
            "results": output_results,
            "exit_point": "ç”¨æˆ·ç•Œé¢åé¦ˆæˆ–æ•°æ®æŒä¹…åŒ–"
        }
    
    async def _identify_data_boundary(
        self,
        process: Dict[str, Any],
        project_info: ProjectInfo
    ) -> Dict[str, Any]:
        """è¯†åˆ«æ•°æ®è¾¹ç•Œ"""
        
        input_data = process.get("input_data", [])
        output_results = process.get("output_results", [])
        
        # åˆ†ææ•°æ®æµåŠ¨
        data_flow = await self._analyze_data_flow(input_data, output_results, project_info)
        
        return {
            "input_sources": input_data,
            "output_targets": output_results,
            "data_flow": data_flow,
            "storage_requirements": self._analyze_storage_requirements(data_flow)
        }
    
    async def _analyze_data_flow(
        self,
        input_data: List[str],
        output_results: List[str],
        project_info: ProjectInfo
    ) -> Dict[str, Any]:
        """åˆ†ææ•°æ®æµåŠ¨"""
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯æ•°æ®æµåˆ†æä¸“å®¶ï¼Œéœ€è¦åˆ†æä¸šåŠ¡æµç¨‹ä¸­çš„æ•°æ®æµåŠ¨ã€‚

åˆ†æè¦ç‚¹ï¼š
1. æ•°æ®æ¥æºå’Œå»å‘
2. æ•°æ®è½¬æ¢å’Œå¤„ç†
3. æ•°æ®ä¾èµ–å…³ç³»
4. å­˜å‚¨éœ€æ±‚"""),
            ("human", """é¡¹ç›®ä¿¡æ¯ï¼š{project_info}

è¾“å…¥æ•°æ®ï¼š{input_data}
è¾“å‡ºç»“æœï¼š{output_results}

è¯·åˆ†ææ•°æ®æµåŠ¨æ¨¡å¼ï¼Œè¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœã€‚""")
        ])
        
        response = await self.llm.ainvoke(
            prompt.format_messages(
                project_info=project_info.description,
                input_data=input_data,
                output_results=output_results
            )
        )
        
        return await self._parse_data_flow_analysis(response.content)
    
    def _analyze_storage_requirements(self, data_flow: Dict[str, Any]) -> List[str]:
        """åˆ†æå­˜å‚¨éœ€æ±‚"""
        
        requirements = []
        
        # åŸºäºæ•°æ®æµåˆ†æå­˜å‚¨éœ€æ±‚
        if data_flow.get("persistent_data"):
            requirements.append("æŒä¹…åŒ–å­˜å‚¨")
        
        if data_flow.get("temporary_data"):
            requirements.append("ä¸´æ—¶å­˜å‚¨")
        
        if data_flow.get("cached_data"):
            requirements.append("ç¼“å­˜å­˜å‚¨")
        
        return requirements
    
    async def _optimize_processes(
        self,
        bounded_processes: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """ä¼˜åŒ–å’Œåˆå¹¶æµç¨‹"""
        
        optimized = []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ä»¥åˆå¹¶çš„æµç¨‹
        merged_groups = self._identify_mergeable_processes(bounded_processes)
        
        for group in merged_groups:
            if len(group) == 1:
                optimized.append(group[0])
            else:
                # åˆå¹¶æµç¨‹ç»„
                merged_process = await self._merge_process_group(group)
                optimized.append(merged_process)
        
        return optimized
    
    def _identify_mergeable_processes(
        self,
        processes: List[Dict[str, Any]]
    ) -> List[List[Dict[str, Any]]]:
        """è¯†åˆ«å¯åˆå¹¶çš„æµç¨‹"""
        
        groups = []
        processed = set()
        
        for i, process1 in enumerate(processes):
            if i in processed:
                continue
            
            group = [process1]
            processed.add(i)
            
            for j, process2 in enumerate(processes):
                if j <= i or j in processed:
                    continue
                
                if self._should_merge_processes(process1, process2):
                    group.append(process2)
                    processed.add(j)
            
            groups.append(group)
        
        return groups
    
    def _should_merge_processes(
        self,
        process1: Dict[str, Any],
        process2: Dict[str, Any]
    ) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆå¹¶ä¸¤ä¸ªæµç¨‹"""
        
        # æ£€æŸ¥ä¸šåŠ¡ç›®æ ‡ç›¸ä¼¼æ€§
        goal1 = process1.get("business_goal", "").lower()
        goal2 = process2.get("business_goal", "").lower()
        
        if self._calculate_text_similarity(goal1, goal2) > 0.8:
            return True
        
        # æ£€æŸ¥æ•°æ®å…³è”æ€§
        data1 = set(process1.get("input_data", []) + process1.get("output_results", []))
        data2 = set(process2.get("input_data", []) + process2.get("output_results", []))
        
        overlap = len(data1 & data2) / max(len(data1 | data2), 1)
        if overlap > 0.6:
            return True
        
        return False
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
        
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        if not words1 and not words2:
            return 1.0
        
        intersection = words1 & words2
        union = words1 | words2
        
        return len(intersection) / len(union) if union else 0.0
    
    async def _merge_process_group(
        self,
        process_group: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """åˆå¹¶æµç¨‹ç»„"""
        
        if len(process_group) == 1:
            return process_group[0]
        
        # åˆå¹¶æµç¨‹ä¿¡æ¯
        merged_name = " & ".join([p.get("name", "æœªçŸ¥") for p in process_group])
        merged_goal = self._merge_business_goals([p.get("business_goal", "") for p in process_group])
        merged_steps = self._merge_process_steps([p.get("main_steps", []) for p in process_group])
        
        return {
            "name": merged_name,
            "business_goal": merged_goal,
            "trigger_conditions": self._merge_lists([p.get("trigger_conditions", []) for p in process_group]),
            "main_steps": merged_steps,
            "input_data": self._merge_lists([p.get("input_data", []) for p in process_group]),
            "output_results": self._merge_lists([p.get("output_results", []) for p in process_group]),
            "involved_roles": self._merge_lists([p.get("involved_roles", []) for p in process_group]),
            "boundaries": self._merge_boundaries([p.get("boundaries", {}) for p in process_group])
        }
    
    def _merge_business_goals(self, goals: List[str]) -> str:
        """åˆå¹¶ä¸šåŠ¡ç›®æ ‡"""
        unique_goals = list(set([goal.strip() for goal in goals if goal.strip()]))
        return "ï¼›".join(unique_goals)
    
    def _merge_process_steps(self, step_lists: List[List[str]]) -> List[str]:
        """åˆå¹¶æµç¨‹æ­¥éª¤"""
        all_steps = []
        for steps in step_lists:
            all_steps.extend(steps)
        
        # å»é‡å¹¶ä¿æŒé¡ºåº
        seen = set()
        unique_steps = []
        for step in all_steps:
            if step not in seen:
                unique_steps.append(step)
                seen.add(step)
        
        return unique_steps
    
    def _merge_lists(self, list_of_lists: List[List[str]]) -> List[str]:
        """åˆå¹¶å­—ç¬¦ä¸²åˆ—è¡¨"""
        all_items = []
        for lst in list_of_lists:
            all_items.extend(lst)
        
        return list(set(all_items))  # å»é‡
    
    def _merge_boundaries(self, boundary_list: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆå¹¶è¾¹ç•Œä¿¡æ¯"""
        
        if not boundary_list:
            return {}
        
        merged = {
            "start": boundary_list[0].get("start", {}),
            "end": boundary_list[-1].get("end", {}),
            "data": {}
        }
        
        # åˆå¹¶æ•°æ®è¾¹ç•Œ
        all_input_sources = []
        all_output_targets = []
        
        for boundary in boundary_list:
            data_boundary = boundary.get("data", {})
            all_input_sources.extend(data_boundary.get("input_sources", []))
            all_output_targets.extend(data_boundary.get("output_targets", []))
        
        merged["data"] = {
            "input_sources": list(set(all_input_sources)),
            "output_targets": list(set(all_output_targets))
        }
        
        return merged
    
    async def _validate_processes(
        self,
        processes: List[Dict[str, Any]],
        requirement_analysis: Any
    ) -> List[Dict[str, Any]]:
        """éªŒè¯æµç¨‹å®Œæ•´æ€§"""
        
        validated_processes = []
        
        for process in processes:
            validation_result = await self._validate_single_process(process, requirement_analysis)
            
            if validation_result["is_valid"]:
                process["validation"] = validation_result
                validated_processes.append(process)
            else:
                logger.warning(f"æµç¨‹ '{process.get('name', 'æœªçŸ¥')}' éªŒè¯å¤±è´¥: {validation_result['issues']}")
        
        return validated_processes
    
    async def _validate_single_process(
        self,
        process: Dict[str, Any],
        requirement_analysis: Any
    ) -> Dict[str, Any]:
        """éªŒè¯å•ä¸ªæµç¨‹"""
        
        validation_result = {
            "is_valid": True,
            "confidence_score": 1.0,
            "issues": [],
            "suggestions": []
        }
        
        # æ£€æŸ¥æµç¨‹å®Œæ•´æ€§
        completeness_issues = self._check_process_completeness(process)
        validation_result["issues"].extend(completeness_issues)
        
        # æ£€æŸ¥ä¸éœ€æ±‚çš„ä¸€è‡´æ€§
        consistency_issues = self._check_requirement_consistency(process, requirement_analysis)
        validation_result["issues"].extend(consistency_issues)
        
        # æ£€æŸ¥æµç¨‹åˆç†æ€§
        rationality_issues = await self._check_process_rationality(process)
        validation_result["issues"].extend(rationality_issues)
        
        # è®¡ç®—éªŒè¯åˆ†æ•°
        if validation_result["issues"]:
            validation_result["is_valid"] = len(validation_result["issues"]) <= 2
            validation_result["confidence_score"] = max(0.1,
                1.0 - len(validation_result["issues"]) * 0.15
            )
        
        return validation_result
    
    def _check_process_completeness(self, process: Dict[str, Any]) -> List[str]:
        """æ£€æŸ¥æµç¨‹å®Œæ•´æ€§"""
        
        issues = []
        
        required_fields = ["name", "business_goal", "trigger_conditions", "main_steps", "output_results"]
        
        for field in required_fields:
            if not process.get(field):
                issues.append(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {field}")
        
        # æ£€æŸ¥æ­¥éª¤æ•°é‡
        steps = process.get("main_steps", [])
        if len(steps) < 2:
            issues.append("æµç¨‹æ­¥éª¤è¿‡å°‘ï¼Œå¯èƒ½ä¸å®Œæ•´")
        
        return issues
    
    def _check_requirement_consistency(
        self,
        process: Dict[str, Any],
        requirement_analysis: Any
    ) -> List[str]:
        """æ£€æŸ¥ä¸éœ€æ±‚çš„ä¸€è‡´æ€§"""
        
        issues = []
        
        # æ£€æŸ¥åŠŸèƒ½æ¨¡å—ä¸€è‡´æ€§
        if hasattr(requirement_analysis, 'functional_modules'):
            functional_modules = requirement_analysis.functional_modules
        elif isinstance(requirement_analysis, dict):
            functional_modules = requirement_analysis.get("functional_modules", [])
        else:
            functional_modules = []
        
        module_names = [m.get("æ¨¡å—åç§°", "") for m in functional_modules]
        
        process_name = process.get("name", "")
        if not any(name in process_name or process_name in name for name in module_names):
            issues.append("æµç¨‹ä¸åŠŸèƒ½æ¨¡å—ä¸åŒ¹é…")
        
        return issues
    
    async def _check_process_rationality(self, process: Dict[str, Any]) -> List[str]:
        """æ£€æŸ¥æµç¨‹åˆç†æ€§"""
        
        issues = []
        
        # æ£€æŸ¥è¾“å…¥è¾“å‡ºé€»è¾‘
        input_data = process.get("input_data", [])
        output_results = process.get("output_results", [])
        
        if not input_data and output_results:
            issues.append("æœ‰è¾“å‡ºä½†æ— è¾“å…¥ï¼Œå¯èƒ½é€»è¾‘ä¸åˆç†")
        
        # æ£€æŸ¥æ­¥éª¤é€»è¾‘
        main_steps = process.get("main_steps", [])
        if len(main_steps) > 10:
            issues.append("æµç¨‹æ­¥éª¤è¿‡å¤šï¼Œå»ºè®®æ‹†åˆ†")
        
        return issues
    
    async def _generate_process_details(
        self,
        validated_processes: List[Dict[str, Any]]
    ) -> List[ProcessDetails]:
        """ç”Ÿæˆæµç¨‹è¯¦æƒ…"""
        
        process_details = []
        
        for i, process in enumerate(validated_processes):
            detail = ProcessDetails(
                id=f"process_{i+1}",
                name=process.get("name", f"æµç¨‹{i+1}"),
                description=process.get("business_goal", ""),
                data_groups=process.get("input_data", []) + process.get("output_results", []),
                dependencies=self._extract_dependencies(process, validated_processes),
                inputs=process.get("input_data", []),
                outputs=process.get("output_results", []),
                business_rules=process.get("business_rules", []),
                complexity_indicators=process.get("complexity_indicators", {}),
                metadata={
                    "trigger_conditions": process.get("trigger_conditions", []),
                    "main_steps": process.get("main_steps", []),
                    "involved_roles": process.get("involved_roles", []),
                    "boundaries": process.get("boundaries", {}),
                    "validation": process.get("validation", {})
                }
            )
            
            process_details.append(detail)
        
        return process_details
    
    def _extract_dependencies(
        self,
        current_process: Dict[str, Any],
        all_processes: List[Dict[str, Any]]
    ) -> List[str]:
        """æå–æµç¨‹ä¾èµ–å…³ç³»"""
        
        dependencies = []
        current_inputs = set(current_process.get("input_data", []))
        
        for process in all_processes:
            if process == current_process:
                continue
            
            process_outputs = set(process.get("output_results", []))
            
            # å¦‚æœå½“å‰æµç¨‹çš„è¾“å…¥ä¾èµ–å…¶ä»–æµç¨‹çš„è¾“å‡º
            if current_inputs & process_outputs:
                dependencies.append(process.get("name", "æœªçŸ¥æµç¨‹"))
        
        return dependencies
    
    # è¾…åŠ©æ–¹æ³•
    def _format_requirement_analysis(self, requirement_analysis: Any) -> str:
        """æ ¼å¼åŒ–éœ€æ±‚åˆ†æç»“æœ"""
        
        formatted = []
        
        # å¤„ç†RequirementAnalysiså¯¹è±¡æˆ–å­—å…¸
        if hasattr(requirement_analysis, 'functional_modules'):
            functional_modules = requirement_analysis.functional_modules
            business_entities = requirement_analysis.business_entities
        elif isinstance(requirement_analysis, dict):
            functional_modules = requirement_analysis.get("functional_modules", [])
            business_entities = requirement_analysis.get("business_entities", {})
        else:
            functional_modules = []
            business_entities = {}
        
        if functional_modules:
            formatted.append("åŠŸèƒ½æ¨¡å—ï¼š")
            for module in functional_modules:
                formatted.append(f"- {module.get('æ¨¡å—åç§°', 'æœªçŸ¥')}: {module.get('åŠŸèƒ½æè¿°', '')}")
        
        if business_entities:
            formatted.append("\nä¸šåŠ¡å®ä½“ï¼š")
            for category, entities in business_entities.items():
                formatted.append(f"- {category}: {', '.join(entities)}")
        
        return "\n".join(formatted)
    
    async def _parse_process_candidates(self, response_content: str) -> List[Dict[str, Any]]:
        """è§£ææµç¨‹å€™é€‰"""
        try:
            import json
            logger.debug(f"ğŸ” LLMåŸå§‹å“åº”å†…å®¹: {response_content}")
            
            # ç®€åŒ–çš„JSONè§£æï¼Œå®é™…åº”è¯¥æ›´å¥å£®
            if "```json" in response_content:
                json_part = response_content.split("```json")[1].split("```")[0]
                parsed_result = json.loads(json_part)
                logger.info(f"âœ… æˆåŠŸè§£æJSONï¼Œè·å¾— {len(parsed_result)} ä¸ªæµç¨‹å€™é€‰")
                return parsed_result
            else:
                parsed_result = json.loads(response_content)
                logger.info(f"âœ… æˆåŠŸè§£æJSONï¼Œè·å¾— {len(parsed_result)} ä¸ªæµç¨‹å€™é€‰")
                return parsed_result
        except Exception as e:
            logger.error(f"âŒ è§£ææµç¨‹å€™é€‰å¤±è´¥: {e}")
            logger.error(f"å“åº”å†…å®¹: {response_content[:500]}...")
            return []
    
    async def _parse_data_flow_analysis(self, response_content: str) -> Dict[str, Any]:
        """è§£ææ•°æ®æµåˆ†æ"""
        try:
            import json
            if "```json" in response_content:
                json_part = response_content.split("```json")[1].split("```")[0]
                return json.loads(json_part)
            else:
                return json.loads(response_content)
        except Exception as e:
            logger.error(f"è§£ææ•°æ®æµåˆ†æå¤±è´¥: {e}")
            return {}
    
    async def _execute_task(self, task_name: str, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæµç¨‹è¯†åˆ«ä»»åŠ¡"""
        if task_name == "identify_processes":
            processes = await self.identify_processes(
                inputs["requirement_analysis"],
                inputs["project_info"]
            )
            return {
                "processes": processes,
                "process_count": len(processes),
                "task_status": "completed"
            }
        elif task_name == "extract_process_candidates":
            candidates = await self._extract_process_candidates(
                inputs["requirement_analysis"]
            )
            return {
                "candidates": candidates,
                "task_status": "completed"
            }
        elif task_name == "validate_processes":
            validated = await self._validate_processes(
                inputs["processes"],
                inputs["requirement_analysis"]
            )
            return {
                "validated_processes": validated,
                "task_status": "completed"
            }
        else:
            raise ValueError(f"æœªçŸ¥ä»»åŠ¡: {task_name}")


if __name__ == "__main__":
    # æµ‹è¯•æµç¨‹è¯†åˆ«æ™ºèƒ½ä½“
    async def test_process_identifier():
        agent = ProcessIdentifierAgent()
        
        # æµ‹è¯•éœ€æ±‚åˆ†æç»“æœ
        test_requirement_analysis = {
            "functional_modules": [
                {
                    "æ¨¡å—åç§°": "ç”¨æˆ·ç®¡ç†",
                    "åŠŸèƒ½æè¿°": "ç”¨æˆ·æ³¨å†Œã€ç™»å½•ã€ä¿¡æ¯ç»´æŠ¤"
                },
                {
                    "æ¨¡å—åç§°": "è®¢å•å¤„ç†",
                    "åŠŸèƒ½æè¿°": "è®¢å•åˆ›å»ºã€æŸ¥è¯¢ã€ä¿®æ”¹ã€å–æ¶ˆ"
                }
            ],
            "business_entities": {
                "ç”¨æˆ·è§’è‰²": ["å®¢æˆ·", "ç®¡ç†å‘˜"],
                "ä¸šåŠ¡å¯¹è±¡": ["ç”¨æˆ·", "è®¢å•", "äº§å“"]
            }
        }
        
        project_info = ProjectInfo(
            name="ç”µå•†ç³»ç»Ÿ",
            description="åœ¨çº¿è´­ç‰©å¹³å°",
            technology_stack=["Python", "Django", "MySQL"],
            business_domain="ç”µå•†"
        )
        
        processes = await agent.identify_processes(test_requirement_analysis, project_info)
        
        print(f"è¯†åˆ«å‡º {len(processes)} ä¸ªåŠŸèƒ½æµç¨‹ï¼š")
        for process in processes:
            print(f"- {process.name}: {process.description}")
    
    asyncio.run(test_process_identifier()) 