#!/usr/bin/env python3
"""
ç®€åŒ–çš„çŸ¥è¯†åº“è®¾ç½®è„šæœ¬
åŸºäºç°æœ‰æ–‡æ¡£è¿›è¡ŒåŸºç¡€å¤„ç†
"""

import asyncio
from pathlib import Path
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

class SimpleKnowledgeBaseSetup:
    def __init__(self):
        self.base_dir = Path("knowledge_base")
        
        # æ–‡æ¡£å¤„ç†é¡ºåºï¼ˆç®€åŒ–ç‰ˆï¼‰
        self.processing_plan = [
            {
                "category": "nesma",
                "filename": "NESMA_FPA_Method_v2.3.pdf",
                "title": "NESMAåŠŸèƒ½ç‚¹åˆ†ææ–¹æ³•v2.3",
                "priority": "high"
            },
            {
                "category": "cosmic", 
                "filename": "COSMICåº¦é‡æ‰‹å†ŒV5.0-part-1-åŸåˆ™ã€å®šä¹‰ä¸è§„åˆ™.pdf",
                "title": "COSMICåº¦é‡æ‰‹å†Œ-åŸåˆ™ä¸è§„åˆ™",
                "priority": "high"
            },
            {
                "category": "cosmic",
                "filename": "COSMICåº¦é‡æ‰‹å†ŒV5.0-part-2-æŒ‡å—.pdf", 
                "title": "COSMICåº¦é‡æ‰‹å†Œ-å®æ–½æŒ‡å—",
                "priority": "high"
            },
            {
                "category": "cosmic",
                "filename": "COSMICåº¦é‡æ‰‹å†ŒV5.0-part-3-æ¡ˆä¾‹.pdf",
                "title": "COSMICåº¦é‡æ‰‹å†Œ-æ¡ˆä¾‹é›†",
                "priority": "medium"
            }
        ]
    
    async def process_documents(self):
        """å¤„ç†æ–‡æ¡£å¹¶ç”Ÿæˆåˆ†å—"""
        
        print("ğŸš€ å¼€å§‹å¤„ç†çŸ¥è¯†åº“æ–‡æ¡£...")
        
        # ä¸­è‹±æ–‡ä¼˜åŒ–çš„åˆ†è¯å™¨
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", "ã€‚", ".", "!", "?", "ï¼›", ";", "ï¼š", ":"]
        )
        
        all_processed_docs = []
        
        for item in self.processing_plan:
            category = item["category"]
            filename = item["filename"]
            title = item["title"]
            
            file_path = self.base_dir / "documents" / category / filename
            
            if not file_path.exists():
                print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
                continue
            
            print(f"ğŸ”„ å¤„ç†æ–‡æ¡£: {title}")
            
            try:
                # ä½¿ç”¨ç®€å•çš„PDFåŠ è½½å™¨
                loader = PyPDFLoader(str(file_path))
                
                # åŠ è½½æ–‡æ¡£
                docs = loader.load()
                print(f"   ğŸ“„ åŠ è½½äº† {len(docs)} é¡µå†…å®¹")
                
                # æ–‡æœ¬åˆ†å—
                split_docs = text_splitter.split_documents(docs)
                print(f"   âœ‚ï¸ åˆ†å‰²ä¸º {len(split_docs)} ä¸ªæ–‡æ¡£å—")
                
                # æ·»åŠ å…ƒæ•°æ®
                for doc in split_docs:
                    doc.metadata.update({
                        "source_category": category,
                        "document_title": title,
                        "filename": filename,
                        "priority": item["priority"],
                        "chunk_id": f"{category}_{len(all_processed_docs)}"
                    })
                
                all_processed_docs.extend(split_docs)
                print(f"   âœ… å®Œæˆå¤„ç†: {title}")
                
            except Exception as e:
                print(f"   âŒ å¤„ç†å¤±è´¥ {title}: {e}")
                continue
        
        print(f"\nğŸ“Š æ–‡æ¡£å¤„ç†å®Œæˆ!")
        print(f"   æ€»æ–‡æ¡£å—æ•°: {len(all_processed_docs)}")
        
        # ç»Ÿè®¡å„ç±»æ–‡æ¡£
        categories = {}
        for doc in all_processed_docs:
            cat = doc.metadata.get("source_category", "unknown")
            categories[cat] = categories.get(cat, 0) + 1
        
        print("   åˆ†ç±»ç»Ÿè®¡:")
        for cat, count in categories.items():
            print(f"     {cat.upper()}: {count} ä¸ªæ–‡æ¡£å—")
        
        # ä¿å­˜ç¤ºä¾‹åˆ°æ–‡ä»¶
        await self.save_sample_chunks(all_processed_docs[:5])
        
        return all_processed_docs
    
    async def save_sample_chunks(self, sample_docs):
        """ä¿å­˜ç¤ºä¾‹æ–‡æ¡£å—åˆ°æ–‡ä»¶"""
        
        sample_file = self.base_dir / "sample_chunks.txt"
        
        with open(sample_file, 'w', encoding='utf-8') as f:
            f.write("çŸ¥è¯†åº“æ–‡æ¡£å—ç¤ºä¾‹\n")
            f.write("=" * 50 + "\n\n")
            
            for i, doc in enumerate(sample_docs, 1):
                f.write(f"æ–‡æ¡£å— {i}:\n")
                f.write(f"æ¥æº: {doc.metadata.get('document_title', 'æœªçŸ¥')}\n")
                f.write(f"ç±»åˆ«: {doc.metadata.get('source_category', 'æœªçŸ¥')}\n")
                f.write(f"å†…å®¹é¢„è§ˆ:\n{doc.page_content[:500]}...\n")
                f.write("-" * 50 + "\n\n")
        
        print(f"ğŸ“„ ç¤ºä¾‹æ–‡æ¡£å—å·²ä¿å­˜åˆ°: {sample_file}")
    
    async def test_basic_search(self, processed_docs):
        """æµ‹è¯•åŸºç¡€æœç´¢åŠŸèƒ½"""
        
        print("\nğŸ” æµ‹è¯•åŸºç¡€æ–‡æœ¬æœç´¢...")
        
        test_keywords = [
            "åŠŸèƒ½ç‚¹",
            "å¤æ‚åº¦", 
            "æ•°æ®ç§»åŠ¨",
            "ILF",
            "Entry"
        ]
        
        search_results = {}
        
        for keyword in test_keywords:
            matches = []
            for doc in processed_docs:
                if keyword.lower() in doc.page_content.lower():
                    matches.append({
                        "title": doc.metadata.get("document_title", "æœªçŸ¥"),
                        "category": doc.metadata.get("source_category", "æœªçŸ¥"),
                        "content_preview": doc.page_content[:200] + "..."
                    })
            
            search_results[keyword] = matches[:3]  # åªå–å‰3ä¸ªç»“æœ
            print(f"   å…³é”®è¯ '{keyword}': æ‰¾åˆ° {len(matches)} ä¸ªåŒ¹é…")
        
        # ä¿å­˜æœç´¢ç»“æœç¤ºä¾‹
        await self.save_search_results(search_results)
        
        return search_results
    
    async def save_search_results(self, results):
        """ä¿å­˜æœç´¢ç»“æœç¤ºä¾‹"""
        
        results_file = self.base_dir / "search_test_results.txt"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            f.write("åŸºç¡€æœç´¢æµ‹è¯•ç»“æœ\n")
            f.write("=" * 50 + "\n\n")
            
            for keyword, matches in results.items():
                f.write(f"å…³é”®è¯: {keyword}\n")
                f.write(f"åŒ¹é…æ•°é‡: {len(matches)}\n\n")
                
                for i, match in enumerate(matches, 1):
                    f.write(f"  ç»“æœ {i}:\n")
                    f.write(f"    æ–‡æ¡£: {match['title']}\n") 
                    f.write(f"    ç±»åˆ«: {match['category']}\n")
                    f.write(f"    å†…å®¹: {match['content_preview']}\n\n")
                
                f.write("-" * 30 + "\n\n")
        
        print(f"ğŸ” æœç´¢ç»“æœå·²ä¿å­˜åˆ°: {results_file}")

async def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸš€ é‡å­æ™ºèƒ½åŒ–åŠŸèƒ½ç‚¹ä¼°ç®—ç³»ç»Ÿ - ç®€åŒ–çŸ¥è¯†åº“è®¾ç½®")
    print("=" * 60)
    
    setup = SimpleKnowledgeBaseSetup()
    
    try:
        # å¤„ç†æ–‡æ¡£
        processed_docs = await setup.process_documents()
        
        if processed_docs:
            # æµ‹è¯•åŸºç¡€æœç´¢
            search_results = await setup.test_basic_search(processed_docs)
            
            print("\n" + "=" * 60)
            print("âœ… çŸ¥è¯†åº“åŸºç¡€è®¾ç½®å®Œæˆ!")
            print("\nğŸ’¡ åç»­å»ºè®®:")
            print("1. æ£€æŸ¥ç”Ÿæˆçš„sample_chunks.txtæ–‡ä»¶ï¼ŒéªŒè¯æ–‡æ¡£è§£æè´¨é‡")
            print("2. æŸ¥çœ‹search_test_results.txtï¼Œäº†è§£åŸºç¡€æœç´¢æ•ˆæœ")
            print("3. è€ƒè™‘è®¾ç½®å‘é‡å­˜å‚¨ä»¥æ”¯æŒè¯­ä¹‰æœç´¢")
            print("4. å¼€å§‹å¼€å‘åŠŸèƒ½ç‚¹ä¼°ç®—æ™ºèƒ½ä½“")
            
        else:
            print("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ–‡æ¡£ï¼Œè¯·æ£€æŸ¥æ–‡æ¡£è·¯å¾„å’Œæ ¼å¼")
            
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        
    print("\nğŸ‰ å¤„ç†å®Œæˆ!")

if __name__ == "__main__":
    asyncio.run(main()) 