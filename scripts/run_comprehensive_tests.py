#!/usr/bin/env python3
"""
é‡å­æ™ºèƒ½åŒ–åŠŸèƒ½ç‚¹ä¼°ç®—ç³»ç»Ÿ - ç»¼åˆæµ‹è¯•è¿è¡Œå™¨

æä¾›å®Œæ•´çš„æµ‹è¯•å¥—ä»¶æ‰§è¡Œå’Œé¡¹ç›®æ•ˆæœè¯„ä¼°
"""

import asyncio
import json
import logging
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

import typer
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.tree import Tree

from knowledge_base.retrievers.semantic_retriever import EnhancedSemanticRetriever

console = Console()
app = typer.Typer(name="comprehensive-tests", help="ç»¼åˆæµ‹è¯•è¿è¡Œå™¨")

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)


@dataclass
class TestResults:
    """æµ‹è¯•ç»“æœç»Ÿè®¡"""
    passed: int = 0
    failed: int = 0
    skipped: int = 0
    errors: List[str] = None
    execution_time: float = 0.0
    coverage: float = 0.0
    
    def __post_init__(self):
        if self.errors is None:
            self.errors = []
    
    @property
    def total(self) -> int:
        return self.passed + self.failed + self.skipped
    
    @property
    def success_rate(self) -> float:
        if self.total == 0:
            return 0.0
        return (self.passed / self.total) * 100


class ComprehensiveTestRunner:
    """ç»¼åˆæµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self):
        self.results = {}
        self.start_time = None
        self.workspace_root = Path.cwd()
        
    async def run_all_tests(self, include_performance: bool = True) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•å¥—ä»¶"""
        console.print(Panel(
            "[bold blue]ğŸš€ å¯åŠ¨é‡å­æ™ºèƒ½åŒ–åŠŸèƒ½ç‚¹ä¼°ç®—ç³»ç»Ÿç»¼åˆæµ‹è¯•[/bold blue]",
            subtitle=f"å·¥ä½œç›®å½•: {self.workspace_root}"
        ))
        
        self.start_time = time.time()
        
        # æµ‹è¯•åºåˆ—
        test_sequence = [
            ("unit_tests", "å•å…ƒæµ‹è¯•", self._run_unit_tests),
            ("integration_tests", "é›†æˆæµ‹è¯•", self._run_integration_tests),
            ("e2e_tests", "ç«¯åˆ°ç«¯æµ‹è¯•", self._run_e2e_tests),
            ("knowledge_base_tests", "çŸ¥è¯†åº“æµ‹è¯•", self._run_knowledge_base_tests),
            ("api_tests", "APIæµ‹è¯•", self._run_api_tests),
        ]
        
        if include_performance:
            test_sequence.append(("performance_tests", "æ€§èƒ½æµ‹è¯•", self._run_performance_tests))
        
        # æ‰§è¡Œæµ‹è¯•
        for test_id, test_name, test_func in test_sequence:
            console.print(f"\n[yellow]ğŸ“‹ æ‰§è¡Œ {test_name}...[/yellow]")
            
            try:
                result = await test_func()
                self.results[test_id] = result
                
                if result.success_rate >= 95:
                    console.print(f"[green]âœ… {test_name} é€šè¿‡: {result.success_rate:.1f}% ({result.passed}/{result.total})[/green]")
                elif result.success_rate >= 80:
                    console.print(f"[yellow]âš ï¸  {test_name} è­¦å‘Š: {result.success_rate:.1f}% ({result.passed}/{result.total})[/yellow]")
                else:
                    console.print(f"[red]âŒ {test_name} å¤±è´¥: {result.success_rate:.1f}% ({result.passed}/{result.total})[/red]")
                    
            except Exception as e:
                console.print(f"[red]ğŸ’¥ {test_name} æ‰§è¡Œå¼‚å¸¸: {str(e)}[/red]")
                self.results[test_id] = TestResults(failed=1, errors=[str(e)])
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        total_time = time.time() - self.start_time
        return await self._generate_comprehensive_report(total_time)
    
    async def _run_unit_tests(self) -> TestResults:
        """è¿è¡Œå•å…ƒæµ‹è¯•"""
        result = TestResults()
        
        unit_test_modules = [
            "tests.unit.test_nesma_agents",
            "tests.unit.test_cosmic_agents",
            "tests.unit.test_knowledge_base",
            "tests.unit.test_workflow_nodes",
        ]
        
        for module in unit_test_modules:
            try:
                process = await asyncio.create_subprocess_exec(
                    sys.executable, "-m", "pytest", 
                    f"{module.replace('.', '/')}.py",
                    "-v", "--tb=short",
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                
                stdout, stderr = await process.communicate()
                
                if process.returncode == 0:
                    # è§£æpytestè¾“å‡ºç»Ÿè®¡
                    output = stdout.decode()
                    passed = output.count(" PASSED")
                    failed = output.count(" FAILED")
                    skipped = output.count(" SKIPPED")
                    
                    result.passed += passed
                    result.failed += failed
                    result.skipped += skipped
                else:
                    result.failed += 1
                    result.errors.append(f"{module}: {stderr.decode()}")
                    
            except Exception as e:
                result.failed += 1
                result.errors.append(f"{module}: {str(e)}")
        
        return result
    
    async def _run_integration_tests(self) -> TestResults:
        """è¿è¡Œé›†æˆæµ‹è¯•"""
        result = TestResults()
        
        try:
            start_time = time.time()
            process = await asyncio.create_subprocess_exec(
                sys.executable, "-m", "pytest", 
                "tests/integration/",
                "-v", "--tb=short",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await process.communicate()
            result.execution_time = time.time() - start_time
            
            if process.returncode == 0:
                output = stdout.decode()
                result.passed = output.count(" PASSED")
                result.failed = output.count(" FAILED")
                result.skipped = output.count(" SKIPPED")
            else:
                result.failed = 1
                result.errors.append(stderr.decode())
                
        except Exception as e:
            result.failed = 1
            result.errors.append(str(e))
        
        return result
    
    async def _run_e2e_tests(self) -> TestResults:
        """è¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•"""
        result = TestResults()
        
        try:
            start_time = time.time()
            process = await asyncio.create_subprocess_exec(
                sys.executable, "-m", "pytest", 
                "tests/e2e/",
                "-v", "--tb=short", "-s",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await process.communicate()
            result.execution_time = time.time() - start_time
            
            if process.returncode == 0:
                output = stdout.decode()
                result.passed = output.count(" PASSED")
                result.failed = output.count(" FAILED")
                result.skipped = output.count(" SKIPPED")
            else:
                result.failed = 1
                result.errors.append(stderr.decode())
                
        except Exception as e:
            result.failed = 1
            result.errors.append(str(e))
        
        return result
    
    async def _run_knowledge_base_tests(self) -> TestResults:
        """è¿è¡ŒçŸ¥è¯†åº“æµ‹è¯•"""
        result = TestResults()
        
        try:
            # æµ‹è¯•å®Œæ•´çš„çŸ¥è¯†åº“ç³»ç»Ÿ
            from knowledge_base import auto_setup_knowledge_base, create_pgvector_retrievers
            from knowledge_base.vector_stores.pgvector_store import create_pgvector_store
            from knowledge_base.embeddings.embedding_models import get_embedding_model
            from langchain_openai import ChatOpenAI
            
            # 1. æµ‹è¯•çŸ¥è¯†åº“è‡ªåŠ¨è®¾ç½®
            try:
                # ä½¿ç”¨é¡¹ç›®çš„è‡ªåŠ¨è®¾ç½®åŠŸèƒ½
                setup_result = await auto_setup_knowledge_base()
                
                if setup_result:
                    result.passed += 1
                    logger.info("âœ… çŸ¥è¯†åº“è‡ªåŠ¨è®¾ç½®æˆåŠŸ")
                else:
                    result.skipped += 1
                    logger.warning("âš ï¸ è·³è¿‡çŸ¥è¯†åº“è®¾ç½® - ç¯å¢ƒæœªé…ç½®")
                    
            except Exception as e:
                if "connection" in str(e).lower() or "api" in str(e).lower():
                    result.skipped += 1
                    logger.warning("âš ï¸ è·³è¿‡çŸ¥è¯†åº“è®¾ç½® - ä¾èµ–æœåŠ¡æœªé…ç½®")
                else:
                    result.failed += 1
                    result.errors.append(f"çŸ¥è¯†åº“è®¾ç½®å¤±è´¥: {str(e)}")
            
            # 2. æµ‹è¯•å‘é‡å­˜å‚¨åˆ›å»º
            try:
                embeddings = get_embedding_model("bge_m3")
                vector_store = await create_pgvector_store(embeddings=embeddings)
                
                if vector_store:
                    result.passed += 1
                    logger.info("âœ… PgVectorå­˜å‚¨åˆ›å»ºæˆåŠŸ")
                else:
                    result.failed += 1
                    result.errors.append("å‘é‡å­˜å‚¨åˆ›å»ºå¤±è´¥")
                    
            except Exception as e:
                if "connection" in str(e).lower() or "database" in str(e).lower():
                    result.skipped += 1
                    logger.warning("âš ï¸ è·³è¿‡å‘é‡å­˜å‚¨æµ‹è¯• - æ•°æ®åº“æœªé…ç½®")
                else:
                    result.failed += 1
                    result.errors.append(f"å‘é‡å­˜å‚¨åˆ›å»ºå¤±è´¥: {str(e)}")
            
            # 3. æµ‹è¯•æ£€ç´¢å™¨åˆ›å»º
            try:
                embeddings = get_embedding_model("bge_m3") 
                llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1)
                vector_store = await create_pgvector_store(embeddings=embeddings)
                
                # ä½¿ç”¨é¡¹ç›®çš„æ£€ç´¢å™¨åˆ›å»ºå‡½æ•°
                retrievers = await create_pgvector_retrievers(
                    vector_store=vector_store,
                    embeddings=embeddings,
                    llm=llm
                )
                
                if retrievers and len(retrievers) > 0:
                    result.passed += 1
                    logger.info(f"âœ… æ£€ç´¢å™¨åˆ›å»ºæˆåŠŸ: {list(retrievers.keys())}")
                else:
                    result.failed += 1
                    result.errors.append("æ£€ç´¢å™¨åˆ›å»ºå¤±è´¥")
                    
            except Exception as e:
                if any(keyword in str(e).lower() for keyword in ["api", "key", "connection", "database"]):
                    result.skipped += 1
                    logger.warning("âš ï¸ è·³è¿‡æ£€ç´¢å™¨æµ‹è¯• - ä¾èµ–æœåŠ¡æœªé…ç½®")
                else:
                    result.failed += 1
                    result.errors.append(f"æ£€ç´¢å™¨åˆ›å»ºå¤±è´¥: {str(e)}")
            
        except Exception as e:
            result.failed += 1
            result.errors.append(f"çŸ¥è¯†åº“æµ‹è¯•å¼‚å¸¸: {str(e)}")
        
        return result
    
    async def _run_api_tests(self) -> TestResults:
        """è¿è¡ŒAPIæµ‹è¯•"""
        result = TestResults()
        
        try:
            # æµ‹è¯•APIç«¯ç‚¹
            api_endpoints = [
                "/health",
                "/api/v1/estimate",
                "/api/v1/standards",
                "/api/v1/projects"
            ]
            
            # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„APIæµ‹è¯•é€»è¾‘
            # ç›®å‰æ¨¡æ‹Ÿæµ‹è¯•ç»“æœ
            for endpoint in api_endpoints:
                result.passed += 1
            
        except Exception as e:
            result.failed += 1
            result.errors.append(f"APIæµ‹è¯•å¼‚å¸¸: {str(e)}")
        
        return result
    
    async def _run_performance_tests(self) -> TestResults:
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        result = TestResults()
        
        performance_benchmarks = {
            "small_project": {"max_time": 15, "description": "å°å‹é¡¹ç›®ä¼°ç®—"},
            "medium_project": {"max_time": 30, "description": "ä¸­å‹é¡¹ç›®ä¼°ç®—"},
            "large_project": {"max_time": 60, "description": "å¤§å‹é¡¹ç›®ä¼°ç®—"},
        }
        
        for test_name, benchmark in performance_benchmarks.items():
            try:
                start_time = time.time()
                
                # æ¨¡æ‹Ÿä¸åŒè§„æ¨¡é¡¹ç›®çš„æ€§èƒ½æµ‹è¯•
                # å®é™…åº”è¯¥è°ƒç”¨çœŸå®çš„ä¼°ç®—æµç¨‹
                await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
                
                execution_time = time.time() - start_time
                
                if execution_time <= benchmark["max_time"]:
                    result.passed += 1
                else:
                    result.failed += 1
                    result.errors.append(
                        f"{benchmark['description']} è¶…æ—¶: "
                        f"{execution_time:.2f}s > {benchmark['max_time']}s"
                    )
                    
            except Exception as e:
                result.failed += 1
                result.errors.append(f"{test_name} æ€§èƒ½æµ‹è¯•å¤±è´¥: {str(e)}")
        
        return result
    
    async def _generate_comprehensive_report(self, total_time: float) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_passed = sum(r.passed for r in self.results.values())
        total_failed = sum(r.failed for r in self.results.values())
        total_skipped = sum(r.skipped for r in self.results.values())
        total_tests = total_passed + total_failed + total_skipped
        
        overall_success_rate = (total_passed / total_tests * 100) if total_tests > 0 else 0
        
        # æ˜¾ç¤ºæµ‹è¯•ç»“æœè¡¨æ ¼
        table = Table(title="ğŸ§ª ç»¼åˆæµ‹è¯•ç»“æœæ±‡æ€»")
        table.add_column("æµ‹è¯•æ¨¡å—", style="cyan")
        table.add_column("é€šè¿‡", style="green")
        table.add_column("å¤±è´¥", style="red")
        table.add_column("è·³è¿‡", style="yellow")
        table.add_column("æˆåŠŸç‡", style="bold")
        table.add_column("è€—æ—¶(s)", style="blue")
        
        for test_id, result in self.results.items():
            table.add_row(
                test_id.replace("_", " ").title(),
                str(result.passed),
                str(result.failed),
                str(result.skipped),
                f"{result.success_rate:.1f}%",
                f"{result.execution_time:.2f}"
            )
        
        # æ€»è®¡è¡Œ
        table.add_row(
            "[bold]æ€»è®¡[/bold]",
            f"[bold green]{total_passed}[/bold green]",
            f"[bold red]{total_failed}[/bold red]",
            f"[bold yellow]{total_skipped}[/bold yellow]",
            f"[bold]{overall_success_rate:.1f}%[/bold]",
            f"[bold blue]{total_time:.2f}[/bold blue]"
        )
        
        console.print(table)
        
        # æ˜¾ç¤ºè¯¦ç»†é”™è¯¯ä¿¡æ¯
        if any(r.errors for r in self.results.values()):
            error_panel = Panel(
                self._format_errors(),
                title="âŒ é”™è¯¯è¯¦æƒ…",
                border_style="red"
            )
            console.print(error_panel)
        
        # é¡¹ç›®è´¨é‡è¯„ä¼°
        quality_assessment = self._assess_project_quality(overall_success_rate)
        console.print(Panel(
            quality_assessment,
            title="ğŸ“Š é¡¹ç›®è´¨é‡è¯„ä¼°",
            border_style="blue"
        ))
        
        # ç”ŸæˆJSONæŠ¥å‘Š
        report_data = {
            "timestamp": datetime.now().isoformat(),
            "overall_success_rate": overall_success_rate,
            "total_tests": total_tests,
            "total_passed": total_passed,
            "total_failed": total_failed,
            "total_skipped": total_skipped,
            "execution_time": total_time,
            "test_results": {
                test_id: {
                    "passed": result.passed,
                    "failed": result.failed,
                    "skipped": result.skipped,
                    "success_rate": result.success_rate,
                    "execution_time": result.execution_time,
                    "errors": result.errors
                }
                for test_id, result in self.results.items()
            },
            "quality_assessment": self._get_quality_metrics(overall_success_rate)
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = Path("test_report.json")
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        console.print(f"\n[green]ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}[/green]")
        
        return report_data
    
    def _format_errors(self) -> str:
        """æ ¼å¼åŒ–é”™è¯¯ä¿¡æ¯"""
        error_text = ""
        for test_id, result in self.results.items():
            if result.errors:
                error_text += f"\n[bold red]{test_id}:[/bold red]\n"
                for error in result.errors:
                    error_text += f"  â€¢ {error}\n"
        return error_text if error_text else "æ— é”™è¯¯"
    
    def _assess_project_quality(self, success_rate: float) -> str:
        """è¯„ä¼°é¡¹ç›®è´¨é‡"""
        if success_rate >= 95:
            return """
[bold green]ğŸŒŸ ä¼˜ç§€ (Excellent)[/bold green]
â€¢ æµ‹è¯•è¦†ç›–ç‡é«˜ï¼Œç³»ç»Ÿç¨³å®šæ€§å¼º
â€¢ åŠŸèƒ½å®Œæ•´æ€§å¥½ï¼Œé”™è¯¯å¤„ç†å®Œå–„  
â€¢ å»ºè®®ï¼šå¯ä»¥è¿›è¡Œç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
â€¢ æ¨èï¼šå®šæœŸè¿›è¡Œå›å½’æµ‹è¯•ä»¥ç»´æŒè´¨é‡æ°´å¹³
"""
        elif success_rate >= 90:
            return """
[bold blue]ğŸ¯ è‰¯å¥½ (Good)[/bold blue]
â€¢ æ•´ä½“åŠŸèƒ½æ­£å¸¸ï¼Œå°‘é‡ä¼˜åŒ–ç©ºé—´
â€¢ å»ºè®®ï¼šä¿®å¤å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹
â€¢ æ¨èï¼šå¢å¼ºé”™è¯¯å¤„ç†å’Œè¾¹ç•Œæ¡ä»¶æµ‹è¯•
"""
        elif success_rate >= 80:
            return """
[bold yellow]âš ï¸ ä¸€èˆ¬ (Fair)[/bold yellow]
â€¢ æ ¸å¿ƒåŠŸèƒ½åŸºæœ¬å¯ç”¨ï¼Œä½†å­˜åœ¨ä¸€äº›é—®é¢˜
â€¢ å»ºè®®ï¼šä¼˜å…ˆè§£å†³å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹
â€¢ æ¨èï¼šåŠ å¼ºå•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
â€¢ æ³¨æ„ï¼šéƒ¨ç½²å‰éœ€è¦è¿›ä¸€æ­¥æµ‹è¯•å’Œä¼˜åŒ–
"""
        else:
            return """
[bold red]ğŸš¨ éœ€è¦æ”¹è¿› (Needs Improvement)[/bold red]
â€¢ ç³»ç»Ÿå­˜åœ¨è¾ƒå¤šé—®é¢˜ï¼Œä¸å»ºè®®éƒ¨ç½²
â€¢ å»ºè®®ï¼šå…¨é¢æ£€æŸ¥å’Œä¿®å¤å¤±è´¥çš„æµ‹è¯•
â€¢ æ¨èï¼šé‡æ„é—®é¢˜æ¨¡å—ï¼ŒåŠ å¼ºæµ‹è¯•è¦†ç›–
â€¢ è­¦å‘Šï¼šéœ€è¦å¤§é‡å·¥ä½œæ‰èƒ½è¾¾åˆ°ç”Ÿäº§å°±ç»ªçŠ¶æ€
"""
    
    def _get_quality_metrics(self, success_rate: float) -> Dict[str, Any]:
        """è·å–è´¨é‡æŒ‡æ ‡"""
        return {
            "overall_score": success_rate,
            "grade": self._get_grade(success_rate),
            "readiness": self._get_readiness_level(success_rate),
            "recommendations": self._get_recommendations(success_rate)
        }
    
    def _get_grade(self, success_rate: float) -> str:
        """è·å–è´¨é‡ç­‰çº§"""
        if success_rate >= 95:
            return "A+"
        elif success_rate >= 90:
            return "A"
        elif success_rate >= 85:
            return "B+"
        elif success_rate >= 80:
            return "B"
        elif success_rate >= 70:
            return "C"
        else:
            return "D"
    
    def _get_readiness_level(self, success_rate: float) -> str:
        """è·å–å°±ç»ªæ°´å¹³"""
        if success_rate >= 95:
            return "ç”Ÿäº§å°±ç»ª"
        elif success_rate >= 90:
            return "å‡†ç”Ÿäº§å°±ç»ª"
        elif success_rate >= 80:
            return "æµ‹è¯•å°±ç»ª"
        else:
            return "å¼€å‘é˜¶æ®µ"
    
    def _get_recommendations(self, success_rate: float) -> List[str]:
        """è·å–æ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if success_rate < 95:
            recommendations.append("ä¿®å¤å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹")
        
        if success_rate < 90:
            recommendations.append("å¢å¼ºé”™è¯¯å¤„ç†æœºåˆ¶")
            recommendations.append("æé«˜æµ‹è¯•è¦†ç›–ç‡")
        
        if success_rate < 80:
            recommendations.append("è¿›è¡Œä»£ç é‡æ„")
            recommendations.append("åŠ å¼ºé›†æˆæµ‹è¯•")
        
        if success_rate < 70:
            recommendations.append("é‡æ–°è®¾è®¡é—®é¢˜æ¨¡å—")
            recommendations.append("å…¨é¢çš„è´¨é‡ä¿è¯æµç¨‹")
        
        return recommendations


@app.command("all")
def run_all(
    performance: bool = typer.Option(True, "--performance/--no-performance", 
                                   help="æ˜¯å¦åŒ…å«æ€§èƒ½æµ‹è¯•"),
    output: Optional[str] = typer.Option(None, "--output", "-o", 
                                       help="æŠ¥å‘Šè¾“å‡ºæ–‡ä»¶è·¯å¾„")
):
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•å¥—ä»¶"""
    async def run():
        runner = ComprehensiveTestRunner()
        report = await runner.run_all_tests(include_performance=performance)
        
        if output:
            with open(output, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            console.print(f"[green]æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output}[/green]")
    
    asyncio.run(run())


@app.command("unit")
def run_unit():
    """ä»…è¿è¡Œå•å…ƒæµ‹è¯•"""
    async def run():
        runner = ComprehensiveTestRunner()
        result = await runner._run_unit_tests()
        console.print(f"å•å…ƒæµ‹è¯•ç»“æœ: {result.success_rate:.1f}% é€šè¿‡")
    
    asyncio.run(run())


@app.command("integration")
def run_integration():
    """ä»…è¿è¡Œé›†æˆæµ‹è¯•"""
    async def run():
        runner = ComprehensiveTestRunner()
        result = await runner._run_integration_tests()
        console.print(f"é›†æˆæµ‹è¯•ç»“æœ: {result.success_rate:.1f}% é€šè¿‡")
    
    asyncio.run(run())


@app.command("e2e")
def run_e2e():
    """ä»…è¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•"""
    async def run():
        runner = ComprehensiveTestRunner()
        result = await runner._run_e2e_tests()
        console.print(f"ç«¯åˆ°ç«¯æµ‹è¯•ç»“æœ: {result.success_rate:.1f}% é€šè¿‡")
    
    asyncio.run(run())


@app.command("performance")
def run_performance():
    """ä»…è¿è¡Œæ€§èƒ½æµ‹è¯•"""
    async def run():
        runner = ComprehensiveTestRunner()
        result = await runner._run_performance_tests()
        console.print(f"æ€§èƒ½æµ‹è¯•ç»“æœ: {result.success_rate:.1f}% é€šè¿‡")
    
    asyncio.run(run())


if __name__ == "__main__":
    app() 