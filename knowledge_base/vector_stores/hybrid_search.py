"""
é‡å­æ™ºèƒ½åŒ–åŠŸèƒ½ç‚¹ä¼°ç®—ç³»ç»Ÿ - æ··åˆæ£€ç´¢ç­–ç•¥

ç»“åˆè¯­ä¹‰æœç´¢å’Œå…³é”®è¯æœç´¢çš„æ··åˆæ£€ç´¢æ–¹æ¡ˆ
"""

import logging
from typing import List, Dict, Any, Optional, Union, Tuple
from datetime import datetime
import re
from collections import defaultdict

from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_core.vectorstores import VectorStore

logger = logging.getLogger(__name__)

# å®‰è£…æ—¶å¯èƒ½éœ€è¦ï¼špip install rank-bm25
try:
    from rank_bm25 import BM25Okapi
    BM25_AVAILABLE = True
except ImportError:
    BM25_AVAILABLE = False
    logger.warning("rank-bm25 æœªå®‰è£…ï¼Œå…³é”®è¯æœç´¢åŠŸèƒ½å°†å—é™")


class HybridSearchStrategy:
    """æ··åˆæœç´¢ç­–ç•¥åŸºç±»"""
    
    def __init__(
        self,
        vector_store: VectorStore,
        embeddings: Embeddings,
        semantic_weight: float = 0.7,
        keyword_weight: float = 0.3
    ):
        self.vector_store = vector_store
        self.embeddings = embeddings
        self.semantic_weight = semantic_weight
        self.keyword_weight = keyword_weight
        
        # éªŒè¯æƒé‡
        if abs(semantic_weight + keyword_weight - 1.0) > 0.001:
            raise ValueError("è¯­ä¹‰æƒé‡å’Œå…³é”®è¯æƒé‡ä¹‹å’Œå¿…é¡»ç­‰äº1.0")
        
        # BM25ç´¢å¼•ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._bm25_index = None
        self._bm25_documents: List[Document] = []
        self._bm25_corpus: List[List[str]] = []
        
    def build_bm25_index(self, documents: List[Document]):
        """æ„å»ºBM25ç´¢å¼•"""
        
        if not BM25_AVAILABLE:
            logger.warning("BM25ä¸å¯ç”¨ï¼Œè·³è¿‡å…³é”®è¯ç´¢å¼•æ„å»º")
            return
        
        logger.info(f"ğŸ”¨ æ„å»ºBM25ç´¢å¼•ï¼Œæ–‡æ¡£æ•°: {len(documents)}")
        
        self._bm25_documents = documents
        self._bm25_corpus = []
        
        for doc in documents:
            # åˆ†è¯å¤„ç†
            tokens = self._tokenize(doc.page_content)
            self._bm25_corpus.append(tokens)
        
        # åˆ›å»ºBM25ç´¢å¼•
        self._bm25_index = BM25Okapi(self._bm25_corpus)
        
        logger.info("âœ… BM25ç´¢å¼•æ„å»ºå®Œæˆ")
    
    def _tokenize(self, text: str) -> List[str]:
        """æ–‡æœ¬åˆ†è¯"""
        
        # ç®€å•çš„ä¸­è‹±æ–‡åˆ†è¯
        # å¯¹äºç”Ÿäº§ç¯å¢ƒï¼Œå»ºè®®ä½¿ç”¨jiebaç­‰ä¸“ä¸šåˆ†è¯å·¥å…·
        
        # è‹±æ–‡å•è¯åˆ†å‰²
        words = re.findall(r'\b\w+\b', text.lower())
        
        # ä¸­æ–‡å­—ç¬¦åˆ†å‰²
        chinese_chars = re.findall(r'[\u4e00-\u9fff]', text)
        
        # åˆå¹¶ç»“æœ
        tokens = words + chinese_chars
        
        # è¿‡æ»¤åœç”¨è¯
        stop_words = {'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'è¿™', 'é‚£', 'ä¸€ä¸ª', 'ä¸€ç§'}
        tokens = [token for token in tokens if token not in stop_words]
        
        return tokens
    
    def semantic_search(
        self,
        query: str,
        k: int = 10,
        filter_metadata: Optional[Dict[str, Any]] = None
    ) -> List[Tuple[Document, float]]:
        """è¯­ä¹‰æœç´¢"""
        
        try:
            if filter_metadata:
                results = self.vector_store.similarity_search_with_score(
                    query=query,
                    k=k,
                    filter=filter_metadata
                )
            else:
                results = self.vector_store.similarity_search_with_score(
                    query=query,
                    k=k
                )
            
            logger.debug(f"è¯­ä¹‰æœç´¢è¿”å› {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
            return []
    
    def keyword_search(
        self,
        query: str,
        k: int = 10,
        filter_metadata: Optional[Dict[str, Any]] = None
    ) -> List[Tuple[Document, float]]:
        """å…³é”®è¯æœç´¢ï¼ˆBM25ï¼‰"""
        
        if not BM25_AVAILABLE or not self._bm25_index:
            logger.warning("BM25ç´¢å¼•ä¸å¯ç”¨ï¼Œå›é€€åˆ°ç®€å•æ–‡æœ¬åŒ¹é…")
            return self._simple_text_search(query, k, filter_metadata)
        
        try:
            # åˆ†è¯æŸ¥è¯¢
            query_tokens = self._tokenize(query)
            
            if not query_tokens:
                logger.warning("æŸ¥è¯¢è¯ä¸ºç©ºï¼Œè¿”å›ç©ºç»“æœ")
                return []
            
            # BM25è¯„åˆ†
            scores = self._bm25_index.get_scores(query_tokens)
            
            # è·å–top-kç»“æœ
            top_indices = sorted(
                range(len(scores)),
                key=lambda i: scores[i],
                reverse=True
            )[:k]
            
            results = []
            for idx in top_indices:
                doc = self._bm25_documents[idx]
                score = float(scores[idx])
                
                # åº”ç”¨å…ƒæ•°æ®è¿‡æ»¤
                if filter_metadata:
                    if not all(
                        doc.metadata.get(key) == value
                        for key, value in filter_metadata.items()
                    ):
                        continue
                
                results.append((doc, score))
            
            logger.debug(f"å…³é”®è¯æœç´¢è¿”å› {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            logger.error(f"âŒ å…³é”®è¯æœç´¢å¤±è´¥: {e}")
            return []
    
    def _simple_text_search(
        self,
        query: str,
        k: int,
        filter_metadata: Optional[Dict[str, Any]] = None
    ) -> List[Tuple[Document, float]]:
        """ç®€å•æ–‡æœ¬åŒ¹é…æœç´¢ï¼ˆBM25ä¸å¯ç”¨æ—¶çš„åå¤‡æ–¹æ¡ˆï¼‰"""
        
        query_lower = query.lower()
        results = []
        
        for doc in self._bm25_documents:
            # åº”ç”¨å…ƒæ•°æ®è¿‡æ»¤
            if filter_metadata:
                if not all(
                    doc.metadata.get(key) == value
                    for key, value in filter_metadata.items()
                ):
                    continue
            
            # è®¡ç®—ç®€å•çš„æ–‡æœ¬åŒ¹é…åˆ†æ•°
            content_lower = doc.page_content.lower()
            score = 0.0
            
            # åŸºäºå…³é”®è¯å‡ºç°æ¬¡æ•°è®¡ç®—åˆ†æ•°
            for token in self._tokenize(query):
                if token in content_lower:
                    score += content_lower.count(token)
            
            if score > 0:
                results.append((doc, score))
        
        # æŒ‰åˆ†æ•°æ’åº
        results.sort(key=lambda x: x[1], reverse=True)
        
        return results[:k]
    
    def hybrid_search(
        self,
        query: str,
        k: int = 10,
        filter_metadata: Optional[Dict[str, Any]] = None,
        rerank: bool = True
    ) -> List[Tuple[Document, float]]:
        """æ··åˆæœç´¢"""
        
        # è·å–æ›´å¤šå€™é€‰ç»“æœè¿›è¡Œèåˆ
        candidate_k = min(k * 3, 50)
        
        # è¯­ä¹‰æœç´¢
        semantic_results = self.semantic_search(
            query=query,
            k=candidate_k,
            filter_metadata=filter_metadata
        )
        
        # å…³é”®è¯æœç´¢
        keyword_results = self.keyword_search(
            query=query,
            k=candidate_k,
            filter_metadata=filter_metadata
        )
        
        # èåˆç»“æœ
        fused_results = self._fuse_results(
            semantic_results,
            keyword_results,
            query
        )
        
        # é‡æ’åº
        if rerank and len(fused_results) > k:
            fused_results = self._rerank_results(fused_results, query)
        
        # è¿”å›top-k
        return fused_results[:k]
    
    def _fuse_results(
        self,
        semantic_results: List[Tuple[Document, float]],
        keyword_results: List[Tuple[Document, float]],
        query: str
    ) -> List[Tuple[Document, float]]:
        """èåˆæœç´¢ç»“æœ"""
        
        # å½’ä¸€åŒ–åˆ†æ•°
        semantic_normalized = self._normalize_scores(semantic_results)
        keyword_normalized = self._normalize_scores(keyword_results)
        
        # åˆå¹¶ç»“æœï¼ŒæŒ‰æ–‡æ¡£å†…å®¹å»é‡
        doc_scores: Dict[str, Tuple[Document, float, float]] = {}
        
        # å¤„ç†è¯­ä¹‰æœç´¢ç»“æœ
        for doc, score in semantic_normalized:
            doc_id = self._get_document_id(doc)
            doc_scores[doc_id] = (doc, score, 0.0)
        
        # å¤„ç†å…³é”®è¯æœç´¢ç»“æœ
        for doc, score in keyword_normalized:
            doc_id = self._get_document_id(doc)
            if doc_id in doc_scores:
                # æ–‡æ¡£å·²å­˜åœ¨ï¼Œæ›´æ–°å…³é”®è¯åˆ†æ•°
                existing_doc, sem_score, _ = doc_scores[doc_id]
                doc_scores[doc_id] = (existing_doc, sem_score, score)
            else:
                # æ–°æ–‡æ¡£ï¼Œä»…æœ‰å…³é”®è¯åˆ†æ•°
                doc_scores[doc_id] = (doc, 0.0, score)
        
        # è®¡ç®—æ··åˆåˆ†æ•°
        hybrid_results = []
        for doc_id, (doc, sem_score, kw_score) in doc_scores.items():
            hybrid_score = (
                self.semantic_weight * sem_score +
                self.keyword_weight * kw_score
            )
            hybrid_results.append((doc, hybrid_score))
        
        # æŒ‰æ··åˆåˆ†æ•°æ’åº
        hybrid_results.sort(key=lambda x: x[1], reverse=True)
        
        logger.debug(f"æ··åˆæœç´¢èåˆäº† {len(hybrid_results)} ä¸ªå”¯ä¸€ç»“æœ")
        return hybrid_results
    
    def _normalize_scores(
        self,
        results: List[Tuple[Document, float]]
    ) -> List[Tuple[Document, float]]:
        """å½’ä¸€åŒ–åˆ†æ•°åˆ°[0,1]èŒƒå›´"""
        
        if not results:
            return []
        
        scores = [score for _, score in results]
        min_score = min(scores)
        max_score = max(scores)
        
        if max_score == min_score:
            # æ‰€æœ‰åˆ†æ•°ç›¸åŒï¼Œè¿”å›ç›¸åŒçš„å½’ä¸€åŒ–åˆ†æ•°
            return [(doc, 1.0) for doc, _ in results]
        
        # Min-Maxå½’ä¸€åŒ–
        normalized_results = []
        for doc, score in results:
            normalized_score = (score - min_score) / (max_score - min_score)
            normalized_results.append((doc, normalized_score))
        
        return normalized_results
    
    def _get_document_id(self, doc: Document) -> str:
        """è·å–æ–‡æ¡£å”¯ä¸€æ ‡è¯†"""
        
        # åŸºäºå†…å®¹å’Œä¸»è¦å…ƒæ•°æ®ç”ŸæˆID
        content_hash = hash(doc.page_content[:500])  # ä½¿ç”¨å‰500å­—ç¬¦é¿å…è¿‡é•¿
        source = doc.metadata.get('source', 'unknown')
        chunk_index = doc.metadata.get('chunk_index', 0)
        
        return f"{source}_{chunk_index}_{content_hash}"
    
    def _rerank_results(
        self,
        results: List[Tuple[Document, float]],
        query: str
    ) -> List[Tuple[Document, float]]:
        """é‡æ’åºç»“æœ"""
        
        # ç®€å•çš„é‡æ’åºç­–ç•¥ï¼šæ ¹æ®æŸ¥è¯¢è¯åœ¨æ–‡æ¡£ä¸­çš„å‡ºç°æƒ…å†µè°ƒæ•´åˆ†æ•°
        query_terms = set(self._tokenize(query))
        
        reranked_results = []
        for doc, score in results:
            doc_terms = set(self._tokenize(doc.page_content))
            
            # è®¡ç®—æŸ¥è¯¢è¯è¦†ç›–ç‡
            if query_terms:
                coverage = len(query_terms.intersection(doc_terms)) / len(query_terms)
            else:
                coverage = 0.0
            
            # è°ƒæ•´åˆ†æ•°ï¼ˆåŠ æƒç»„åˆï¼‰
            boosted_score = score * (1 + coverage * 0.2)  # æœ€å¤šæå‡20%
            
            reranked_results.append((doc, boosted_score))
        
        # é‡æ–°æ’åº
        reranked_results.sort(key=lambda x: x[1], reverse=True)
        
        return reranked_results


class NESMAHybridSearch(HybridSearchStrategy):
    """NESMAä¸“ç”¨æ··åˆæœç´¢"""
    
    def __init__(self, vector_store: VectorStore, embeddings: Embeddings):
        super().__init__(
            vector_store=vector_store,
            embeddings=embeddings,
            semantic_weight=0.6,  # NESMAæ›´ä¾èµ–è§„åˆ™åŒ¹é…
            keyword_weight=0.4
        )
        
        # NESMAç‰¹å®šå…³é”®è¯
        self.nesma_keywords = {
            'function_types': ['ILF', 'EIF', 'EI', 'EO', 'EQ'],
            'complexity_terms': ['DET', 'RET', 'Low', 'Average', 'High'],
            'data_terms': ['data element', 'record element', 'logical file'],
            'process_terms': ['input', 'output', 'inquiry', 'maintain']
        }
    
    def _tokenize(self, text: str) -> List[str]:
        """NESMAç‰¹å®šåˆ†è¯"""
        
        # åŸºç¡€åˆ†è¯
        tokens = super()._tokenize(text)
        
        # æ·»åŠ NESMAæœ¯è¯­è¯†åˆ«
        text_upper = text.upper()
        for category, keywords in self.nesma_keywords.items():
            for keyword in keywords:
                if keyword in text_upper:
                    tokens.append(keyword.lower())
        
        return tokens
    
    def search_function_classification_rules(
        self,
        function_description: str,
        function_type: Optional[str] = None,
        k: int = 5
    ) -> List[Tuple[Document, float]]:
        """æœç´¢åŠŸèƒ½åˆ†ç±»è§„åˆ™"""
        
        # æ„å»ºæŸ¥è¯¢
        query_parts = [function_description]
        if function_type:
            query_parts.append(f"åŠŸèƒ½ç±»å‹ {function_type}")
        
        query = " ".join(query_parts)
        
        # è¿‡æ»¤æ¡ä»¶
        filter_metadata = {
            'source_type': 'NESMA',
            'category': 'classification'
        }
        
        if function_type:
            filter_metadata['function_type'] = function_type
        
        return self.hybrid_search(
            query=query,
            k=k,
            filter_metadata=filter_metadata
        )
    
    def search_complexity_calculation_rules(
        self,
        function_type: str,
        det_count: Optional[int] = None,
        ret_count: Optional[int] = None,
        k: int = 3
    ) -> List[Tuple[Document, float]]:
        """æœç´¢å¤æ‚åº¦è®¡ç®—è§„åˆ™"""
        
        query_parts = [f"{function_type} å¤æ‚åº¦è®¡ç®—"]
        
        if det_count is not None:
            query_parts.append(f"DET {det_count}")
        if ret_count is not None:
            query_parts.append(f"RET {ret_count}")
        
        query = " ".join(query_parts)
        
        filter_metadata = {
            'source_type': 'NESMA',
            'category': 'complexity',
            'function_type': function_type
        }
        
        return self.hybrid_search(
            query=query,
            k=k,
            filter_metadata=filter_metadata
        )


class COSMICHybridSearch(HybridSearchStrategy):
    """COSMICä¸“ç”¨æ··åˆæœç´¢"""
    
    def __init__(self, vector_store: VectorStore, embeddings: Embeddings):
        super().__init__(
            vector_store=vector_store,
            embeddings=embeddings,
            semantic_weight=0.7,  # COSMICæ›´ä¾èµ–è¯­ä¹‰ç†è§£
            keyword_weight=0.3
        )
        
        # COSMICç‰¹å®šå…³é”®è¯
        self.cosmic_keywords = {
            'data_movements': ['Entry', 'Exit', 'Read', 'Write'],
            'boundary_terms': ['è½¯ä»¶è¾¹ç•Œ', 'persistent storage', 'functional user'],
            'process_terms': ['åŠŸèƒ½è¿‡ç¨‹', 'functional process', 'data group'],
            'measurement_terms': ['CFP', 'COSMIC Function Point', 'data movement']
        }
    
    def _tokenize(self, text: str) -> List[str]:
        """COSMICç‰¹å®šåˆ†è¯"""
        
        # åŸºç¡€åˆ†è¯
        tokens = super()._tokenize(text)
        
        # æ·»åŠ COSMICæœ¯è¯­è¯†åˆ«
        for category, keywords in self.cosmic_keywords.items():
            for keyword in keywords:
                if keyword.lower() in text.lower():
                    tokens.append(keyword.lower().replace(' ', '_'))
        
        return tokens
    
    def search_data_movement_rules(
        self,
        process_description: str,
        movement_type: Optional[str] = None,
        k: int = 5
    ) -> List[Tuple[Document, float]]:
        """æœç´¢æ•°æ®ç§»åŠ¨è¯†åˆ«è§„åˆ™"""
        
        query_parts = [process_description, "æ•°æ®ç§»åŠ¨"]
        if movement_type:
            query_parts.append(movement_type)
        
        query = " ".join(query_parts)
        
        filter_metadata = {
            'source_type': 'COSMIC',
            'category': 'data_movement'
        }
        
        if movement_type:
            filter_metadata['movement_type'] = movement_type
        
        return self.hybrid_search(
            query=query,
            k=k,
            filter_metadata=filter_metadata
        )
    
    def search_functional_user_rules(
        self,
        user_description: str,
        k: int = 3
    ) -> List[Tuple[Document, float]]:
        """æœç´¢åŠŸèƒ½ç”¨æˆ·è¯†åˆ«è§„åˆ™"""
        
        query = f"{user_description} åŠŸèƒ½ç”¨æˆ· è¾¹ç•Œ"
        
        filter_metadata = {
            'source_type': 'COSMIC',
            'category': 'functional_user'
        }
        
        return self.hybrid_search(
            query=query,
            k=k,
            filter_metadata=filter_metadata
        )
    
    def search_boundary_analysis_rules(
        self,
        boundary_description: str,
        k: int = 5
    ) -> List[Tuple[Document, float]]:
        """æœç´¢è¾¹ç•Œåˆ†æè§„åˆ™"""
        
        query = f"{boundary_description} è½¯ä»¶è¾¹ç•Œ æŒä¹…å­˜å‚¨"
        
        filter_metadata = {
            'source_type': 'COSMIC',
            'category': 'boundary'
        }
        
        return self.hybrid_search(
            query=query,
            k=k,
            filter_metadata=filter_metadata
        )


class AdaptiveHybridSearch:
    """è‡ªé€‚åº”æ··åˆæœç´¢"""
    
    def __init__(
        self,
        nesma_search: NESMAHybridSearch,
        cosmic_search: COSMICHybridSearch
    ):
        self.nesma_search = nesma_search
        self.cosmic_search = cosmic_search
        
    def intelligent_search(
        self,
        query: str,
        context: Optional[Dict[str, Any]] = None,
        k: int = 10
    ) -> Dict[str, List[Tuple[Document, float]]]:
        """æ™ºèƒ½æœç´¢ - æ ¹æ®æŸ¥è¯¢å†…å®¹è‡ªåŠ¨é€‰æ‹©æœç´¢ç­–ç•¥"""
        
        # åˆ†ææŸ¥è¯¢æ„å›¾
        query_intent = self._analyze_query_intent(query, context)
        
        results = {}
        
        if query_intent.get('standard') in ['NESMA', 'BOTH']:
            # æ‰§è¡ŒNESMAæœç´¢
            nesma_results = self._execute_nesma_search(query, query_intent, k)
            if nesma_results:
                results['NESMA'] = nesma_results
        
        if query_intent.get('standard') in ['COSMIC', 'BOTH']:
            # æ‰§è¡ŒCOSMICæœç´¢
            cosmic_results = self._execute_cosmic_search(query, query_intent, k)
            if cosmic_results:
                results['COSMIC'] = cosmic_results
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ ‡å‡†ï¼Œä¸¤ä¸ªéƒ½æœç´¢
        if not results:
            results['NESMA'] = self.nesma_search.hybrid_search(query, k)
            results['COSMIC'] = self.cosmic_search.hybrid_search(query, k)
        
        return results
    
    def _analyze_query_intent(
        self,
        query: str,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """åˆ†ææŸ¥è¯¢æ„å›¾"""
        
        intent = {
            'standard': 'BOTH',  # é»˜è®¤æœç´¢ä¸¤ä¸ªæ ‡å‡†
            'category': 'general',
            'specificity': 'medium'
        }
        
        query_lower = query.lower()
        
        # è¯†åˆ«æ ‡å‡†ç‰¹å®šæœ¯è¯­
        nesma_terms = ['nesma', 'ilf', 'eif', 'det', 'ret', 'ufp']
        cosmic_terms = ['cosmic', 'cfp', 'entry', 'exit', 'read', 'write', 'data movement']
        
        nesma_count = sum(1 for term in nesma_terms if term in query_lower)
        cosmic_count = sum(1 for term in cosmic_terms if term in query_lower)
        
        if nesma_count > cosmic_count:
            intent['standard'] = 'NESMA'
        elif cosmic_count > nesma_count:
            intent['standard'] = 'COSMIC'
        
        # è¯†åˆ«æŸ¥è¯¢ç±»åˆ«
        if any(term in query_lower for term in ['åˆ†ç±»', 'classification', 'ç±»å‹']):
            intent['category'] = 'classification'
        elif any(term in query_lower for term in ['å¤æ‚åº¦', 'complexity']):
            intent['category'] = 'complexity'
        elif any(term in query_lower for term in ['è¾¹ç•Œ', 'boundary']):
            intent['category'] = 'boundary'
        elif any(term in query_lower for term in ['è®¡ç®—', 'calculation']):
            intent['category'] = 'calculation'
        
        # è€ƒè™‘ä¸Šä¸‹æ–‡ä¿¡æ¯
        if context:
            if context.get('current_standard'):
                intent['standard'] = context['current_standard']
            if context.get('function_type'):
                intent['function_type'] = context['function_type']
        
        return intent
    
    def _execute_nesma_search(
        self,
        query: str,
        intent: Dict[str, Any],
        k: int
    ) -> List[Tuple[Document, float]]:
        """æ‰§è¡ŒNESMAç‰¹å®šæœç´¢"""
        
        category = intent.get('category', 'general')
        
        if category == 'classification':
            return self.nesma_search.search_function_classification_rules(
                function_description=query,
                k=k
            )
        elif category == 'complexity':
            function_type = intent.get('function_type')
            if function_type:
                return self.nesma_search.search_complexity_calculation_rules(
                    function_type=function_type,
                    k=k
                )
        
        # é»˜è®¤æ··åˆæœç´¢
        return self.nesma_search.hybrid_search(query, k)
    
    def _execute_cosmic_search(
        self,
        query: str,
        intent: Dict[str, Any],
        k: int
    ) -> List[Tuple[Document, float]]:
        """æ‰§è¡ŒCOSMICç‰¹å®šæœç´¢"""
        
        category = intent.get('category', 'general')
        
        if category == 'data_movement' or 'data movement' in query.lower():
            return self.cosmic_search.search_data_movement_rules(
                process_description=query,
                k=k
            )
        elif category == 'boundary' or 'è¾¹ç•Œ' in query:
            return self.cosmic_search.search_boundary_analysis_rules(
                boundary_description=query,
                k=k
            )
        elif 'functional user' in query.lower() or 'åŠŸèƒ½ç”¨æˆ·' in query:
            return self.cosmic_search.search_functional_user_rules(
                user_description=query,
                k=k
            )
        
        # é»˜è®¤æ··åˆæœç´¢
        return self.cosmic_search.hybrid_search(query, k)


if __name__ == "__main__":
    # æµ‹è¯•æ··åˆæœç´¢ï¼ˆéœ€è¦å®é™…çš„å‘é‡å­˜å‚¨å’ŒåµŒå…¥æ¨¡å‹ï¼‰
    print("æ··åˆæœç´¢ç­–ç•¥æ¨¡å—å·²åŠ è½½")
    print(f"BM25å¯ç”¨: {BM25_AVAILABLE}")
    print("ä½¿ç”¨è¯´æ˜:")
    print("1. é¦–å…ˆåˆ›å»ºå‘é‡å­˜å‚¨å’ŒåµŒå…¥æ¨¡å‹")
    print("2. å®ä¾‹åŒ–HybridSearchStrategyæˆ–å…¶å­ç±»")
    print("3. è°ƒç”¨build_bm25_index()æ„å»ºå…³é”®è¯ç´¢å¼•")
    print("4. ä½¿ç”¨hybrid_search()è¿›è¡Œæ··åˆæœç´¢") 