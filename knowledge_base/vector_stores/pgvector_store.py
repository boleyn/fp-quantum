"""
é‡å­æ™ºèƒ½åŒ–åŠŸèƒ½ç‚¹ä¼°ç®—ç³»ç»Ÿ - PgVectorå‘é‡å­˜å‚¨

åŸºäºLangChain PostgreSQL PGVectoré›†æˆçš„å‘é‡å­˜å‚¨è§£å†³æ–¹æ¡ˆ
ä¸“é—¨ç”¨äºçŸ¥è¯†åº“å‘é‡åŒ–æ•°æ®çš„å­˜å‚¨å’Œæ£€ç´¢
"""

import asyncio
import logging
import os
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

from langchain_core.documents import Document
from langchain_postgres import PGVector
from langchain_core.embeddings import Embeddings
from langchain_postgres.vectorstores import DistanceStrategy

from config.settings import get_settings

logger = logging.getLogger(__name__)


class PgVectorStore:
    """PgVectorå‘é‡å­˜å‚¨ç®¡ç†å™¨ - åŸºäºLangChain PostgreSQLé›†æˆ"""
    
    def __init__(self):
        self.settings = get_settings()
        self.vector_stores: Dict[str, PGVector] = {}
        
        # æ”¯æŒçš„æºç±»å‹
        self.source_types = ["nesma", "cosmic", "common", "mixed"]
        
        # æ„å»ºPostgreSQLè¿æ¥å­—ç¬¦ä¸²ï¼ˆä½¿ç”¨psycopg3ï¼‰
        self.connection_string = self._build_connection_string()
    
    def _build_connection_string(self) -> str:
        """æ„å»ºPostgreSQLè¿æ¥å­—ç¬¦ä¸²"""
        db_settings = self.settings.database
        logger.info(db_settings.postgres_host)
        # ä½¿ç”¨psycopg3è¿æ¥å­—ç¬¦ä¸²æ ¼å¼
        connection_string = f"postgresql+psycopg://{db_settings.postgres_user}:{db_settings.postgres_password}@{db_settings.postgres_host}:{db_settings.postgres_port}/{db_settings.postgres_database}"
        
        logger.info(f"ğŸ”— PgVectorè¿æ¥å­—ç¬¦ä¸²: {connection_string.replace(db_settings.postgres_password, '***')}")
        return connection_string
    
    async def initialize(self, embeddings: Embeddings):
        """åˆå§‹åŒ–PgVectorè¿æ¥å’Œå‘é‡å­˜å‚¨"""
        try:
            # ä¸ºæ¯ä¸ªæºç±»å‹åˆ›å»ºLangChain PGVectorå®ä¾‹
            await self._initialize_langchain_stores(embeddings)
            
            logger.info("âœ… PgVectorå‘é‡å­˜å‚¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ PgVectoråˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def _initialize_langchain_stores(self, embeddings: Embeddings):
        """åˆå§‹åŒ–LangChain PGVectorå­˜å‚¨"""
        for source_type in self.source_types:
            try:
                # ä¸ºæ¯ä¸ªæºç±»å‹åˆ›å»ºç‹¬ç«‹çš„collection
                collection_name = f"fp_quantum_{source_type}"
                
                # åˆ›å»ºPGVectorå®ä¾‹
                vector_store = PGVector(
                    embeddings=embeddings,
                    collection_name=collection_name,
                    connection=self.connection_string,
                    distance_strategy=DistanceStrategy.COSINE,
                    use_jsonb=True,  # ä½¿ç”¨JSONBå­˜å‚¨å…ƒæ•°æ®
                    create_extension=True,  # è‡ªåŠ¨åˆ›å»ºpgvectoræ‰©å±•
                    pre_delete_collection=False  # ä¸åˆ é™¤ç°æœ‰collection
                )
                
                self.vector_stores[source_type] = vector_store
                logger.info(f"âœ… {source_type} LangChain PGVectorå­˜å‚¨å·²åˆ›å»º")
                
            except Exception as e:
                logger.error(f"âŒ åˆ›å»º {source_type} LangChainå­˜å‚¨å¤±è´¥: {e}")
                raise
    
    async def add_documents(
        self,
        documents: List[Document],
        source_type: str = "common"
    ) -> List[str]:
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨"""
        if source_type not in self.source_types:
            raise ValueError(f"ä¸æ”¯æŒçš„æºç±»å‹: {source_type}")
        
        if not documents:
            return []
        
        try:
            # è·å–å¯¹åº”çš„å‘é‡å­˜å‚¨
            vector_store = self.vector_stores[source_type]
            
            # ä¸ºæ–‡æ¡£æ·»åŠ å…ƒæ•°æ®
            for doc in documents:
                if not doc.metadata:
                    doc.metadata = {}
                doc.metadata.update({
                    "source_type": source_type.upper(),
                    "standard": source_type.upper(),
                    "created_at": datetime.utcnow().isoformat()
                })
            
            # æ‰¹é‡æ·»åŠ æ–‡æ¡£
            ids = await vector_store.aadd_documents(documents)
            
            logger.info(f"âœ… æˆåŠŸæ·»åŠ  {len(ids)} ä¸ªæ–‡æ¡£åˆ° {source_type} å‘é‡å­˜å‚¨")
            return ids
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ æ–‡æ¡£åˆ° {source_type} å¤±è´¥: {e}")
            raise
    
    async def similarity_search(
        self,
        query: str,
        source_type: Optional[str] = None,
        k: int = 5,
        filter_metadata: Optional[Dict[str, Any]] = None
    ) -> List[Tuple[Document, float]]:
        """ç›¸ä¼¼åº¦æœç´¢"""
        try:
            results = []
            
            # ç¡®å®šæœç´¢èŒƒå›´
            if source_type and source_type in self.vector_stores:
                search_stores = [source_type]
            else:
                search_stores = list(self.vector_stores.keys())
            
            # åœ¨æŒ‡å®šçš„å­˜å‚¨ä¸­æœç´¢
            for store_type in search_stores:
                vector_store = self.vector_stores[store_type]
                
                try:
                    # ä½¿ç”¨å¼‚æ­¥ç›¸ä¼¼åº¦æœç´¢å¹¶è·å–åˆ†æ•°
                    store_results = await vector_store.asimilarity_search_with_score(
                        query=query,
                        k=k,
                        filter=filter_metadata
                    )
                    
                    # æ·»åŠ æºç±»å‹åˆ°ç»“æœ
                    for doc, score in store_results:
                        if not doc.metadata:
                            doc.metadata = {}
                        doc.metadata["search_source"] = store_type
                        results.append((doc, score))
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ åœ¨ {store_type} ä¸­æœç´¢å¤±è´¥: {e}")
                    continue
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›top-kç»“æœ
            results.sort(key=lambda x: x[1], reverse=True)
            final_results = results[:k]
            
            logger.debug(f"ğŸ” ç›¸ä¼¼åº¦æœç´¢è¿”å› {len(final_results)} ä¸ªç»“æœ")
            return final_results
            
        except Exception as e:
            logger.error(f"âŒ ç›¸ä¼¼åº¦æœç´¢å¤±è´¥: {e}")
            return []
    
    async def similarity_search_simple(
        self,
        query: str,
        source_type: Optional[str] = None,
        k: int = 5,
        filter_metadata: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """ç®€å•ç›¸ä¼¼åº¦æœç´¢ï¼ˆä¸è¿”å›åˆ†æ•°ï¼‰"""
        try:
            results = []
            
            # ç¡®å®šæœç´¢èŒƒå›´
            if source_type and source_type in self.vector_stores:
                search_stores = [source_type]
            else:
                search_stores = list(self.vector_stores.keys())
            
            # åœ¨æŒ‡å®šçš„å­˜å‚¨ä¸­æœç´¢
            for store_type in search_stores:
                vector_store = self.vector_stores[store_type]
                
                try:
                    # ä½¿ç”¨å¼‚æ­¥ç›¸ä¼¼åº¦æœç´¢
                    store_results = await vector_store.asimilarity_search(
                        query=query,
                        k=k,
                        filter=filter_metadata
                    )
                    
                    # æ·»åŠ æºç±»å‹åˆ°ç»“æœ
                    for doc in store_results:
                        if not doc.metadata:
                            doc.metadata = {}
                        doc.metadata["search_source"] = store_type
                        results.append(doc)
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ åœ¨ {store_type} ä¸­æœç´¢å¤±è´¥: {e}")
                    continue
            
            # è¿”å›å‰kä¸ªç»“æœ
            final_results = results[:k]
            
            logger.debug(f"ğŸ” ç®€å•ç›¸ä¼¼åº¦æœç´¢è¿”å› {len(final_results)} ä¸ªç»“æœ")
            return final_results
            
        except Exception as e:
            logger.error(f"âŒ ç®€å•ç›¸ä¼¼åº¦æœç´¢å¤±è´¥: {e}")
            return []
    
    async def get_collection_stats(self, source_type: str) -> Dict[str, Any]:
        """è·å–collectionç»Ÿè®¡ä¿¡æ¯"""
        if source_type not in self.vector_stores:
            raise ValueError(f"ä¸æ”¯æŒçš„æºç±»å‹: {source_type}")
        
        try:
            vector_store = self.vector_stores[source_type]
            
            # è·å–åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            stats = {
                "source_type": source_type,
                "collection_name": f"fp_quantum_{source_type}",
                "status": "active"
            }
            
            logger.debug(f"ğŸ“Š {source_type} collectionç»Ÿè®¡: {stats}")
            return stats
            
        except Exception as e:
            logger.error(f"âŒ è·å– {source_type} ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    async def delete_documents(
        self,
        source_type: str,
        ids: Optional[List[str]] = None,
        filter_metadata: Optional[Dict[str, Any]] = None
    ) -> int:
        """åˆ é™¤æ–‡æ¡£"""
        if source_type not in self.vector_stores:
            raise ValueError(f"ä¸æ”¯æŒçš„æºç±»å‹: {source_type}")
        
        try:
            vector_store = self.vector_stores[source_type]
            
            if ids:
                # æŒ‰IDåˆ é™¤
                await vector_store.adelete(ids=ids)
                deleted_count = len(ids)
            else:
                # æŒ‰ç­›é€‰æ¡ä»¶åˆ é™¤ï¼ˆéœ€è¦ç‰¹æ®Šå®ç°ï¼‰
                logger.warning("âš ï¸ æŒ‰ç­›é€‰æ¡ä»¶åˆ é™¤æš‚æœªå®ç°")
                deleted_count = 0
            
            logger.info(f"âœ… ä» {source_type} åˆ é™¤äº† {deleted_count} ä¸ªæ–‡æ¡£")
            return deleted_count
            
        except Exception as e:
            logger.error(f"âŒ ä» {source_type} åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
            return 0
    
    async def close(self):
        """å…³é—­è¿æ¥"""
        try:
            # PGVectorä½¿ç”¨çš„æ˜¯LangChainçš„è¿æ¥ç®¡ç†ï¼Œé€šå¸¸è‡ªåŠ¨å…³é—­
            logger.info("âœ… PgVectorè¿æ¥å·²å…³é—­")
        except Exception as e:
            logger.error(f"âŒ å…³é—­PgVectorè¿æ¥å¤±è´¥: {e}")
    
    def as_retriever(self, source_type: Optional[str] = None, **kwargs):
        """åˆ›å»ºæ£€ç´¢å™¨æ¥å£ - ç›´æ¥ä½¿ç”¨LangChainåŸç”Ÿçš„as_retrieveræ–¹æ³•"""
        
        # å¦‚æœæŒ‡å®šäº†æºç±»å‹ï¼Œä½¿ç”¨å¯¹åº”çš„LangChain PGVectorå®ä¾‹
        if source_type and source_type in self.vector_stores:
            vector_store = self.vector_stores[source_type]
            return vector_store.as_retriever(**kwargs)
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæºç±»å‹ï¼Œä½¿ç”¨é»˜è®¤çš„ç¬¬ä¸€ä¸ªå­˜å‚¨
        if self.vector_stores:
            default_store = list(self.vector_stores.values())[0]
            return default_store.as_retriever(**kwargs)
        
        raise ValueError("æ²¡æœ‰å¯ç”¨çš„å‘é‡å­˜å‚¨")
    
    async def similarity_search_with_score(
        self,
        query: str,
        source_type: Optional[str] = None,
        k: int = 5,
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Tuple[Document, float]]:
        """å¸¦åˆ†æ•°çš„ç›¸ä¼¼åº¦æœç´¢ï¼ˆåˆ«åæ–¹æ³•ï¼‰"""
        return await self.similarity_search(
            query=query,
            source_type=source_type,
            k=k,
            filter_metadata=filter
        )

    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„å‘é‡å­˜å‚¨
            if not self.vector_stores:
                logger.warning("âš ï¸ æœªåˆå§‹åŒ–å‘é‡å­˜å‚¨")
                return False
            
            # æµ‹è¯•ä¸€ä¸ªç®€å•çš„æœç´¢æ¥æ£€æŸ¥è¿æ¥
            test_store = list(self.vector_stores.values())[0]
            await test_store.asimilarity_search("test", k=1)
            
            logger.info("âœ… PgVectorå¥åº·æ£€æŸ¥é€šè¿‡")
            return True
            
        except Exception as e:
            logger.error(f"âŒ PgVectorå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False


async def create_pgvector_store(embeddings: Embeddings) -> PgVectorStore:
    """åˆ›å»ºå¹¶åˆå§‹åŒ–PgVectorå­˜å‚¨"""
    store = PgVectorStore()
    await store.initialize(embeddings)
    return store


def get_langchain_pgvector(
    source_type: str,
    embeddings: Embeddings,
    pgvector_store: PgVectorStore
) -> PGVector:
    """è·å–æŒ‡å®šæºç±»å‹çš„LangChain PGVectorå®ä¾‹"""
    if source_type not in pgvector_store.vector_stores:
        raise ValueError(f"ä¸æ”¯æŒçš„æºç±»å‹: {source_type}")
    
    return pgvector_store.vector_stores[source_type]


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    async def main():
        # æµ‹è¯•PgVectorå­˜å‚¨
        from langchain_openai import OpenAIEmbeddings
        
        # åˆ›å»ºæµ‹è¯•embeddings
        embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            openai_api_key="test-key"
        )
        
        try:
            # åˆ›å»ºå­˜å‚¨
            store = await create_pgvector_store(embeddings)
            
            # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
            test_docs = [
                Document(
                    page_content="è¿™æ˜¯ä¸€ä¸ªNESMAåŠŸèƒ½ç‚¹ä¼°ç®—çš„æµ‹è¯•æ–‡æ¡£",
                    metadata={"category": "test", "type": "nesma"}
                ),
                Document(
                    page_content="COSMICæ•°æ®ç§»åŠ¨åˆ†æè§„åˆ™",
                    metadata={"category": "test", "type": "cosmic"}
                )
            ]
            
            # æ·»åŠ æ–‡æ¡£
            ids = await store.add_documents(test_docs, "nesma")
            print(f"æ·»åŠ äº† {len(ids)} ä¸ªæ–‡æ¡£")
            
            # æœç´¢æµ‹è¯•
            results = await store.similarity_search("NESMAåŠŸèƒ½ç‚¹", "nesma", k=2)
            print(f"æœç´¢åˆ° {len(results)} ä¸ªç»“æœ")
            
            # å…³é—­å­˜å‚¨
            await store.close()
            
        except Exception as e:
            print(f"æµ‹è¯•å¤±è´¥: {e}")
    
    asyncio.run(main()) 