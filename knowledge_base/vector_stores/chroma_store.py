"""
é‡å­æ™ºèƒ½åŒ–åŠŸèƒ½ç‚¹ä¼°ç®—ç³»ç»Ÿ - Chromaå‘é‡å­˜å‚¨

å¼€å‘ç¯å¢ƒä½¿ç”¨çš„è½»é‡çº§å‘é‡å­˜å‚¨è§£å†³æ–¹æ¡ˆ
"""

import os
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
from datetime import datetime

from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain_core.embeddings import Embeddings
from langchain_core.vectorstores import VectorStore

logger = logging.getLogger(__name__)


class ChromaVectorStore:
    """Chromaå‘é‡å­˜å‚¨ç®¡ç†å™¨"""
    
    def __init__(
        self,
        persist_directory: str = "./chroma_db",
        collection_prefix: str = "fp_quantum"
    ):
        self.persist_directory = Path(persist_directory)
        self.collection_prefix = collection_prefix
        self.collections: Dict[str, Chroma] = {}
        
        # ç¡®ä¿æŒä¹…åŒ–ç›®å½•å­˜åœ¨
        self.persist_directory.mkdir(parents=True, exist_ok=True)
        
    def get_collection(
        self,
        collection_name: str,
        embeddings: Embeddings,
        create_if_not_exists: bool = True
    ) -> Optional[Chroma]:
        """è·å–æˆ–åˆ›å»ºé›†åˆ"""
        
        full_collection_name = f"{self.collection_prefix}_{collection_name}"
        
        if full_collection_name in self.collections:
            return self.collections[full_collection_name]
        
        try:
            collection_path = self.persist_directory / full_collection_name
            
            if create_if_not_exists or collection_path.exists():
                vector_store = Chroma(
                    collection_name=full_collection_name,
                    embedding_function=embeddings,
                    persist_directory=str(collection_path)
                )
                
                self.collections[full_collection_name] = vector_store
                logger.info(f"âœ… Chromaé›†åˆå·²å‡†å¤‡: {full_collection_name}")
                return vector_store
            else:
                logger.warning(f"Chromaé›†åˆä¸å­˜åœ¨: {full_collection_name}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ è·å–Chromaé›†åˆå¤±è´¥ {full_collection_name}: {e}")
            return None
    
    def create_collection_from_documents(
        self,
        collection_name: str,
        documents: List[Document],
        embeddings: Embeddings,
        batch_size: int = 100
    ) -> Optional[Chroma]:
        """ä»æ–‡æ¡£åˆ›å»ºé›†åˆ"""
        
        if not documents:
            logger.warning(f"æ–‡æ¡£åˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•åˆ›å»ºé›†åˆ: {collection_name}")
            return None
        
        try:
            full_collection_name = f"{self.collection_prefix}_{collection_name}"
            collection_path = self.persist_directory / full_collection_name
            
            # åˆ é™¤ç°æœ‰é›†åˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if collection_path.exists():
                import shutil
                shutil.rmtree(collection_path)
                logger.info(f"åˆ é™¤ç°æœ‰é›†åˆ: {full_collection_name}")
            
            # æ‰¹é‡å¤„ç†æ–‡æ¡£
            vector_store = None
            total_docs = len(documents)
            
            logger.info(f"ğŸ“š å¼€å§‹åˆ›å»ºChromaé›†åˆ: {full_collection_name} ({total_docs} æ–‡æ¡£)")
            
            for i in range(0, total_docs, batch_size):
                batch = documents[i:i + batch_size]
                batch_num = i // batch_size + 1
                total_batches = (total_docs + batch_size - 1) // batch_size
                
                logger.info(f"å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} æ–‡æ¡£)")
                
                if vector_store is None:
                    # åˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨
                    vector_store = Chroma.from_documents(
                        documents=batch,
                        embedding=embeddings,
                        collection_name=full_collection_name,
                        persist_directory=str(collection_path)
                    )
                else:
                    # æ·»åŠ æ–‡æ¡£åˆ°ç°æœ‰å­˜å‚¨
                    vector_store.add_documents(batch)
            
            # æŒä¹…åŒ–
            if hasattr(vector_store, 'persist'):
                vector_store.persist()
            
            self.collections[full_collection_name] = vector_store
            
            logger.info(f"âœ… Chromaé›†åˆåˆ›å»ºå®Œæˆ: {full_collection_name}")
            return vector_store
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºChromaé›†åˆå¤±è´¥ {collection_name}: {e}")
            return None
    
    def search_similar_documents(
        self,
        collection_name: str,
        query: str,
        embeddings: Embeddings,
        k: int = 5,
        filter_metadata: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """æœç´¢ç›¸ä¼¼æ–‡æ¡£"""
        
        vector_store = self.get_collection(collection_name, embeddings, create_if_not_exists=False)
        if not vector_store:
            logger.warning(f"é›†åˆä¸å­˜åœ¨: {collection_name}")
            return []
        
        try:
            if filter_metadata:
                # ä½¿ç”¨è¿‡æ»¤å™¨æœç´¢
                results = vector_store.similarity_search(
                    query=query,
                    k=k,
                    filter=filter_metadata
                )
            else:
                # åŸºæœ¬ç›¸ä¼¼åº¦æœç´¢
                results = vector_store.similarity_search(query=query, k=k)
            
            logger.debug(f"ä» {collection_name} æ£€ç´¢åˆ° {len(results)} ä¸ªç›¸ä¼¼æ–‡æ¡£")
            return results
            
        except Exception as e:
            logger.error(f"âŒ æœç´¢ç›¸ä¼¼æ–‡æ¡£å¤±è´¥ {collection_name}: {e}")
            return []
    
    def search_with_scores(
        self,
        collection_name: str,
        query: str,
        embeddings: Embeddings,
        k: int = 5,
        score_threshold: float = 0.5
    ) -> List[tuple]:
        """æœç´¢ç›¸ä¼¼æ–‡æ¡£å¹¶è¿”å›ç›¸ä¼¼åº¦åˆ†æ•°"""
        
        vector_store = self.get_collection(collection_name, embeddings, create_if_not_exists=False)
        if not vector_store:
            logger.warning(f"é›†åˆä¸å­˜åœ¨: {collection_name}")
            return []
        
        try:
            # æœç´¢å¸¦åˆ†æ•°çš„ç»“æœ
            results_with_scores = vector_store.similarity_search_with_score(
                query=query,
                k=k
            )
            
            # è¿‡æ»¤ä½åˆ†ç»“æœ
            filtered_results = [
                (doc, score) for doc, score in results_with_scores
                if score >= score_threshold
            ]
            
            logger.debug(
                f"ä» {collection_name} æ£€ç´¢åˆ° {len(filtered_results)} ä¸ªæ–‡æ¡£ "
                f"(é˜ˆå€¼: {score_threshold})"
            )
            return filtered_results
            
        except Exception as e:
            logger.error(f"âŒ æœç´¢å¸¦åˆ†æ•°æ–‡æ¡£å¤±è´¥ {collection_name}: {e}")
            return []
    
    def get_collection_stats(self, collection_name: str, embeddings: Embeddings) -> Dict[str, Any]:
        """è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯"""
        
        vector_store = self.get_collection(collection_name, embeddings, create_if_not_exists=False)
        if not vector_store:
            return {"exists": False}
        
        try:
            # è·å–é›†åˆä¿¡æ¯
            collection = vector_store._collection
            stats = {
                "exists": True,
                "name": collection.name,
                "count": collection.count(),
                "metadata": collection.metadata or {}
            }
            
            # å°è¯•è·å–é¢å¤–ç»Ÿè®¡ä¿¡æ¯
            try:
                # è·å–æ‰€æœ‰æ–‡æ¡£ID
                all_docs = collection.get()
                if all_docs:
                    stats["document_ids"] = len(all_docs["ids"]) if all_docs.get("ids") else 0
                    
                    # åˆ†æå…ƒæ•°æ®
                    if all_docs.get("metadatas"):
                        metadata_analysis = self._analyze_metadata(all_docs["metadatas"])
                        stats["metadata_analysis"] = metadata_analysis
            except Exception as e:
                logger.debug(f"æ— æ³•è·å–è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯: {e}")
            
            return stats
            
        except Exception as e:
            logger.error(f"âŒ è·å–é›†åˆç»Ÿè®¡å¤±è´¥ {collection_name}: {e}")
            return {"exists": False, "error": str(e)}
    
    def _analyze_metadata(self, metadatas: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æå…ƒæ•°æ®"""
        
        analysis = {
            "total_documents": len(metadatas),
            "metadata_keys": set(),
            "source_types": {},
            "loader_types": {}
        }
        
        for metadata in metadatas:
            if metadata:
                # æ”¶é›†å…ƒæ•°æ®é”®
                analysis["metadata_keys"].update(metadata.keys())
                
                # ç»Ÿè®¡æ¥æºç±»å‹
                source_type = metadata.get("source_type", "unknown")
                analysis["source_types"][source_type] = \
                    analysis["source_types"].get(source_type, 0) + 1
                
                # ç»Ÿè®¡åŠ è½½å™¨ç±»å‹
                loader_type = metadata.get("loader_type", "unknown")
                analysis["loader_types"][loader_type] = \
                    analysis["loader_types"].get(loader_type, 0) + 1
        
        # è½¬æ¢é›†åˆä¸ºåˆ—è¡¨
        analysis["metadata_keys"] = list(analysis["metadata_keys"])
        
        return analysis
    
    def delete_collection(self, collection_name: str) -> bool:
        """åˆ é™¤é›†åˆ"""
        
        try:
            full_collection_name = f"{self.collection_prefix}_{collection_name}"
            collection_path = self.persist_directory / full_collection_name
            
            # ä»å†…å­˜ä¸­ç§»é™¤
            if full_collection_name in self.collections:
                del self.collections[full_collection_name]
            
            # åˆ é™¤ç£ç›˜æ–‡ä»¶
            if collection_path.exists():
                import shutil
                shutil.rmtree(collection_path)
                logger.info(f"âœ… å·²åˆ é™¤Chromaé›†åˆ: {full_collection_name}")
                return True
            else:
                logger.warning(f"é›†åˆä¸å­˜åœ¨: {full_collection_name}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤é›†åˆå¤±è´¥ {collection_name}: {e}")
            return False
    
    def list_collections(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰é›†åˆ"""
        
        try:
            collections = []
            
            # ä»æŒä¹…åŒ–ç›®å½•æ‰«æ
            if self.persist_directory.exists():
                for item in self.persist_directory.iterdir():
                    if item.is_dir() and item.name.startswith(self.collection_prefix):
                        # ç§»é™¤å‰ç¼€
                        collection_name = item.name[len(self.collection_prefix) + 1:]
                        collections.append(collection_name)
            
            return sorted(collections)
            
        except Exception as e:
            logger.error(f"âŒ åˆ—å‡ºé›†åˆå¤±è´¥: {e}")
            return []
    
    def backup_collection(self, collection_name: str, backup_path: str) -> bool:
        """å¤‡ä»½é›†åˆ"""
        
        try:
            full_collection_name = f"{self.collection_prefix}_{collection_name}"
            source_path = self.persist_directory / full_collection_name
            backup_path = Path(backup_path)
            
            if not source_path.exists():
                logger.warning(f"é›†åˆä¸å­˜åœ¨ï¼Œæ— æ³•å¤‡ä»½: {collection_name}")
                return False
            
            # åˆ›å»ºå¤‡ä»½ç›®å½•
            backup_path.parent.mkdir(parents=True, exist_ok=True)
            
            # å¤åˆ¶é›†åˆç›®å½•
            import shutil
            shutil.copytree(source_path, backup_path, dirs_exist_ok=True)
            
            logger.info(f"âœ… é›†åˆå¤‡ä»½å®Œæˆ: {collection_name} -> {backup_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å¤‡ä»½é›†åˆå¤±è´¥ {collection_name}: {e}")
            return False
    
    def restore_collection(self, collection_name: str, backup_path: str) -> bool:
        """æ¢å¤é›†åˆ"""
        
        try:
            full_collection_name = f"{self.collection_prefix}_{collection_name}"
            target_path = self.persist_directory / full_collection_name
            backup_path = Path(backup_path)
            
            if not backup_path.exists():
                logger.warning(f"å¤‡ä»½ä¸å­˜åœ¨ï¼Œæ— æ³•æ¢å¤: {backup_path}")
                return False
            
            # åˆ é™¤ç°æœ‰é›†åˆ
            if target_path.exists():
                import shutil
                shutil.rmtree(target_path)
            
            # ä»å¤‡ä»½æ¢å¤
            import shutil
            shutil.copytree(backup_path, target_path)
            
            # ä»å†…å­˜ä¸­ç§»é™¤ï¼ˆå¼ºåˆ¶é‡æ–°åŠ è½½ï¼‰
            if full_collection_name in self.collections:
                del self.collections[full_collection_name]
            
            logger.info(f"âœ… é›†åˆæ¢å¤å®Œæˆ: {backup_path} -> {collection_name}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¢å¤é›†åˆå¤±è´¥ {collection_name}: {e}")
            return False


def create_chroma_knowledge_base(
    documents_by_type: Dict[str, List[Document]],
    embeddings: Embeddings,
    persist_directory: str = "./chroma_db",
    collection_prefix: str = "fp_quantum"
) -> ChromaVectorStore:
    """åˆ›å»ºChromaçŸ¥è¯†åº“"""
    
    logger.info("ğŸš€ å¼€å§‹åˆ›å»ºChromaçŸ¥è¯†åº“...")
    
    # åˆ›å»ºChromaå­˜å‚¨ç®¡ç†å™¨
    chroma_store = ChromaVectorStore(
        persist_directory=persist_directory,
        collection_prefix=collection_prefix
    )
    
    # ä¸ºæ¯ç§ç±»å‹çš„æ–‡æ¡£åˆ›å»ºé›†åˆ
    total_docs = 0
    created_collections = []
    
    for doc_type, documents in documents_by_type.items():
        if documents:
            collection = chroma_store.create_collection_from_documents(
                collection_name=doc_type,
                documents=documents,
                embeddings=embeddings
            )
            
            if collection:
                created_collections.append(doc_type)
                total_docs += len(documents)
                logger.info(f"âœ… é›†åˆ '{doc_type}' åˆ›å»ºå®Œæˆ: {len(documents)} æ–‡æ¡£")
            else:
                logger.error(f"âŒ é›†åˆ '{doc_type}' åˆ›å»ºå¤±è´¥")
    
    logger.info(f"ğŸ‰ ChromaçŸ¥è¯†åº“åˆ›å»ºå®Œæˆ!")
    logger.info(f"   æ€»æ–‡æ¡£æ•°: {total_docs}")
    logger.info(f"   é›†åˆæ•°: {len(created_collections)}")
    logger.info(f"   é›†åˆåˆ—è¡¨: {created_collections}")
    
    return chroma_store


if __name__ == "__main__":
    # æµ‹è¯•Chromaå‘é‡å­˜å‚¨
    
    from langchain_openai import OpenAIEmbeddings
    
    # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£
    test_documents = [
        Document(
            page_content="NESMAåŠŸèƒ½ç‚¹åˆ†ææ˜¯ä¸€ç§æ ‡å‡†åŒ–çš„è½¯ä»¶è§„æ¨¡ä¼°ç®—æ–¹æ³•",
            metadata={"source_type": "NESMA", "category": "introduction"}
        ),
        Document(
            page_content="COSMICåŠŸèƒ½ç‚¹åˆ†æåŸºäºæ•°æ®ç§»åŠ¨çš„æ¦‚å¿µè¿›è¡Œä¼°ç®—",
            metadata={"source_type": "COSMIC", "category": "introduction"}
        ),
        Document(
            page_content="å†…éƒ¨é€»è¾‘æ–‡ä»¶(ILF)æ˜¯åº”ç”¨ç¨‹åºå†…éƒ¨ç»´æŠ¤çš„æ•°æ®",
            metadata={"source_type": "NESMA", "category": "data_function"}
        )
    ]
    
    # åˆ›å»ºåµŒå…¥æ¨¡å‹ï¼ˆéœ€è¦é…ç½®APIå¯†é’¥ï¼‰
    try:
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        
        # åˆ›å»ºChromaå­˜å‚¨
        chroma_store = ChromaVectorStore(persist_directory="./test_chroma")
        
        # æµ‹è¯•åˆ›å»ºé›†åˆ
        collection = chroma_store.create_collection_from_documents(
            collection_name="test",
            documents=test_documents,
            embeddings=embeddings
        )
        
        if collection:
            print("âœ… Chromaé›†åˆåˆ›å»ºæˆåŠŸ")
            
            # æµ‹è¯•æœç´¢
            results = chroma_store.search_similar_documents(
                collection_name="test",
                query="ä»€ä¹ˆæ˜¯NESMA",
                embeddings=embeddings,
                k=2
            )
            
            print(f"æœç´¢ç»“æœ: {len(results)} ä¸ªæ–‡æ¡£")
            for i, doc in enumerate(results):
                print(f"  {i+1}. {doc.page_content[:50]}...")
        
        # æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯
        stats = chroma_store.get_collection_stats("test", embeddings)
        print(f"é›†åˆç»Ÿè®¡: {stats}")
        
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥ (å¯èƒ½éœ€è¦é…ç½®OpenAI APIå¯†é’¥): {e}") 