"""
é‡å­æ™ºèƒ½åŒ–åŠŸèƒ½ç‚¹ä¼°ç®—ç³»ç»Ÿ - RAGé“¾

æ„å»ºå®Œæ•´çš„RAGç®¡é“ï¼Œæ•´åˆæ–‡æ¡£åŠ è½½ã€å‘é‡å­˜å‚¨ã€æ£€ç´¢å’Œç”Ÿæˆ
ç»Ÿä¸€ä½¿ç”¨PgVectorå‘é‡å­˜å‚¨
"""

import logging
from typing import List, Dict, Any, Optional, Union, Tuple
from pathlib import Path
import asyncio

from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_core.language_models import BaseLanguageModel
from langchain_core.vectorstores import VectorStore
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain.chains import RetrievalQA
from langchain.retrievers import EnsembleRetriever
from langchain_text_splitters import RecursiveCharacterTextSplitter

# å¯¼å…¥æœ¬åœ°æ¨¡å—
from .loaders.pdf_loader import EnhancedPDFLoader, BatchPDFProcessor
from .loaders.web_loader import load_web_knowledge_base
from .loaders.custom_loaders import FunctionPointDocumentLoader
# ç»Ÿä¸€ä½¿ç”¨PgVectorå‘é‡å­˜å‚¨
from .vector_stores.pgvector_store import PgVectorStore, create_pgvector_store
from .vector_stores.hybrid_search import HybridSearchStrategy, NESMAHybridSearch, COSMICHybridSearch
from .retrievers.semantic_retriever import EnhancedSemanticRetriever
from .retrievers.keyword_retriever import TFIDFRetriever, NESMAKeywordRetriever, COSMICKeywordRetriever
from .retrievers.multi_query_retriever import NESMAMultiQueryRetriever, COSMICMultiQueryRetriever
from .embeddings.embedding_models import get_embedding_model

logger = logging.getLogger(__name__)


class RAGChainBuilder:
    """RAGé“¾æ„å»ºå™¨ - åŸºäºPgVector"""
    
    def __init__(
        self,
        embeddings: Embeddings,
        llm: BaseLanguageModel,
        use_hybrid_search: bool = True
    ):
        self.embeddings = embeddings
        self.llm = llm
        self.use_hybrid_search = use_hybrid_search
        
        # å­˜å‚¨ç»„ä»¶
        self.documents: Dict[str, List[Document]] = {}
        self.vector_store: Optional[PgVectorStore] = None
        self.retrievers: Dict[str, Any] = {}
        self.chains: Dict[str, Any] = {}
        
    async def build_knowledge_base(
        self,
        document_paths: Dict[str, Union[str, Path, List[str]]],
        include_web_sources: bool = True
    ) -> Dict[str, int]:
        """æ„å»ºçŸ¥è¯†åº“"""
        
        logger.info("ğŸš€ å¼€å§‹æ„å»ºåŸºäºPgVectorçš„çŸ¥è¯†åº“...")
        
        # 1. åŠ è½½æ–‡æ¡£
        all_documents = await self._load_all_documents(document_paths, include_web_sources)
        
        # 2. åˆ›å»ºPgVectorå­˜å‚¨
        vector_store = await self._create_pgvector_store(all_documents)
        
        # 3. åˆ›å»ºæ£€ç´¢å™¨
        retrievers = await self._create_retrievers(vector_store, all_documents)
        
        # 4. æ„å»ºRAGé“¾
        chains = await self._build_rag_chains(retrievers)
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {}
        for doc_type, docs in all_documents.items():
            stats[doc_type] = len(docs)
        
        logger.info(f"âœ… PgVectorçŸ¥è¯†åº“æ„å»ºå®Œæˆ: {stats}")
        return stats
    
    async def _load_all_documents(
        self,
        document_paths: Dict[str, Union[str, Path, List[str]]],
        include_web_sources: bool
    ) -> Dict[str, List[Document]]:
        """åŠ è½½æ‰€æœ‰æ–‡æ¡£"""
        
        all_documents = {}
        
        # åŠ è½½æœ¬åœ°æ–‡æ¡£
        for doc_type, paths in document_paths.items():
            logger.info(f"ğŸ“š åŠ è½½ {doc_type} æ–‡æ¡£...")
            
            if isinstance(paths, (str, Path)):
                paths = [paths]
            
            type_documents = []
            
            for path in paths:
                path = Path(path)
                
                if path.is_dir():
                    # ç›®å½•ï¼šä½¿ç”¨è‡ªå®šä¹‰åŠ è½½å™¨
                    loader = FunctionPointDocumentLoader()
                    docs = loader.load_directory(path)
                    type_documents.extend(docs)
                    
                elif path.suffix.lower() == '.pdf':
                    # PDFæ–‡ä»¶ï¼šä½¿ç”¨å¢å¼ºPDFåŠ è½½å™¨
                    pdf_loader = EnhancedPDFLoader(str(path))
                    docs = await pdf_loader.aload()
                    type_documents.extend(docs)
                    
                else:
                    # å…¶ä»–æ–‡ä»¶ï¼šä½¿ç”¨è‡ªå®šä¹‰åŠ è½½å™¨
                    loader = FunctionPointDocumentLoader()
                    docs = loader.load_file(path)
                    type_documents.extend(docs)
            
            # ä¸ºæ–‡æ¡£æ·»åŠ ç±»å‹æ ‡è®°
            for doc in type_documents:
                doc.metadata['source_type'] = doc_type.upper()
            
            all_documents[doc_type] = type_documents
            logger.info(f"âœ… {doc_type} æ–‡æ¡£åŠ è½½å®Œæˆ: {len(type_documents)} ä¸ª")
        
        # åŠ è½½ç½‘é¡µèµ„æº
        if include_web_sources:
            logger.info("ğŸŒ åŠ è½½ç½‘é¡µèµ„æº...")
            try:
                web_documents = await load_web_knowledge_base()
                all_documents.update(web_documents)
                
                web_total = sum(len(docs) for docs in web_documents.values())
                logger.info(f"âœ… ç½‘é¡µèµ„æºåŠ è½½å®Œæˆ: {web_total} ä¸ªæ–‡æ¡£")
                
            except Exception as e:
                logger.error(f"âŒ ç½‘é¡µèµ„æºåŠ è½½å¤±è´¥: {e}")
        
        return all_documents
    
    async def _create_pgvector_store(
        self,
        all_documents: Dict[str, List[Document]]
    ) -> PgVectorStore:
        """åˆ›å»ºPgVectorå­˜å‚¨"""
        
        logger.info("ï¿½ï¿½ï¸ åˆ›å»ºPgVectorå­˜å‚¨...")
        
        vector_store = create_pgvector_store(
            documents_by_type=all_documents,
            embeddings=self.embeddings
        )
        
        self.vector_store = vector_store
        logger.info(f"âœ… PgVectorå­˜å‚¨åˆ›å»ºå®Œæˆ: {vector_store.collection_name}")
        return vector_store
    
    async def _create_retrievers(
        self,
        vector_store: PgVectorStore,
        all_documents: Dict[str, List[Document]]
    ) -> Dict[str, Any]:
        """åˆ›å»ºæ£€ç´¢å™¨"""
        
        logger.info("ğŸ” åˆ›å»ºæ£€ç´¢å™¨...")
        
        retrievers = {}
        
        # ä¸ºæ¯ç§æ–‡æ¡£ç±»å‹åˆ›å»ºæ£€ç´¢å™¨
        for doc_type, documents in all_documents.items():
            type_documents = documents
            
            if self.use_hybrid_search:
                # åˆ›å»ºæ··åˆæ£€ç´¢å™¨
                if doc_type.upper() == "NESMA":
                    hybrid_retriever = NESMAHybridSearch(vector_store, self.embeddings)
                elif doc_type.upper() == "COSMIC":
                    hybrid_retriever = COSMICHybridSearch(vector_store, self.embeddings)
                else:
                    hybrid_retriever = HybridSearchStrategy(vector_store, self.embeddings)
                
                # æ„å»ºBM25ç´¢å¼•
                hybrid_retriever.build_bm25_index(type_documents)
                retrievers[f"{doc_type}_hybrid"] = hybrid_retriever
            
            # åˆ›å»ºè¯­ä¹‰æ£€ç´¢å™¨
            semantic_retriever = EnhancedSemanticRetriever(
                vector_store=vector_store,
                embeddings=self.embeddings
            )
            retrievers[f"{doc_type}_semantic"] = semantic_retriever
            
            # åˆ›å»ºå…³é”®è¯æ£€ç´¢å™¨
            if doc_type.upper() == "NESMA":
                keyword_retriever = NESMAKeywordRetriever(type_documents)
            elif doc_type.upper() == "COSMIC":
                keyword_retriever = COSMICKeywordRetriever(type_documents)
            else:
                keyword_retriever = TFIDFRetriever(type_documents)
            
            retrievers[f"{doc_type}_keyword"] = keyword_retriever
            
            # åˆ›å»ºå¤šæŸ¥è¯¢æ£€ç´¢å™¨
            if doc_type.upper() == "NESMA":
                multi_query_retriever = NESMAMultiQueryRetriever(
                    vector_store, self.llm, self.embeddings
                )
            elif doc_type.upper() == "COSMIC":
                multi_query_retriever = COSMICMultiQueryRetriever(
                    vector_store, self.llm, self.embeddings
                )
            else:
                from .retrievers.multi_query_retriever import EnhancedMultiQueryRetriever
                multi_query_retriever = EnhancedMultiQueryRetriever(
                    vector_store, self.llm, self.embeddings
                )
            
            retrievers[f"{doc_type}_multi_query"] = multi_query_retriever
            
            # åˆ›å»ºé›†æˆæ£€ç´¢å™¨ï¼ˆç»„åˆå¤šç§æ£€ç´¢æ–¹æ³•ï¼‰
            base_retriever = vector_store.as_retriever(search_kwargs={"k": 10})
            
            # å¦‚æœæœ‰å…³é”®è¯æ£€ç´¢å™¨ï¼Œåˆ›å»ºé›†æˆæ£€ç´¢å™¨
            try:
                from langchain.retrievers import EnsembleRetriever
                
                # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦é€‚é…ä¸åŒæ£€ç´¢å™¨çš„æ¥å£
                ensemble_retriever = EnsembleRetriever(
                    retrievers=[base_retriever],  # ç›®å‰åªåŒ…å«å‘é‡æ£€ç´¢å™¨
                    weights=[1.0]
                )
                retrievers[f"{doc_type}_ensemble"] = ensemble_retriever
                
            except Exception as e:
                logger.warning(f"åˆ›å»ºé›†æˆæ£€ç´¢å™¨å¤±è´¥ {doc_type}: {e}")
        
        self.retrievers = retrievers
        logger.info(f"âœ… æ£€ç´¢å™¨åˆ›å»ºå®Œæˆ: {len(retrievers)} ä¸ª")
        return retrievers
    
    async def _build_rag_chains(
        self,
        retrievers: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æ„å»ºRAGé“¾"""
        
        logger.info("â›“ï¸ æ„å»ºRAGé“¾...")
        
        chains = {}
        
        # NESMAä¸“ç”¨RAGé“¾
        if "nesma_hybrid" in retrievers or "nesma_semantic" in retrievers:
            nesma_chain = self._create_standard_rag_chain(
                retriever=retrievers.get("nesma_hybrid") or retrievers.get("nesma_semantic"),
                standard="NESMA"
            )
            chains["nesma"] = nesma_chain
        
        # COSMICä¸“ç”¨RAGé“¾
        if "cosmic_hybrid" in retrievers or "cosmic_semantic" in retrievers:
            cosmic_chain = self._create_standard_rag_chain(
                retriever=retrievers.get("cosmic_hybrid") or retrievers.get("cosmic_semantic"),
                standard="COSMIC"
            )
            chains["cosmic"] = cosmic_chain
        
        # é€šç”¨RAGé“¾
        if "common_hybrid" in retrievers or "common_semantic" in retrievers:
            common_chain = self._create_standard_rag_chain(
                retriever=retrievers.get("common_hybrid") or retrievers.get("common_semantic"),
                standard="COMMON"
            )
            chains["common"] = common_chain
        
        self.chains = chains
        logger.info(f"âœ… RAGé“¾æ„å»ºå®Œæˆ: {list(chains.keys())}")
        return chains
    
    def _create_standard_rag_chain(self, retriever: Any, standard: str):
        """åˆ›å»ºæ ‡å‡†RAGé“¾"""
        
        # åˆ›å»ºæ£€ç´¢æç¤ºæ¨¡æ¿
        if standard == "NESMA":
            system_prompt = """
            ä½ æ˜¯NESMAåŠŸèƒ½ç‚¹åˆ†æä¸“å®¶ã€‚è¯·åŸºäºæä¾›çš„NESMAçŸ¥è¯†åº“æ–‡æ¡£å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
            
            å›ç­”è¦æ±‚ï¼š
            1. ä¸¥æ ¼åŸºäºNESMAå®˜æ–¹æ ‡å‡†å’Œè§„åˆ™
            2. å¼•ç”¨å…·ä½“çš„NESMAåˆ†ç±»è§„åˆ™å’Œè®¡ç®—æ–¹æ³•
            3. æä¾›å‡†ç¡®çš„åŠŸèƒ½ç±»å‹è¯†åˆ«æŒ‡å¯¼
            4. åŒ…å«DETã€RETç­‰å¤æ‚åº¦è®¡ç®—è¦ç´ 
            5. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œæ˜ç¡®è¯´æ˜é™åˆ¶
            
            ç›¸å…³æ–‡æ¡£ï¼š
            {context}
            
            ç”¨æˆ·é—®é¢˜ï¼š{question}
            """
            
        elif standard == "COSMIC":
            system_prompt = """
            ä½ æ˜¯COSMICåŠŸèƒ½ç‚¹åˆ†æä¸“å®¶ã€‚è¯·åŸºäºæä¾›çš„COSMICçŸ¥è¯†åº“æ–‡æ¡£å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
            
            å›ç­”è¦æ±‚ï¼š
            1. ä¸¥æ ¼åŸºäºCOSMICå®˜æ–¹æ ‡å‡†å’Œæ–¹æ³•
            2. é‡ç‚¹å…³æ³¨æ•°æ®ç§»åŠ¨è¯†åˆ«å’Œåˆ†ç±»
            3. æä¾›åŠŸèƒ½ç”¨æˆ·å’Œè¾¹ç•Œåˆ†ææŒ‡å¯¼
            4. è§£é‡ŠEntryã€Exitã€Readã€Writeå››ç§æ•°æ®ç§»åŠ¨
            5. åŒ…å«CFPè®¡ç®—çš„å…·ä½“æ­¥éª¤
            
            ç›¸å…³æ–‡æ¡£ï¼š
            {context}
            
            ç”¨æˆ·é—®é¢˜ï¼š{question}
            """
            
        else:
            system_prompt = """
            ä½ æ˜¯åŠŸèƒ½ç‚¹ä¼°ç®—ä¸“å®¶ã€‚è¯·åŸºäºæä¾›çš„çŸ¥è¯†åº“æ–‡æ¡£å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
            
            å›ç­”è¦æ±‚ï¼š
            1. åŸºäºæä¾›çš„æ ‡å‡†æ–‡æ¡£å’Œæœ€ä½³å®è·µ
            2. æä¾›å‡†ç¡®ã€å®ç”¨çš„åˆ†ææŒ‡å¯¼
            3. å¦‚æ¶‰åŠå…·ä½“æ ‡å‡†ï¼Œæ˜ç¡®æŒ‡å‡ºæ˜¯NESMAè¿˜æ˜¯COSMIC
            4. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œæ˜ç¡®è¯´æ˜å¹¶å»ºè®®è¿›ä¸€æ­¥æŸ¥è¯¢
            
            ç›¸å…³æ–‡æ¡£ï¼š
            {context}
            
            ç”¨æˆ·é—®é¢˜ï¼š{question}
            """
        
        # åˆ›å»ºæç¤ºæ¨¡æ¿
        prompt = ChatPromptTemplate.from_template(system_prompt)
        
        # æ„å»ºRAGé“¾
        def format_docs(docs):
            """æ ¼å¼åŒ–æ–‡æ¡£"""
            if not docs:
                return "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚"
            
            formatted = []
            for i, doc in enumerate(docs, 1):
                source = doc.metadata.get('source', 'æœªçŸ¥æ¥æº')
                content = doc.page_content.strip()
                formatted.append(f"æ–‡æ¡£ {i} (æ¥æº: {source}):\n{content}")
            
            return "\n\n".join(formatted)
        
        # å¤„ç†æ··åˆæ£€ç´¢å™¨å’Œæ ‡å‡†æ£€ç´¢å™¨çš„å·®å¼‚
        if hasattr(retriever, 'hybrid_search'):
            # æ··åˆæ£€ç´¢å™¨
            def retrieve_context(query):
                results = retriever.hybrid_search(query["question"], k=5)
                docs = [doc for doc, score in results]
                return format_docs(docs)
        elif hasattr(retriever, 'retrieve_documents'):
            # å¤šæŸ¥è¯¢æ£€ç´¢å™¨
            async def retrieve_context(query):
                docs = await retriever.retrieve_documents(query["question"], k=5)
                return format_docs(docs)
        else:
            # æ ‡å‡†æ£€ç´¢å™¨
            def retrieve_context(query):
                docs = retriever.search(query["question"], k=5)
                if docs and isinstance(docs[0], tuple):
                    # å¦‚æœè¿”å›çš„æ˜¯(doc, score)å…ƒç»„
                    docs = [doc for doc, score in docs]
                return format_docs(docs)
        
        # æ„å»ºRAGé“¾
        rag_chain = (
            {"context": retrieve_context, "question": RunnablePassthrough()}
            | prompt
            | self.llm
            | StrOutputParser()
        )
        
        return rag_chain
    
    def get_chain(self, standard: str) -> Optional[Any]:
        """è·å–æŒ‡å®šæ ‡å‡†çš„RAGé“¾"""
        return self.chains.get(standard.lower())
    
    def get_retriever(self, retriever_name: str) -> Optional[Any]:
        """è·å–æŒ‡å®šçš„æ£€ç´¢å™¨"""
        return self.retrievers.get(retriever_name)
    
    async def query(
        self,
        question: str,
        standard: Optional[str] = None,
        use_multi_standard: bool = False
    ) -> Dict[str, Any]:
        """æ‰§è¡ŒæŸ¥è¯¢"""
        
        if use_multi_standard or standard is None:
            # æŸ¥è¯¢å¤šä¸ªæ ‡å‡†
            results = {}
            
            for std in ["nesma", "cosmic", "common"]:
                chain = self.get_chain(std)
                if chain:
                    try:
                        if asyncio.iscoroutinefunction(chain.invoke):
                            answer = await chain.ainvoke({"question": question})
                        else:
                            answer = chain.invoke({"question": question})
                        results[std.upper()] = answer
                    except Exception as e:
                        logger.error(f"æŸ¥è¯¢{std}æ ‡å‡†å¤±è´¥: {e}")
                        results[std.upper()] = f"æŸ¥è¯¢å¤±è´¥: {e}"
            
            return results
        
        else:
            # æŸ¥è¯¢å•ä¸ªæ ‡å‡†
            chain = self.get_chain(standard.lower())
            if not chain:
                return {"error": f"æœªæ‰¾åˆ°{standard}æ ‡å‡†çš„RAGé“¾"}
            
            try:
                if asyncio.iscoroutinefunction(chain.invoke):
                    answer = await chain.ainvoke({"question": question})
                else:
                    answer = chain.invoke({"question": question})
                
                return {standard.upper(): answer}
                
            except Exception as e:
                logger.error(f"æŸ¥è¯¢å¤±è´¥: {e}")
                return {"error": str(e)}


class RAGChainFactory:
    """RAGé“¾å·¥å‚"""
    
    @staticmethod
    async def create_complete_rag_system(
        document_paths: Dict[str, Union[str, Path, List[str]]],
        embedding_model_name: str = "bge_m3",
        llm: Optional[BaseLanguageModel] = None,
        use_hybrid_search: bool = True
    ) -> RAGChainBuilder:
        """åˆ›å»ºå®Œæ•´çš„RAGç³»ç»Ÿ"""
        
        # è·å–åµŒå…¥æ¨¡å‹
        embeddings = get_embedding_model(embedding_model_name)
        
        # å¦‚æœæ²¡æœ‰æä¾›LLMï¼Œä½¿ç”¨é»˜è®¤é…ç½®
        if llm is None:
            from langchain_openai import ChatOpenAI
            llm = ChatOpenAI(
                base_url="https://api.deepseek.com/v1",
                model="deepseek-chat",
                temperature=0.1,
                max_tokens=4000
            )
        
        # åˆ›å»ºRAGæ„å»ºå™¨
        rag_builder = RAGChainBuilder(
            embeddings=embeddings,
            llm=llm,
            use_hybrid_search=use_hybrid_search
        )
        
        # æ„å»ºçŸ¥è¯†åº“
        await rag_builder.build_knowledge_base(
            document_paths=document_paths,
            include_web_sources=True
        )
        
        return rag_builder


# é¢„å®šä¹‰çš„æ–‡æ¡£è·¯å¾„é…ç½®
DEFAULT_DOCUMENT_PATHS = {
    "nesma": "knowledge_base/documents/nesma",
    "cosmic": "knowledge_base/documents/cosmic", 
    "common": "knowledge_base/documents/common"
}


async def setup_default_rag_system() -> RAGChainBuilder:
    """è®¾ç½®é»˜è®¤çš„RAGç³»ç»Ÿ"""
    
    return await RAGChainFactory.create_complete_rag_system(
        document_paths=DEFAULT_DOCUMENT_PATHS,
        embedding_model_name="bge_m3",
        use_hybrid_search=True
    )


if __name__ == "__main__":
    async def main():
        # æµ‹è¯•RAGç³»ç»Ÿ
        print("ğŸš€ æµ‹è¯•RAGé“¾ç³»ç»Ÿ...")
        
        try:
            # åˆ›å»ºRAGç³»ç»Ÿ
            rag_system = await setup_default_rag_system()
            
            # æµ‹è¯•æŸ¥è¯¢
            test_questions = [
                "ä»€ä¹ˆæ˜¯ILFåŠŸèƒ½ç±»å‹ï¼Ÿ",
                "å¦‚ä½•è¯†åˆ«Entryæ•°æ®ç§»åŠ¨ï¼Ÿ",
                "DETå’ŒRETçš„è®¡ç®—æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ"
            ]
            
            for question in test_questions:
                print(f"\nâ“ é—®é¢˜: {question}")
                
                try:
                    results = await rag_system.query(
                        question=question,
                        use_multi_standard=True
                    )
                    
                    for standard, answer in results.items():
                        print(f"ğŸ“ {standard} å›ç­”:")
                        print(f"   {answer[:200]}...")
                        
                except Exception as e:
                    print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
            
        except Exception as e:
            print(f"âŒ RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(main()) 