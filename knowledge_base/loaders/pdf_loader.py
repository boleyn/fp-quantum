"""
é‡å­æ™ºèƒ½åŒ–åŠŸèƒ½ç‚¹ä¼°ç®—ç³»ç»Ÿ - PDFæ–‡æ¡£åŠ è½½å™¨

æ”¯æŒå¤šç§PDFæ–‡æ¡£åŠ è½½ç­–ç•¥ï¼Œé’ˆå¯¹NESMA/COSMICæ ‡å‡†æ–‡æ¡£ä¼˜åŒ–
"""

import os
from pathlib import Path
from typing import List, Optional, Dict, Any

from langchain_community.document_loaders import (
    PyPDFLoader,
    UnstructuredPDFLoader,
    PDFMinerLoader
)
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

from config.settings import get_settings


class PDFLoaderStrategy:
    """PDFåŠ è½½ç­–ç•¥æšä¸¾"""
    PYPDF = "pypdf"                    # åŸºç¡€PDFåŠ è½½
    UNSTRUCTURED = "unstructured"      # ç»“æ„åŒ–PDFåŠ è½½
    PDFMINER = "pdfminer"             # é«˜ç²¾åº¦æ–‡æœ¬æå–


class EnhancedPDFLoader:
    """å¢å¼ºçš„PDFæ–‡æ¡£åŠ è½½å™¨"""
    
    def __init__(self, strategy: str = PDFLoaderStrategy.UNSTRUCTURED):
        self.strategy = strategy
        self.settings = get_settings()
        
        # æ–‡æœ¬åˆ†å‰²å™¨é…ç½®
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.settings.knowledge_base.chunk_size,
            chunk_overlap=self.settings.knowledge_base.chunk_overlap,
            separators=[
                "\n\n",  # æ®µè½åˆ†éš”
                "\n",    # è¡Œåˆ†éš”
                "ã€‚",    # ä¸­æ–‡å¥å·
                ".",     # è‹±æ–‡å¥å·
                "!",     # æ„Ÿå¹å·
                "?",     # é—®å·
                "ï¼›",    # ä¸­æ–‡åˆ†å·
                ";",     # è‹±æ–‡åˆ†å·
                "ï¼Œ",    # ä¸­æ–‡é€—å·
                ",",     # è‹±æ–‡é€—å·
                " ",     # ç©ºæ ¼
                ""       # å­—ç¬¦
            ]
        )
    
    def load_pdf(self, file_path: str, metadata: Optional[Dict[str, Any]] = None) -> List[Document]:
        """åŠ è½½å•ä¸ªPDFæ–‡ä»¶"""
        file_path = Path(file_path)
        if not file_path.exists():
            raise FileNotFoundError(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # æ ¹æ®ç­–ç•¥é€‰æ‹©åŠ è½½å™¨
        if self.strategy == PDFLoaderStrategy.PYPDF:
            loader = PyPDFLoader(str(file_path))
        elif self.strategy == PDFLoaderStrategy.UNSTRUCTURED:
            loader = UnstructuredPDFLoader(
                str(file_path),
                mode="elements",
                strategy="hi_res"
            )
        elif self.strategy == PDFLoaderStrategy.PDFMINER:
            loader = PDFMinerLoader(str(file_path))
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„PDFåŠ è½½ç­–ç•¥: {self.strategy}")
        
        # åŠ è½½æ–‡æ¡£
        documents = loader.load()
        
        # æ·»åŠ å…ƒæ•°æ®
        base_metadata = {
            "source": str(file_path),
            "file_name": file_path.name,
            "file_type": "pdf",
            "loader_strategy": self.strategy,
        }
        
        if metadata:
            base_metadata.update(metadata)
        
        # ä¸ºæ¯ä¸ªæ–‡æ¡£æ·»åŠ å…ƒæ•°æ®
        for doc in documents:
            doc.metadata.update(base_metadata)
        
        return documents
    
    def load_directory(
        self, 
        directory_path: str, 
        pattern: str = "*.pdf",
        metadata_extractor: Optional[callable] = None
    ) -> List[Document]:
        """åŠ è½½ç›®å½•ä¸­çš„æ‰€æœ‰PDFæ–‡ä»¶"""
        directory_path = Path(directory_path)
        if not directory_path.exists():
            raise FileNotFoundError(f"ç›®å½•ä¸å­˜åœ¨: {directory_path}")
        
        all_documents = []
        pdf_files = list(directory_path.glob(pattern))
        
        if not pdf_files:
            print(f"âš ï¸ åœ¨ç›®å½• {directory_path} ä¸­æœªæ‰¾åˆ°PDFæ–‡ä»¶")
            return all_documents
        
        for pdf_file in pdf_files:
            try:
                # æå–æ–‡ä»¶å…ƒæ•°æ®
                file_metadata = {}
                if metadata_extractor:
                    file_metadata = metadata_extractor(pdf_file)
                
                # åŠ è½½æ–‡æ¡£
                documents = self.load_pdf(str(pdf_file), file_metadata)
                all_documents.extend(documents)
                
                print(f"âœ… æˆåŠŸåŠ è½½: {pdf_file.name} ({len(documents)} ä¸ªæ–‡æ¡£å—)")
                
            except Exception as e:
                print(f"âŒ åŠ è½½å¤±è´¥: {pdf_file.name} - {str(e)}")
                continue
        
        return all_documents
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """åˆ†å‰²æ–‡æ¡£ä¸ºè¾ƒå°çš„å—"""
        return self.text_splitter.split_documents(documents)
    
    def load_and_split(
        self, 
        file_path: str, 
        metadata: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """åŠ è½½å¹¶åˆ†å‰²PDFæ–‡æ¡£"""
        documents = self.load_pdf(file_path, metadata)
        return self.split_documents(documents)


class NESMAPDFLoader(EnhancedPDFLoader):
    """NESMAæ ‡å‡†æ–‡æ¡£ä¸“ç”¨åŠ è½½å™¨"""
    
    def __init__(self):
        super().__init__(strategy=PDFLoaderStrategy.UNSTRUCTURED)
    
    def extract_nesma_metadata(self, file_path: Path) -> Dict[str, Any]:
        """æå–NESMAæ–‡æ¡£çš„å…ƒæ•°æ®"""
        metadata = {
            "source_type": "NESMA",
            "standard": "NESMA",
            "version": self._extract_version_from_filename(file_path.name),
            "document_type": self._classify_nesma_document(file_path.name)
        }
        return metadata
    
    def _extract_version_from_filename(self, filename: str) -> str:
        """ä»æ–‡ä»¶åæå–ç‰ˆæœ¬ä¿¡æ¯"""
        # å¯»æ‰¾ç‰ˆæœ¬æ¨¡å¼ï¼Œå¦‚ v2.3, version_2.3, 2.3ç­‰
        import re
        patterns = [
            r'v?(\d+\.\d+)',
            r'version[_\s]?(\d+\.\d+)',
            r'nesma[_\s]?(\d+\.\d+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, filename.lower())
            if match:
                return match.group(1)
        
        return "unknown"
    
    def _classify_nesma_document(self, filename: str) -> str:
        """åˆ†ç±»NESMAæ–‡æ¡£ç±»å‹"""
        filename_lower = filename.lower()
        
        if any(keyword in filename_lower for keyword in ['manual', 'æ‰‹å†Œ', 'guide', 'æŒ‡å—']):
            return "manual"
        elif any(keyword in filename_lower for keyword in ['specification', 'è§„èŒƒ', 'spec']):
            return "specification"
        elif any(keyword in filename_lower for keyword in ['example', 'ç¤ºä¾‹', 'case']):
            return "example"
        elif any(keyword in filename_lower for keyword in ['rule', 'è§„åˆ™', 'standard']):
            return "rules"
        else:
            return "general"


class COSMICPDFLoader(EnhancedPDFLoader):
    """COSMICæ ‡å‡†æ–‡æ¡£ä¸“ç”¨åŠ è½½å™¨"""
    
    def __init__(self):
        super().__init__(strategy=PDFLoaderStrategy.UNSTRUCTURED)
    
    def extract_cosmic_metadata(self, file_path: Path) -> Dict[str, Any]:
        """æå–COSMICæ–‡æ¡£çš„å…ƒæ•°æ®"""
        metadata = {
            "source_type": "COSMIC",
            "standard": "COSMIC",
            "version": self._extract_version_from_filename(file_path.name),
            "document_type": self._classify_cosmic_document(file_path.name)
        }
        return metadata
    
    def _extract_version_from_filename(self, filename: str) -> str:
        """ä»æ–‡ä»¶åæå–ç‰ˆæœ¬ä¿¡æ¯"""
        import re
        patterns = [
            r'v?(\d+\.\d+)',
            r'version[_\s]?(\d+\.\d+)',
            r'cosmic[_\s]?(\d+\.\d+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, filename.lower())
            if match:
                return match.group(1)
        
        return "unknown"
    
    def _classify_cosmic_document(self, filename: str) -> str:
        """åˆ†ç±»COSMICæ–‡æ¡£ç±»å‹"""
        filename_lower = filename.lower()
        
        if any(keyword in filename_lower for keyword in ['manual', 'æ‰‹å†Œ', 'handbook']):
            return "manual"
        elif any(keyword in filename_lower for keyword in ['method', 'æ–¹æ³•', 'methodology']):
            return "methodology"
        elif any(keyword in filename_lower for keyword in ['guideline', 'æŒ‡å—', 'guide']):
            return "guideline"
        elif any(keyword in filename_lower for keyword in ['example', 'ç¤ºä¾‹', 'case']):
            return "example"
        else:
            return "general"


def create_pdf_loader(source_type: str = "general") -> EnhancedPDFLoader:
    """å·¥å‚å‡½æ•°ï¼šåˆ›å»ºPDFåŠ è½½å™¨"""
    if source_type.upper() == "NESMA":
        return NESMAPDFLoader()
    elif source_type.upper() == "COSMIC":
        return COSMICPDFLoader()
    else:
        return EnhancedPDFLoader()


async def load_knowledge_base_pdfs(
    nesma_path: Optional[str] = None,
    cosmic_path: Optional[str] = None,
    common_path: Optional[str] = None
) -> Dict[str, List[Document]]:
    """åŠ è½½çŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰PDFæ–‡æ¡£"""
    settings = get_settings()
    results = {}
    
    # åŠ è½½NESMAæ–‡æ¡£
    if nesma_path or settings.knowledge_base.nesma_docs_path.exists():
        path = nesma_path or str(settings.knowledge_base.nesma_docs_path)
        loader = NESMAPDFLoader()
        documents = loader.load_directory(
            path, 
            metadata_extractor=loader.extract_nesma_metadata
        )
        if documents:
            # åˆ†å‰²æ–‡æ¡£
            split_docs = loader.split_documents(documents)
            results["nesma"] = split_docs
            print(f"ğŸ“š NESMAæ–‡æ¡£åŠ è½½å®Œæˆ: {len(documents)} åŸå§‹æ–‡æ¡£ -> {len(split_docs)} åˆ†å‰²å—")
    
    # åŠ è½½COSMICæ–‡æ¡£
    if cosmic_path or settings.knowledge_base.cosmic_docs_path.exists():
        path = cosmic_path or str(settings.knowledge_base.cosmic_docs_path)
        loader = COSMICPDFLoader()
        documents = loader.load_directory(
            path,
            metadata_extractor=loader.extract_cosmic_metadata
        )
        if documents:
            # åˆ†å‰²æ–‡æ¡£
            split_docs = loader.split_documents(documents)
            results["cosmic"] = split_docs
            print(f"ğŸŒŒ COSMICæ–‡æ¡£åŠ è½½å®Œæˆ: {len(documents)} åŸå§‹æ–‡æ¡£ -> {len(split_docs)} åˆ†å‰²å—")
    
    # åŠ è½½é€šç”¨æ–‡æ¡£
    if common_path or settings.knowledge_base.common_docs_path.exists():
        path = common_path or str(settings.knowledge_base.common_docs_path)
        loader = EnhancedPDFLoader()
        documents = loader.load_directory(path)
        if documents:
            # æ·»åŠ é€šç”¨æ ‡è¯†
            for doc in documents:
                doc.metadata["source_type"] = "COMMON"
                doc.metadata["standard"] = "COMMON"
            
            # åˆ†å‰²æ–‡æ¡£
            split_docs = loader.split_documents(documents)
            results["common"] = split_docs
            print(f"ğŸ“– é€šç”¨æ–‡æ¡£åŠ è½½å®Œæˆ: {len(documents)} åŸå§‹æ–‡æ¡£ -> {len(split_docs)} åˆ†å‰²å—")
    
    return results


if __name__ == "__main__":
    import asyncio
    
    async def main():
        # æµ‹è¯•PDFåŠ è½½å™¨
        print("ğŸ” æµ‹è¯•PDFåŠ è½½å™¨...")
        
        # åŠ è½½çŸ¥è¯†åº“æ–‡æ¡£
        documents = await load_knowledge_base_pdfs()
        
        total_docs = sum(len(docs) for docs in documents.values())
        print(f"\nğŸ“Š æ€»è®¡åŠ è½½ {total_docs} ä¸ªæ–‡æ¡£å—:")
        
        for source_type, docs in documents.items():
            print(f"  {source_type.upper()}: {len(docs)} ä¸ªæ–‡æ¡£å—")
            
            if docs:
                # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ–‡æ¡£çš„ç¤ºä¾‹
                sample_doc = docs[0]
                print(f"    ç¤ºä¾‹å†…å®¹: {sample_doc.page_content[:200]}...")
                print(f"    å…ƒæ•°æ®: {sample_doc.metadata}")
    
    asyncio.run(main()) 