"""
é‡å­æ™ºèƒ½åŒ–åŠŸèƒ½ç‚¹ä¼°ç®—ç³»ç»Ÿ - ç½‘é¡µå†…å®¹åŠ è½½å™¨

ä»ç½‘é¡µæŠ“å–å’Œå¤„ç†NESMAã€COSMICç›¸å…³å†…å®¹
"""

import asyncio
import aiohttp
from typing import List, Dict, Any, Optional, Union
from urllib.parse import urljoin, urlparse
import logging
from datetime import datetime
from bs4 import BeautifulSoup
import re

from langchain_core.documents import Document
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

logger = logging.getLogger(__name__)


class EnhancedWebLoader:
    """å¢å¼ºçš„ç½‘é¡µåŠ è½½å™¨"""
    
    def __init__(
        self,
        headers: Optional[Dict[str, str]] = None,
        timeout: int = 30,
        max_retries: int = 3
    ):
        self.headers = headers or {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        self.timeout = timeout
        self.max_retries = max_retries
        self.session: Optional[aiohttp.ClientSession] = None
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            headers=self.headers,
            timeout=aiohttp.ClientTimeout(total=self.timeout)
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def load_url(self, url: str) -> Document:
        """åŠ è½½å•ä¸ªURL"""
        
        for attempt in range(self.max_retries):
            try:
                async with self.session.get(url) as response:
                    if response.status == 200:
                        content = await response.text()
                        
                        # è§£æHTML
                        soup = BeautifulSoup(content, 'html.parser')
                        
                        # æå–æ ‡é¢˜
                        title = soup.find('title')
                        title_text = title.get_text().strip() if title else url
                        
                        # æå–ä¸»è¦å†…å®¹
                        text_content = self._extract_main_content(soup)
                        
                        # åˆ›å»ºæ–‡æ¡£
                        metadata = {
                            'source': url,
                            'title': title_text,
                            'loaded_at': datetime.now().isoformat(),
                            'content_length': len(text_content)
                        }
                        
                        return Document(page_content=text_content, metadata=metadata)
                    
                    else:
                        logger.warning(f"HTTP {response.status} for {url}")
                        
            except Exception as e:
                logger.error(f"åŠ è½½URLå¤±è´¥ {url} (å°è¯• {attempt + 1}): {e}")
                if attempt == self.max_retries - 1:
                    raise
                
                await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
    
    async def load_urls(self, urls: List[str]) -> List[Document]:
        """æ‰¹é‡åŠ è½½URL"""
        
        tasks = [self.load_url(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        documents = []
        for i, result in enumerate(results):
            if isinstance(result, Document):
                documents.append(result)
            else:
                logger.error(f"åŠ è½½URLå¤±è´¥ {urls[i]}: {result}")
        
        return documents
    
    def _extract_main_content(self, soup: BeautifulSoup) -> str:
        """æå–ä¸»è¦å†…å®¹"""
        
        # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
        for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            tag.decompose()
        
        # å°è¯•æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸ
        main_content = None
        
        # å¸¸è§çš„ä¸»è¦å†…å®¹æ ‡ç­¾
        for selector in ['main', 'article', '.content', '#content', '.main']:
            main_content = soup.select_one(selector)
            if main_content:
                break
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨body
        if not main_content:
            main_content = soup.find('body') or soup
        
        # æå–æ–‡æœ¬
        text = main_content.get_text(separator='\n', strip=True)
        
        # æ¸…ç†æ–‡æœ¬
        text = re.sub(r'\n\s*\n', '\n\n', text)  # åˆå¹¶å¤šä¸ªç©ºè¡Œ
        text = re.sub(r'[ \t]+', ' ', text)  # åˆå¹¶å¤šä¸ªç©ºæ ¼
        
        return text.strip()


class NESMAWebLoader:
    """NESMAå®˜æ–¹èµ„æºç½‘é¡µåŠ è½½å™¨"""
    
    def __init__(self):
        self.nesma_urls = [
            "https://www.nesma.org/",
            "https://www.nesma.org/function-point-analysis/",
            # å¯ä»¥æ·»åŠ æ›´å¤šNESMAç›¸å…³URL
        ]
        
    async def load_nesma_resources(self) -> List[Document]:
        """åŠ è½½NESMAå®˜æ–¹èµ„æº"""
        
        async with EnhancedWebLoader() as loader:
            documents = await loader.load_urls(self.nesma_urls)
            
            # ä¸ºNESMAæ–‡æ¡£æ·»åŠ ç‰¹å®šæ ‡è®°
            for doc in documents:
                doc.metadata['source_type'] = 'NESMA'
                doc.metadata['category'] = 'official_resource'
            
            return documents


class COSMICWebLoader:
    """COSMICå®˜æ–¹èµ„æºç½‘é¡µåŠ è½½å™¨"""
    
    def __init__(self):
        self.cosmic_urls = [
            "https://cosmic-sizing.org/",
            "https://cosmic-sizing.org/cosmic-method/",
            # å¯ä»¥æ·»åŠ æ›´å¤šCOSMICç›¸å…³URL
        ]
        
    async def load_cosmic_resources(self) -> List[Document]:
        """åŠ è½½COSMICå®˜æ–¹èµ„æº"""
        
        async with EnhancedWebLoader() as loader:
            documents = await loader.load_urls(self.cosmic_urls)
            
            # ä¸ºCOSMICæ–‡æ¡£æ·»åŠ ç‰¹å®šæ ‡è®°
            for doc in documents:
                doc.metadata['source_type'] = 'COSMIC'
                doc.metadata['category'] = 'official_resource'
            
            return documents


class WebContentProcessor:
    """ç½‘é¡µå†…å®¹å¤„ç†å™¨"""
    
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", "ã€‚", ".", "!", "?"]
        )
        
    def process_documents(self, documents: List[Document]) -> List[Document]:
        """å¤„ç†ç½‘é¡µæ–‡æ¡£"""
        
        processed_docs = []
        
        for doc in documents:
            # åˆ†å‰²æ–‡æ¡£
            splits = self.text_splitter.split_documents([doc])
            
            # ä¸ºæ¯ä¸ªåˆ†ç‰‡æ·»åŠ åºå·
            for i, split in enumerate(splits):
                split.metadata.update({
                    'chunk_index': i,
                    'total_chunks': len(splits),
                    'processed_at': datetime.now().isoformat()
                })
                
            processed_docs.extend(splits)
        
        return processed_docs
    
    def filter_by_keywords(
        self, 
        documents: List[Document], 
        keywords: List[str]
    ) -> List[Document]:
        """æ ¹æ®å…³é”®è¯è¿‡æ»¤æ–‡æ¡£"""
        
        filtered_docs = []
        
        for doc in documents:
            content_lower = doc.page_content.lower()
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»ä½•å…³é”®è¯
            if any(keyword.lower() in content_lower for keyword in keywords):
                doc.metadata['matched_keywords'] = [
                    kw for kw in keywords 
                    if kw.lower() in content_lower
                ]
                filtered_docs.append(doc)
        
        return filtered_docs


# é¢„å®šä¹‰çš„å…³é”®è¯é›†åˆ
NESMA_KEYWORDS = [
    "function point", "functional point", "NESMA", "ILF", "EIF", "EI", "EO", "EQ",
    "DET", "RET", "complexity", "UFP", "unadjusted function points"
]

COSMIC_KEYWORDS = [
    "COSMIC", "CFP", "data movement", "entry", "exit", "read", "write",
    "functional user", "boundary", "functional process", "data group"
]


async def load_web_knowledge_base() -> Dict[str, List[Document]]:
    """åŠ è½½ç½‘é¡µçŸ¥è¯†åº“"""
    
    logger.info("ğŸŒ å¼€å§‹åŠ è½½ç½‘é¡µçŸ¥è¯†åº“...")
    
    try:
        # åŠ è½½NESMAèµ„æº
        nesma_loader = NESMAWebLoader()
        nesma_docs = await nesma_loader.load_nesma_resources()
        
        # åŠ è½½COSMICèµ„æº
        cosmic_loader = COSMICWebLoader()
        cosmic_docs = await cosmic_loader.load_cosmic_resources()
        
        # å¤„ç†æ–‡æ¡£
        processor = WebContentProcessor()
        
        # è¿‡æ»¤å’Œå¤„ç†NESMAæ–‡æ¡£
        nesma_filtered = processor.filter_by_keywords(nesma_docs, NESMA_KEYWORDS)
        nesma_processed = processor.process_documents(nesma_filtered)
        
        # è¿‡æ»¤å’Œå¤„ç†COSMICæ–‡æ¡£
        cosmic_filtered = processor.filter_by_keywords(cosmic_docs, COSMIC_KEYWORDS)
        cosmic_processed = processor.process_documents(cosmic_filtered)
        
        logger.info(f"âœ… ç½‘é¡µçŸ¥è¯†åº“åŠ è½½å®Œæˆ:")
        logger.info(f"   NESMAæ–‡æ¡£: {len(nesma_processed)} ä¸ªç‰‡æ®µ")
        logger.info(f"   COSMICæ–‡æ¡£: {len(cosmic_processed)} ä¸ªç‰‡æ®µ")
        
        return {
            'nesma': nesma_processed,
            'cosmic': cosmic_processed,
            'all': nesma_processed + cosmic_processed
        }
        
    except Exception as e:
        logger.error(f"âŒ ç½‘é¡µçŸ¥è¯†åº“åŠ è½½å¤±è´¥: {e}")
        return {'nesma': [], 'cosmic': [], 'all': []}


if __name__ == "__main__":
    async def main():
        # æµ‹è¯•ç½‘é¡µåŠ è½½å™¨
        result = await load_web_knowledge_base()
        
        print(f"åŠ è½½çš„æ–‡æ¡£æ€»æ•°: {len(result['all'])}")
        print(f"NESMAæ–‡æ¡£: {len(result['nesma'])}")
        print(f"COSMICæ–‡æ¡£: {len(result['cosmic'])}")
        
        # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ–‡æ¡£ç¤ºä¾‹
        if result['all']:
            first_doc = result['all'][0]
            print(f"\nç¤ºä¾‹æ–‡æ¡£:")
            print(f"æ ‡é¢˜: {first_doc.metadata.get('title', 'N/A')}")
            print(f"æ¥æº: {first_doc.metadata.get('source', 'N/A')}")
            print(f"å†…å®¹é•¿åº¦: {len(first_doc.page_content)}")
    
    asyncio.run(main()) 