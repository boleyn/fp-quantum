"""
é‡å­æ™ºèƒ½åŒ–åŠŸèƒ½ç‚¹ä¼°ç®—ç³»ç»Ÿ - å…³é”®è¯æ£€ç´¢å™¨

åŸºäºä¼ ç»ŸIRæŠ€æœ¯çš„å…³é”®è¯æ£€ç´¢å®ç°
"""

import logging
import re
from typing import List, Dict, Any, Optional, Tuple, Set
from collections import defaultdict, Counter
import math

from langchain_core.documents import Document

logger = logging.getLogger(__name__)


class TFIDFRetriever:
    """TF-IDFå…³é”®è¯æ£€ç´¢å™¨"""
    
    def __init__(
        self,
        documents: List[Document],
        language: str = "mixed"  # mixed, chinese, english
    ):
        self.documents = documents
        self.language = language
        
        # é¢„å¤„ç†æ–‡æ¡£
        self.processed_docs = []
        self.doc_frequencies: Dict[str, int] = defaultdict(int)
        self.idf_scores: Dict[str, float] = {}
        self.doc_vectors: List[Dict[str, float]] = []
        
        # æ„å»ºç´¢å¼•
        self._build_index()
        
    def _build_index(self):
        """æ„å»ºTF-IDFç´¢å¼•"""
        
        logger.info(f"ğŸ“š æ„å»ºTF-IDFç´¢å¼•ï¼Œæ–‡æ¡£æ•°: {len(self.documents)}")
        
        # ç¬¬ä¸€éï¼šè®¡ç®—æ–‡æ¡£é¢‘ç‡
        for doc in self.documents:
            tokens = self._tokenize(doc.page_content)
            unique_tokens = set(tokens)
            
            for token in unique_tokens:
                self.doc_frequencies[token] += 1
                
            self.processed_docs.append(tokens)
        
        # è®¡ç®—IDFåˆ†æ•°
        total_docs = len(self.documents)
        for term, doc_freq in self.doc_frequencies.items():
            self.idf_scores[term] = math.log(total_docs / doc_freq)
        
        # ç¬¬äºŒéï¼šè®¡ç®—TF-IDFå‘é‡
        for tokens in self.processed_docs:
            tf_counts = Counter(tokens)
            doc_length = len(tokens)
            
            doc_vector = {}
            for term, count in tf_counts.items():
                tf = count / doc_length if doc_length > 0 else 0
                idf = self.idf_scores[term]
                doc_vector[term] = tf * idf
            
            self.doc_vectors.append(doc_vector)
        
        logger.info(f"âœ… TF-IDFç´¢å¼•æ„å»ºå®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {len(self.idf_scores)}")
    
    def _tokenize(self, text: str) -> List[str]:
        """æ–‡æœ¬åˆ†è¯"""
        
        tokens = []
        
        if self.language in ["mixed", "english"]:
            # è‹±æ–‡å•è¯æå–
            english_words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
            tokens.extend(english_words)
        
        if self.language in ["mixed", "chinese"]:
            # ä¸­æ–‡å­—ç¬¦æå–
            chinese_chars = re.findall(r'[\u4e00-\u9fff]+', text)
            # ç®€å•çš„ä¸­æ–‡åˆ†è¯ï¼ˆæ¯ä¸ªå­—ç¬¦ä½œä¸ºä¸€ä¸ªè¯ï¼‰
            for chars in chinese_chars:
                tokens.extend(list(chars))
        
        # æ•°å­—å’Œç‰¹æ®Šæœ¯è¯­
        numbers = re.findall(r'\d+', text)
        tokens.extend(numbers)
        
        # åŠŸèƒ½ç‚¹ç›¸å…³æœ¯è¯­
        fp_terms = re.findall(r'\b(?:ILF|EIF|EI|EO|EQ|DET|RET|CFP|Entry|Exit|Read|Write)\b', text)
        tokens.extend([term.lower() for term in fp_terms])
        
        # è¿‡æ»¤åœç”¨è¯
        stop_words = {
            'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'è¿™', 'é‚£', 'ä¸€ä¸ª', 'ä¸€ç§',
            'the', 'is', 'are', 'was', 'were', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at'
        }
        
        filtered_tokens = [token for token in tokens if token not in stop_words and len(token) > 1]
        
        return filtered_tokens
    
    def search(
        self,
        query: str,
        k: int = 10,
        filter_metadata: Optional[Dict[str, Any]] = None
    ) -> List[Tuple[Document, float]]:
        """æœç´¢ç›¸å…³æ–‡æ¡£"""
        
        # æŸ¥è¯¢åˆ†è¯
        query_tokens = self._tokenize(query)
        if not query_tokens:
            logger.warning("æŸ¥è¯¢ä¸ºç©ºï¼Œè¿”å›ç©ºç»“æœ")
            return []
        
        # è®¡ç®—æŸ¥è¯¢å‘é‡
        query_tf = Counter(query_tokens)
        query_length = len(query_tokens)
        
        query_vector = {}
        for term, count in query_tf.items():
            if term in self.idf_scores:
                tf = count / query_length
                idf = self.idf_scores[term]
                query_vector[term] = tf * idf
        
        if not query_vector:
            logger.warning("æŸ¥è¯¢ä¸­æ²¡æœ‰æœ‰æ•ˆè¯æ±‡ï¼Œè¿”å›ç©ºç»“æœ")
            return []
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for i, (doc, doc_vector) in enumerate(zip(self.documents, self.doc_vectors)):
            # åº”ç”¨å…ƒæ•°æ®è¿‡æ»¤
            if filter_metadata:
                if not all(
                    doc.metadata.get(key) == value
                    for key, value in filter_metadata.items()
                ):
                    continue
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = self._cosine_similarity(query_vector, doc_vector)
            if similarity > 0:
                similarities.append((doc, similarity))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        logger.debug(f"TF-IDFæ£€ç´¢è¿”å› {len(similarities[:k])} ä¸ªç»“æœ")
        return similarities[:k]
    
    def _cosine_similarity(
        self,
        vector1: Dict[str, float],
        vector2: Dict[str, float]
    ) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        
        # è®¡ç®—ç‚¹ç§¯
        dot_product = 0.0
        common_terms = set(vector1.keys()) & set(vector2.keys())
        
        for term in common_terms:
            dot_product += vector1[term] * vector2[term]
        
        # è®¡ç®—å‘é‡é•¿åº¦
        norm1 = math.sqrt(sum(value ** 2 for value in vector1.values()))
        norm2 = math.sqrt(sum(value ** 2 for value in vector2.values()))
        
        # é¿å…é™¤é›¶
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)


class BooleanRetriever:
    """å¸ƒå°”æ£€ç´¢å™¨"""
    
    def __init__(self, documents: List[Document]):
        self.documents = documents
        
        # æ„å»ºå€’æ’ç´¢å¼•
        self.inverted_index: Dict[str, Set[int]] = defaultdict(set)
        self._build_inverted_index()
    
    def _build_inverted_index(self):
        """æ„å»ºå€’æ’ç´¢å¼•"""
        
        logger.info(f"ğŸ”¨ æ„å»ºå€’æ’ç´¢å¼•ï¼Œæ–‡æ¡£æ•°: {len(self.documents)}")
        
        for doc_id, doc in enumerate(self.documents):
            tokens = self._tokenize(doc.page_content)
            unique_tokens = set(tokens)
            
            for token in unique_tokens:
                self.inverted_index[token].add(doc_id)
        
        logger.info(f"âœ… å€’æ’ç´¢å¼•æ„å»ºå®Œæˆï¼Œè¯æ±‡æ•°: {len(self.inverted_index)}")
    
    def _tokenize(self, text: str) -> List[str]:
        """ç®€å•åˆ†è¯"""
        
        # è‹±æ–‡å’Œæ•°å­—
        tokens = re.findall(r'\b\w+\b', text.lower())
        
        # ä¸­æ–‡å­—ç¬¦
        chinese_chars = re.findall(r'[\u4e00-\u9fff]', text)
        tokens.extend(chinese_chars)
        
        # åŠŸèƒ½ç‚¹æœ¯è¯­
        fp_terms = re.findall(r'\b(?:ILF|EIF|EI|EO|EQ|DET|RET|CFP|Entry|Exit|Read|Write)\b', text)
        tokens.extend([term.lower() for term in fp_terms])
        
        return tokens
    
    def search(
        self,
        query: str,
        operator: str = "AND",  # AND, OR
        filter_metadata: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """å¸ƒå°”æœç´¢"""
        
        # è§£ææŸ¥è¯¢
        query_terms = self._tokenize(query)
        if not query_terms:
            return []
        
        # è·å–æ¯ä¸ªè¯çš„æ–‡æ¡£é›†åˆ
        term_doc_sets = []
        for term in query_terms:
            if term in self.inverted_index:
                term_doc_sets.append(self.inverted_index[term])
            else:
                term_doc_sets.append(set())
        
        # æ ¹æ®æ“ä½œç¬¦åˆå¹¶ç»“æœ
        if operator.upper() == "AND":
            if term_doc_sets:
                result_doc_ids = term_doc_sets[0].copy()
                for doc_set in term_doc_sets[1:]:
                    result_doc_ids &= doc_set
            else:
                result_doc_ids = set()
        elif operator.upper() == "OR":
            result_doc_ids = set()
            for doc_set in term_doc_sets:
                result_doc_ids |= doc_set
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ“ä½œç¬¦: {operator}")
        
        # è·å–æ–‡æ¡£
        results = []
        for doc_id in result_doc_ids:
            doc = self.documents[doc_id]
            
            # åº”ç”¨å…ƒæ•°æ®è¿‡æ»¤
            if filter_metadata:
                if not all(
                    doc.metadata.get(key) == value
                    for key, value in filter_metadata.items()
                ):
                    continue
            
            results.append(doc)
        
        logger.debug(f"å¸ƒå°”æ£€ç´¢({operator})è¿”å› {len(results)} ä¸ªç»“æœ")
        return results


class FuzzyMatchRetriever:
    """æ¨¡ç³ŠåŒ¹é…æ£€ç´¢å™¨"""
    
    def __init__(self, documents: List[Document]):
        self.documents = documents
    
    def search(
        self,
        query: str,
        similarity_threshold: float = 0.6,
        k: int = 10,
        filter_metadata: Optional[Dict[str, Any]] = None
    ) -> List[Tuple[Document, float]]:
        """æ¨¡ç³ŠåŒ¹é…æœç´¢"""
        
        query_lower = query.lower()
        results = []
        
        for doc in self.documents:
            # åº”ç”¨å…ƒæ•°æ®è¿‡æ»¤
            if filter_metadata:
                if not all(
                    doc.metadata.get(key) == value
                    for key, value in filter_metadata.items()
                ):
                    continue
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            content_lower = doc.page_content.lower()
            similarity = self._string_similarity(query_lower, content_lower)
            
            if similarity >= similarity_threshold:
                results.append((doc, similarity))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        results.sort(key=lambda x: x[1], reverse=True)
        
        logger.debug(f"æ¨¡ç³ŠåŒ¹é…è¿”å› {len(results[:k])} ä¸ªç»“æœ")
        return results[:k]
    
    def _string_similarity(self, s1: str, s2: str) -> float:
        """å­—ç¬¦ä¸²ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆç®€åŒ–ç‰ˆLevenshteinè·ç¦»ï¼‰"""
        
        # ä½¿ç”¨Jaccardç›¸ä¼¼åº¦è¿‘ä¼¼
        words1 = set(s1.split())
        words2 = set(s2.split())
        
        if not words1 and not words2:
            return 1.0
        
        intersection = words1 & words2
        union = words1 | words2
        
        return len(intersection) / len(union) if union else 0.0


class KeywordRetrieverFactory:
    """å…³é”®è¯æ£€ç´¢å™¨å·¥å‚"""
    
    @staticmethod
    def create_retriever(
        retriever_type: str,
        documents: List[Document],
        **kwargs
    ):
        """åˆ›å»ºæ£€ç´¢å™¨"""
        
        if retriever_type.lower() == "tfidf":
            return TFIDFRetriever(documents, **kwargs)
        elif retriever_type.lower() == "boolean":
            return BooleanRetriever(documents)
        elif retriever_type.lower() == "fuzzy":
            return FuzzyMatchRetriever(documents)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ£€ç´¢å™¨ç±»å‹: {retriever_type}")


class NESMAKeywordRetriever:
    """NESMAä¸“ç”¨å…³é”®è¯æ£€ç´¢å™¨"""
    
    def __init__(self, documents: List[Document]):
        # è¿‡æ»¤NESMAæ–‡æ¡£
        nesma_docs = [
            doc for doc in documents
            if doc.metadata.get('source_type') == 'NESMA'
        ]
        
        self.tfidf_retriever = TFIDFRetriever(nesma_docs)
        self.boolean_retriever = BooleanRetriever(nesma_docs)
        
        # NESMAç‰¹å®šæœ¯è¯­æƒé‡
        self.term_weights = {
            'ilf': 2.0, 'eif': 2.0, 'ei': 2.0, 'eo': 2.0, 'eq': 2.0,
            'det': 1.5, 'ret': 1.5,
            'complexity': 1.5, 'function': 1.3
        }
    
    def search_function_rules(
        self,
        function_type: str,
        k: int = 5
    ) -> List[Tuple[Document, float]]:
        """æœç´¢åŠŸèƒ½åˆ†ç±»è§„åˆ™"""
        
        query = f"{function_type} åŠŸèƒ½ åˆ†ç±» è§„åˆ™"
        
        # TF-IDFæœç´¢
        results = self.tfidf_retriever.search(
            query=query,
            k=k,
            filter_metadata={'category': 'classification'}
        )
        
        # åº”ç”¨NESMAæœ¯è¯­æƒé‡æå‡
        boosted_results = []
        for doc, score in results:
            boost_factor = 1.0
            content_lower = doc.page_content.lower()
            
            for term, weight in self.term_weights.items():
                if term in content_lower:
                    boost_factor *= weight
            
            boosted_score = min(score * boost_factor, 1.0)  # é™åˆ¶æœ€å¤§åˆ†æ•°
            boosted_results.append((doc, boosted_score))
        
        # é‡æ–°æ’åº
        boosted_results.sort(key=lambda x: x[1], reverse=True)
        
        return boosted_results
    
    def search_complexity_rules(
        self,
        function_type: str,
        det_count: Optional[int] = None,
        ret_count: Optional[int] = None,
        k: int = 3
    ) -> List[Tuple[Document, float]]:
        """æœç´¢å¤æ‚åº¦è®¡ç®—è§„åˆ™"""
        
        query_parts = [f"{function_type}", "å¤æ‚åº¦", "DET", "RET"]
        
        if det_count is not None:
            query_parts.append(f"DET {det_count}")
        if ret_count is not None:
            query_parts.append(f"RET {ret_count}")
        
        query = " ".join(query_parts)
        
        return self.tfidf_retriever.search(
            query=query,
            k=k,
            filter_metadata={
                'category': 'complexity',
                'function_type': function_type
            }
        )


class COSMICKeywordRetriever:
    """COSMICä¸“ç”¨å…³é”®è¯æ£€ç´¢å™¨"""
    
    def __init__(self, documents: List[Document]):
        # è¿‡æ»¤COSMICæ–‡æ¡£
        cosmic_docs = [
            doc for doc in documents
            if doc.metadata.get('source_type') == 'COSMIC'
        ]
        
        self.tfidf_retriever = TFIDFRetriever(cosmic_docs)
        self.boolean_retriever = BooleanRetriever(cosmic_docs)
        
        # COSMICç‰¹å®šæœ¯è¯­æƒé‡
        self.term_weights = {
            'entry': 2.0, 'exit': 2.0, 'read': 2.0, 'write': 2.0,
            'movement': 1.8, 'functional': 1.5, 'boundary': 1.5,
            'cfp': 1.8, 'data': 1.3
        }
    
    def search_data_movement_rules(
        self,
        movement_type: str,
        k: int = 5
    ) -> List[Tuple[Document, float]]:
        """æœç´¢æ•°æ®ç§»åŠ¨è§„åˆ™"""
        
        query = f"{movement_type} æ•°æ®ç§»åŠ¨ è§„åˆ™ è¯†åˆ«"
        
        results = self.tfidf_retriever.search(
            query=query,
            k=k,
            filter_metadata={'category': 'data_movement'}
        )
        
        # åº”ç”¨COSMICæœ¯è¯­æƒé‡æå‡
        boosted_results = []
        for doc, score in results:
            boost_factor = 1.0
            content_lower = doc.page_content.lower()
            
            for term, weight in self.term_weights.items():
                if term in content_lower:
                    boost_factor *= weight
            
            boosted_score = min(score * boost_factor, 1.0)
            boosted_results.append((doc, boosted_score))
        
        boosted_results.sort(key=lambda x: x[1], reverse=True)
        return boosted_results
    
    def search_boundary_rules(
        self,
        boundary_description: str,
        k: int = 5
    ) -> List[Tuple[Document, float]]:
        """æœç´¢è¾¹ç•Œåˆ†æè§„åˆ™"""
        
        query = f"{boundary_description} è½¯ä»¶è¾¹ç•Œ æŒä¹…å­˜å‚¨ åŠŸèƒ½ç”¨æˆ·"
        
        return self.tfidf_retriever.search(
            query=query,
            k=k,
            filter_metadata={'category': 'boundary'}
        )


if __name__ == "__main__":
    # æµ‹è¯•å…³é”®è¯æ£€ç´¢å™¨
    
    # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£
    test_documents = [
        Document(
            page_content="ILFæ˜¯å†…éƒ¨é€»è¾‘æ–‡ä»¶ï¼Œç”±åº”ç”¨ç¨‹åºå†…éƒ¨ç»´æŠ¤çš„æ•°æ®ç»„æˆ",
            metadata={"source_type": "NESMA", "category": "classification", "function_type": "ILF"}
        ),
        Document(
            page_content="Entryæ•°æ®ç§»åŠ¨è¡¨ç¤ºæ•°æ®ä»åŠŸèƒ½ç”¨æˆ·è¿›å…¥è½¯ä»¶è¾¹ç•Œ",
            metadata={"source_type": "COSMIC", "category": "data_movement", "movement_type": "Entry"}
        ),
        Document(
            page_content="DETè®¡ç®—åŒ…æ‹¬ç”¨æˆ·å¯è¯†åˆ«çš„æ•°æ®å…ƒç´ ï¼ŒRETæ˜¯è®°å½•å…ƒç´ ç±»å‹çš„æ•°é‡",
            metadata={"source_type": "NESMA", "category": "complexity", "function_type": "ILF"}
        )
    ]
    
    # æµ‹è¯•TF-IDFæ£€ç´¢å™¨
    print("ğŸ” æµ‹è¯•TF-IDFæ£€ç´¢å™¨")
    tfidf = TFIDFRetriever(test_documents)
    
    results = tfidf.search("ILF å†…éƒ¨é€»è¾‘æ–‡ä»¶", k=2)
    print(f"æ£€ç´¢ç»“æœ: {len(results)} ä¸ªæ–‡æ¡£")
    for i, (doc, score) in enumerate(results):
        print(f"  {i+1}. åˆ†æ•°: {score:.3f}, å†…å®¹: {doc.page_content[:30]}...")
    
    # æµ‹è¯•NESMAä¸“ç”¨æ£€ç´¢å™¨
    print("\nğŸ¯ æµ‹è¯•NESMAä¸“ç”¨æ£€ç´¢å™¨")
    nesma_retriever = NESMAKeywordRetriever(test_documents)
    
    results = nesma_retriever.search_function_rules("ILF", k=2)
    print(f"NESMAåŠŸèƒ½è§„åˆ™: {len(results)} ä¸ªæ–‡æ¡£")
    for i, (doc, score) in enumerate(results):
        print(f"  {i+1}. åˆ†æ•°: {score:.3f}, å†…å®¹: {doc.page_content[:30]}...") 