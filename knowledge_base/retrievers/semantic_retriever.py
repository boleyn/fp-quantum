"""
é‡å­æ™ºèƒ½åŒ–åŠŸèƒ½ç‚¹ä¼°ç®—ç³»ç»Ÿ - è¯­ä¹‰æ£€ç´¢å™¨

åŸºäºPgVectorå®ç°é«˜è´¨é‡çš„è¯­ä¹‰æ£€ç´¢
"""

import asyncio
import logging
from typing import List, Dict, Any, Optional, Tuple, Union
from datetime import datetime

from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_core.language_models import BaseLanguageModel
from langchain_core.vectorstores import VectorStore
from langchain_core.retrievers import BaseRetriever
from langchain_core.callbacks import CallbackManagerForRetrieverRun
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.retrievers.contextual_compression import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

from knowledge_base.vector_stores.pgvector_store import PgVectorStore
from models.common_models import EstimationStandard, KnowledgeQuery, KnowledgeResult

logger = logging.getLogger(__name__)


class EnhancedSemanticRetriever(BaseRetriever):
    """å¢å¼ºè¯­ä¹‰æ£€ç´¢å™¨ - åŸºäºPgVector"""
    
    # å£°æ˜ç±»å­—æ®µï¼ˆPydantic éœ€è¦ï¼‰
    vector_store: Any = None
    embeddings: Any = None 
    llm: Any = None
    source_type: Optional[str] = None
    k: int = 5
    config: Dict[str, Any] = None
    base_retriever: Any = None
    multi_query_retriever: Any = None
    compression_retriever: Any = None
    
    def __init__(
        self,
        vector_store: PgVectorStore,
        embeddings: Embeddings,
        llm: BaseLanguageModel,
        source_type: Optional[str] = None,
        k: int = 5
    ):
        super().__init__()
        self.vector_store = vector_store
        self.embeddings = embeddings
        self.llm = llm
        self.source_type = source_type
        self.k = k
        
        # æ£€ç´¢é…ç½®
        self.config = {
            "score_threshold": 0.7,
            "diversity_threshold": 0.8,
            "use_multi_query": True,
            "use_compression": True
        }
        
        # è®¾ç½®å¢å¼ºæ£€ç´¢å™¨
        self._setup_enhanced_retrievers()

    def _setup_enhanced_retrievers(self):
        """è®¾ç½®å¢å¼ºæ£€ç´¢å™¨"""
        # åˆ›å»ºåŸºç¡€æ£€ç´¢å™¨ - ç›´æ¥ä½¿ç”¨LangChainåŸç”Ÿçš„PGVectorå®ä¾‹
        self.base_retriever = self.vector_store.as_retriever(
            source_type=self.source_type,
            search_kwargs={"k": self.k, "filter": {"source_type": self.source_type} if self.source_type else None}
        )
        
        # å¤šæŸ¥è¯¢æ£€ç´¢å™¨
        if self.config["use_multi_query"] and self.llm:
            try:
                self.multi_query_retriever = MultiQueryRetriever.from_llm(
                    retriever=self.base_retriever,
                    llm=self.llm,
                    include_original=True
                )
            except Exception as e:
                logger.warning(f"âš ï¸ å¤šæŸ¥è¯¢æ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                self.multi_query_retriever = self.base_retriever
        else:
            self.multi_query_retriever = self.base_retriever
        
        # ä¸Šä¸‹æ–‡å‹ç¼©æ£€ç´¢å™¨
        if self.config["use_compression"] and self.llm:
            try:
                compressor = LLMChainExtractor.from_llm(self.llm)
                self.compression_retriever = ContextualCompressionRetriever(
                    base_compressor=compressor,
                    base_retriever=self.multi_query_retriever
                )
            except Exception as e:
                logger.warning(f"âš ï¸ ä¸Šä¸‹æ–‡å‹ç¼©æ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                self.compression_retriever = self.multi_query_retriever
        else:
            self.compression_retriever = self.multi_query_retriever
    
    def _get_relevant_documents(
        self,
        query: str,
        *,
        run_manager: CallbackManagerForRetrieverRun
    ) -> List[Document]:
        """è·å–ç›¸å…³æ–‡æ¡£ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
        try:
            loop = asyncio.get_event_loop()
            return loop.run_until_complete(self.aget_relevant_documents(query, run_manager=run_manager))
        except RuntimeError:
            # å¦‚æœæ²¡æœ‰äº‹ä»¶å¾ªç¯ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„
            return asyncio.run(self.aget_relevant_documents(query, run_manager=run_manager))
    
    async def aget_relevant_documents(
        self, 
        query: str, 
        *, 
        run_manager: Optional[CallbackManagerForRetrieverRun] = None
    ) -> List[Document]:
        """å¼‚æ­¥è·å–ç›¸å…³æ–‡æ¡£"""
        try:
            # ä½¿ç”¨å¢å¼ºæ£€ç´¢å™¨
            if self.compression_retriever:
                return await self.compression_retriever.aget_relevant_documents(query, run_manager=run_manager)
            elif self.multi_query_retriever:
                return await self.multi_query_retriever.aget_relevant_documents(query, run_manager=run_manager)
            else:
                return await self.base_retriever.aget_relevant_documents(query, run_manager=run_manager)
        except Exception as e:
            logger.error(f"âŒ æ£€ç´¢æ–‡æ¡£å¤±è´¥: {str(e)}")
            return []
    
    async def similarity_search_with_score(
        self,
        query: str,
        k: Optional[int] = None,
        filter_dict: Optional[Dict[str, Any]] = None
    ) -> List[Tuple[Document, float]]:
        """å¸¦åˆ†æ•°çš„ç›¸ä¼¼åº¦æœç´¢"""
        k = k or self.k
        
        try:
            # ç›´æ¥ä½¿ç”¨PgVectorçš„ç›¸ä¼¼åº¦æœç´¢
            return await self.vector_store.similarity_search_with_score(
                query=query,
                k=k,
                filter=filter_dict or ({"source_type": self.source_type} if self.source_type else None)
            )
        except Exception as e:
            logger.error(f"âŒ ç›¸ä¼¼åº¦æœç´¢å¤±è´¥: {str(e)}")
            return []


class PgVectorMultiSourceRetriever:
    """å¤šæºæ£€ç´¢å™¨ - åŸºäºPgVectoræ”¯æŒè·¨NESMAã€COSMICã€é€šç”¨çŸ¥è¯†åº“æ£€ç´¢"""
    
    def __init__(
        self,
        vector_store: PgVectorStore,
        embeddings: Embeddings,
        llm: BaseLanguageModel
    ):
        self.vector_store = vector_store
        self.embeddings = embeddings
        self.llm = llm
        
        # ä¸ºæ¯ä¸ªçŸ¥è¯†æºåˆ›å»ºæ£€ç´¢å™¨
        self.retrievers = {}
        for source_type in ["NESMA", "COSMIC", "COMMON"]:
            self.retrievers[source_type] = EnhancedSemanticRetriever(
                vector_store=vector_store,
                embeddings=embeddings,
                llm=llm,
                source_type=source_type,
                k=5
            )
    
    async def retrieve_by_source(
        self,
        query: str,
        source_type: str,
        k: int = 5
    ) -> KnowledgeResult:
        """æŒ‰æŒ‡å®šçŸ¥è¯†æºæ£€ç´¢"""
        import time
        start_time = time.time()
        
        source_type = source_type.upper()
        if source_type not in self.retrievers:
            raise ValueError(f"ä¸æ”¯æŒçš„çŸ¥è¯†æº: {source_type}")
        
        retriever = self.retrievers[source_type]
        
        # æ‰§è¡Œæ£€ç´¢
        try:
            docs_with_scores = await retriever.similarity_search_with_score(query, k)
            
            # è½¬æ¢ä¸ºæ£€ç´¢ç»“æœ
            retrieved_chunks = []
            for doc, score in docs_with_scores:
                chunk_data = {
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "score": score,
                    "source": doc.metadata.get("source", "unknown")
                }
                retrieved_chunks.append(chunk_data)
            
            retrieval_time = int((time.time() - start_time) * 1000)
            
            result = KnowledgeResult(
                query=query,
                source_type=EstimationStandard(source_type),
                retrieved_chunks=retrieved_chunks,
                total_chunks=len(retrieved_chunks),
                max_score=max([chunk["score"] for chunk in retrieved_chunks]) if retrieved_chunks else 0.0,
                min_score=min([chunk["score"] for chunk in retrieved_chunks]) if retrieved_chunks else 0.0,
                avg_score=sum([chunk["score"] for chunk in retrieved_chunks]) / len(retrieved_chunks) if retrieved_chunks else 0.0,
                processing_time_ms=retrieval_time
            )
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ æ£€ç´¢å¤±è´¥ {source_type}: {str(e)}")
            return KnowledgeResult(
                query=query,
                source_type=EstimationStandard(source_type),
                processing_time_ms=int((time.time() - start_time) * 1000)
            )
    
    async def retrieve_multi_source(
        self,
        query: str,
        sources: List[str] = ["NESMA", "COSMIC", "COMMON"],
        k_per_source: int = 3
    ) -> Dict[str, KnowledgeResult]:
        """å¤šæºæ£€ç´¢"""
        results = {}
        
        # å¹¶è¡Œæ£€ç´¢æ‰€æœ‰æº
        tasks = []
        for source in sources:
            task = self.retrieve_by_source(query, source, k_per_source)
            tasks.append((source, task))
        
        # ç­‰å¾…æ‰€æœ‰æ£€ç´¢å®Œæˆ
        for source, task in tasks:
            try:
                result = await task
                results[source] = result
            except Exception as e:
                logger.error(f"âŒ å¤šæºæ£€ç´¢å¤±è´¥ {source}: {str(e)}")
                results[source] = KnowledgeResult(
                    query=query,
                    source_type=EstimationStandard(source)
                )
        
        return results
    
    async def adaptive_retrieve(
        self,
        query: str,
        preferred_source: Optional[str] = None,
        fallback_sources: Optional[List[str]] = None,
        min_chunks: int = 3
    ) -> KnowledgeResult:
        """è‡ªé€‚åº”æ£€ç´¢ - ä¼˜å…ˆä½¿ç”¨æŒ‡å®šæºï¼Œä¸è¶³æ—¶ä½¿ç”¨å¤‡ç”¨æº"""
        
        if preferred_source:
            # ä¼˜å…ˆæ£€ç´¢æŒ‡å®šæº
            result = await self.retrieve_by_source(query, preferred_source, 5)
            if self._is_result_sufficient(result, min_chunks):
                return result
        
        # å¦‚æœé¦–é€‰æºä¸è¶³ï¼Œå°è¯•å¤‡ç”¨æº
        fallback_sources = fallback_sources or ["NESMA", "COSMIC", "COMMON"]
        if preferred_source and preferred_source in fallback_sources:
            fallback_sources.remove(preferred_source)
        
        # å°è¯•å¤‡ç”¨æº
        for source in fallback_sources:
            try:
                result = await self.retrieve_by_source(query, source, 5)
                if self._is_result_sufficient(result, min_chunks):
                    return result
            except Exception as e:
                logger.warning(f"âš ï¸ å¤‡ç”¨æºæ£€ç´¢å¤±è´¥ {source}: {str(e)}")
                continue
        
        # å¦‚æœæ‰€æœ‰å•æºéƒ½ä¸è¶³ï¼Œè¿›è¡Œå¤šæºåˆå¹¶æ£€ç´¢
        multi_results = await self.retrieve_multi_source(query, fallback_sources, 2)
        
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        all_chunks = []
        for result in multi_results.values():
            all_chunks.extend(result.retrieved_chunks)
        
        # æŒ‰åˆ†æ•°æ’åºå¹¶å–top-k
        all_chunks.sort(key=lambda x: x["score"], reverse=True)
        final_chunks = all_chunks[:min_chunks * 2]  # å–è¶³å¤Ÿçš„æ•°é‡
        
        return KnowledgeResult(
            query=query,
            source_type=EstimationStandard.BOTH,
            retrieved_chunks=final_chunks,
            total_chunks=len(final_chunks),
            max_score=max([chunk["score"] for chunk in final_chunks]) if final_chunks else 0.0,
            min_score=min([chunk["score"] for chunk in final_chunks]) if final_chunks else 0.0,
            avg_score=sum([chunk["score"] for chunk in final_chunks]) / len(final_chunks) if final_chunks else 0.0
        )
    
    def _is_result_sufficient(self, result: KnowledgeResult, min_chunks: int) -> bool:
        """æ£€æŸ¥ç»“æœæ˜¯å¦è¶³å¤Ÿ"""
        return (
            result.total_chunks >= min_chunks and
            result.avg_score >= 0.7
        )


async def create_pgvector_retrievers(
    vector_store: PgVectorStore,
    embeddings: Embeddings,
    llm: BaseLanguageModel
) -> Dict[str, Any]:
    """åˆ›å»ºåŸºäºPgVectorçš„çŸ¥è¯†æ£€ç´¢å™¨"""
    
    logger.info("ğŸ” åˆ›å»ºPgVectorçŸ¥è¯†æ£€ç´¢å™¨...")
    
    retrievers = {
        # å•æºæ£€ç´¢å™¨
        "nesma": EnhancedSemanticRetriever(
            vector_store=vector_store,
            embeddings=embeddings,
            llm=llm,
            source_type="NESMA",
            k=5
        ),
        "cosmic": EnhancedSemanticRetriever(
            vector_store=vector_store,
            embeddings=embeddings,
            llm=llm,
            source_type="COSMIC",
            k=5
        ),
        "common": EnhancedSemanticRetriever(
            vector_store=vector_store,
            embeddings=embeddings,
            llm=llm,
            source_type="COMMON",
            k=5
        ),
        
        # å¤šæºæ£€ç´¢å™¨
        "multi_source": PgVectorMultiSourceRetriever(
            vector_store=vector_store,
            embeddings=embeddings,
            llm=llm
        )
    }
    
    logger.info(f"âœ… PgVectorçŸ¥è¯†æ£€ç´¢å™¨åˆ›å»ºå®Œæˆ: {list(retrievers.keys())}")
    return retrievers


if __name__ == "__main__":
    # æµ‹è¯•è¯­ä¹‰æ£€ç´¢å™¨
    import os
    from langchain_openai import OpenAIEmbeddings, ChatOpenAI
    
    async def main():
        # æµ‹è¯•PgVectorè¯­ä¹‰æ£€ç´¢å™¨
        from knowledge_base.vector_stores.pgvector_store import PgVectorStore
        
        # åˆå§‹åŒ–ç»„ä»¶
        vector_store = PgVectorStore()
        await vector_store.initialize()
        
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1)
        
        # åˆ›å»ºæ£€ç´¢å™¨
        retrievers = await create_pgvector_retrievers(vector_store, embeddings, llm)
        multi_retriever = retrievers["multi_source"]
        
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "ä»€ä¹ˆæ˜¯NESMAåŠŸèƒ½ç‚¹ä¼°ç®—",
            "COSMICæ•°æ®ç§»åŠ¨çš„å®šä¹‰",
            "å†…éƒ¨é€»è¾‘æ–‡ä»¶çš„åˆ†ç±»è§„åˆ™"
        ]
        
        for query in test_queries:
            print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢: {query}")
            
            # å•æºæ£€ç´¢æµ‹è¯•
            nesma_result = await multi_retriever.retrieve_by_source(query, "NESMA", 3)
            print(f"NESMAæ£€ç´¢ç»“æœ: {nesma_result.total_chunks}ä¸ªå—, å¹³å‡åˆ†æ•°: {nesma_result.avg_score:.3f}")
            
            # å¤šæºæ£€ç´¢æµ‹è¯•
            multi_results = await multi_retriever.retrieve_multi_source(query, k_per_source=2)
            for source, result in multi_results.items():
                print(f"{source}æ£€ç´¢ç»“æœ: {result.total_chunks}ä¸ªå—, å¹³å‡åˆ†æ•°: {result.avg_score:.3f}")
        
        await vector_store.close()
    
    asyncio.run(main()) 