#!/usr/bin/env python3
"""
é‡å­æ™ºèƒ½åŒ–åŠŸèƒ½ç‚¹ä¼°ç®—ç³»ç»Ÿ - æ™ºèƒ½çŸ¥è¯†åº“ç®¡ç†å™¨

åŸºäºæ–‡ä»¶MD5çš„å¢é‡æ›´æ–°çŸ¥è¯†åº“ç®¡ç†ç³»ç»Ÿï¼š
1. MongoDBè®°å½•æ–‡æ¡£å…ƒæ•°æ®å’ŒçŠ¶æ€
2. å¯åŠ¨æ—¶æ£€æµ‹æ–‡ä»¶å˜åŒ–ï¼ˆæ–°å¢ã€ä¿®æ”¹ã€åˆ é™¤ï¼‰
3. å¢é‡æ›´æ–°PgVectorå‘é‡å­˜å‚¨
4. åŸºäºLangChainçš„å®Œæ•´RAGé›†æˆ

ä¸šåŠ¡é€»è¾‘ï¼š
- è®°å½•æ–‡æ¡£MD5åˆ°MongoDB
- å¯åŠ¨æ—¶å¯¹æ¯”æ–‡ä»¶ç³»ç»Ÿä¸MongoDBè®°å½•
- è‡ªåŠ¨å¤„ç†æ–‡ä»¶å˜åŒ–ï¼ˆå¢åˆ æ”¹ï¼‰
- å‘é‡åŒ–å¤„ç†å’ŒPgVectorå­˜å‚¨
"""

import asyncio
import hashlib
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple, Set
from datetime import datetime
import time

from langchain_core.documents import Document
from langchain_community.document_loaders import (
    PyPDFLoader,
    UnstructuredPowerPointLoader,
    UnstructuredPDFLoader
)
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_postgres import PGVector
from langchain_postgres.vectorstores import DistanceStrategy
from langchain_openai import OpenAIEmbeddings
from motor.motor_asyncio import AsyncIOMotorClient, AsyncIOMotorDatabase

from config.settings import get_settings
from knowledge_base.embeddings.embedding_models import get_embedding_model

logger = logging.getLogger(__name__)


class DocumentRecord:
    """æ–‡æ¡£è®°å½•æ¨¡å‹"""
    
    def __init__(
        self,
        file_path: str,
        category: str,
        file_name: str,
        md5_hash: str,
        file_size: int,
        last_modified: datetime,
        status: str = "pending",  # pending, processing, completed, failed
        chunk_count: int = 0,
        vector_ids: List[str] = None,
        error_message: str = None,
        processed_at: datetime = None
    ):
        self.file_path = file_path
        self.category = category
        self.file_name = file_name
        self.md5_hash = md5_hash
        self.file_size = file_size
        self.last_modified = last_modified
        self.status = status
        self.chunk_count = chunk_count
        self.vector_ids = vector_ids or []
        self.error_message = error_message
        self.processed_at = processed_at
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "file_path": self.file_path,
            "category": self.category,
            "file_name": self.file_name,
            "md5_hash": self.md5_hash,
            "file_size": self.file_size,
            "last_modified": self.last_modified,
            "status": self.status,
            "chunk_count": self.chunk_count,
            "vector_ids": self.vector_ids,
            "error_message": self.error_message,
            "processed_at": self.processed_at,
            "created_at": datetime.utcnow(),
            "updated_at": datetime.utcnow()
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "DocumentRecord":
        """ä»å­—å…¸åˆ›å»ºå®ä¾‹"""
        return cls(
            file_path=data.get("file_path"),
            category=data.get("category"),
            file_name=data.get("file_name"),
            md5_hash=data.get("md5_hash"),
            file_size=data.get("file_size"),
            last_modified=data.get("last_modified"),
            status=data.get("status", "pending"),
            chunk_count=data.get("chunk_count", 0),
            vector_ids=data.get("vector_ids", []),
            error_message=data.get("error_message"),
            processed_at=data.get("processed_at")
        )


class IncrementalKnowledgeBaseManager:
    """å¢é‡çŸ¥è¯†åº“ç®¡ç†å™¨"""
    
    def __init__(self):
        self.settings = get_settings()
        self.base_dir = Path("knowledge_base")
        
        # MongoDBè¿æ¥
        self.mongo_client: Optional[AsyncIOMotorClient] = None
        self.mongo_db: Optional[AsyncIOMotorDatabase] = None
        self.documents_collection = None
        
        # PgVectorè¿æ¥é…ç½®
        self.connection_string = self._build_pg_connection_string()
        self.vector_stores: Dict[str, PGVector] = {}
        
        # æ–‡æ¡£å¤„ç†é…ç½®
        self.supported_categories = ["nesma", "cosmic", "common"]
        self.supported_extensions = {".pdf", ".pptx", ".md", ".txt"}
        
        # æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", "ã€‚", ".", "!", "?", "ï¼›", ";", "ï¼š", ":"]
        )
    
    def _build_pg_connection_string(self) -> str:
        """æ„å»ºPostgreSQLè¿æ¥å­—ç¬¦ä¸²"""
        db_config = self.settings.database
        # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„é…ç½®å±æ€§å
        return (
            f"postgresql+psycopg://{db_config.postgres_user}:"
            f"{db_config.postgres_password}@{db_config.postgres_host}:"
            f"{db_config.postgres_port}/{db_config.postgres_database}"
        )
    
    async def initialize(self):
        """åˆå§‹åŒ–ç®¡ç†å™¨"""
        logger.info("ğŸš€ åˆå§‹åŒ–å¢é‡çŸ¥è¯†åº“ç®¡ç†å™¨...")
        
        # åˆå§‹åŒ–MongoDBè¿æ¥
        await self._init_mongodb()
        
        # åˆå§‹åŒ–PgVectorè¿æ¥
        await self._init_pgvector()
        
        logger.info("âœ… å¢é‡çŸ¥è¯†åº“ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def _init_mongodb(self):
        """åˆå§‹åŒ–MongoDBè¿æ¥"""
        try:
            db_config = self.settings.database
            connection_string = f"mongodb://{db_config.mongodb_user}:{db_config.mongodb_password}@{db_config.mongodb_host}:{db_config.mongodb_port}"
            
            self.mongo_client = AsyncIOMotorClient(connection_string)
            self.mongo_db = self.mongo_client[db_config.mongodb_db]
            self.documents_collection = self.mongo_db.knowledge_documents
            
            # åˆ›å»ºç´¢å¼•
            await self.documents_collection.create_index([
                ("file_path", 1),
                ("category", 1),
                ("md5_hash", 1),
                ("status", 1)
            ])
            
            logger.info("âœ… MongoDBè¿æ¥åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ MongoDBåˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def _init_pgvector(self):
        """åˆå§‹åŒ–PgVectorè¿æ¥ - æŒ‰ç…§LangChainæ ‡å‡†è§„èŒƒ"""
        try:
            embeddings = get_embedding_model()
            
            for category in self.supported_categories:
                collection_name = f"fp_quantum_{category}"
                
                # æŒ‰ç…§LangChainæ ‡å‡†è§„èŒƒåˆ›å»ºPGVectorå®ä¾‹ - ä½¿ç”¨æ­£ç¡®çš„API
                vector_store = PGVector(
                    embeddings=embeddings,
                    collection_name=collection_name,
                    connection=self.connection_string,
                    distance_strategy=DistanceStrategy.COSINE,
                    use_jsonb=True,
                    create_extension=True,
                    pre_delete_collection=False
                )
                
                self.vector_stores[category] = vector_store
                logger.info(f"âœ… {category} PgVectorå­˜å‚¨åˆå§‹åŒ–å®Œæˆ")
                
        except Exception as e:
            logger.error(f"âŒ PgVectoråˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def calculate_file_md5(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶MD5å“ˆå¸Œ"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            logger.error(f"âŒ è®¡ç®—æ–‡ä»¶MD5å¤±è´¥ {file_path}: {e}")
            return ""
    
    async def scan_file_system(self) -> Dict[str, List[DocumentRecord]]:
        """æ‰«ææ–‡ä»¶ç³»ç»Ÿï¼Œåˆ›å»ºæ–‡æ¡£è®°å½•"""
        logger.info("ğŸ” æ‰«ææ–‡ä»¶ç³»ç»Ÿ...")
        
        current_files = {
            "nesma": [],
            "cosmic": [],
            "common": []
        }
        
        for category in self.supported_categories:
            category_dir = self.base_dir / "documents" / category
            
            if not category_dir.exists():
                logger.warning(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨: {category_dir}")
                continue
            
            for file_path in category_dir.rglob("*"):
                if file_path.is_file() and file_path.suffix.lower() in self.supported_extensions:
                    
                    try:
                        # è·å–æ–‡ä»¶ä¿¡æ¯
                        stat = file_path.stat()
                        md5_hash = self.calculate_file_md5(file_path)
                        
                        if not md5_hash:
                            continue
                        
                        record = DocumentRecord(
                            file_path=str(file_path),
                            category=category,
                            file_name=file_path.name,
                            md5_hash=md5_hash,
                            file_size=stat.st_size,
                            last_modified=datetime.fromtimestamp(stat.st_mtime)
                        )
                        
                        current_files[category].append(record)
                        
                    except Exception as e:
                        logger.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                        continue
        
        total_files = sum(len(files) for files in current_files.values())
        logger.info(f"âœ… æ–‡ä»¶ç³»ç»Ÿæ‰«æå®Œæˆï¼Œå‘ç° {total_files} ä¸ªæ–‡ä»¶")
        
        return current_files
    
    async def get_mongodb_records(self) -> Dict[str, List[DocumentRecord]]:
        """ä»MongoDBè·å–å·²è®°å½•çš„æ–‡æ¡£"""
        logger.info("ğŸ“Š è·å–MongoDBä¸­çš„æ–‡æ¡£è®°å½•...")
        
        records = {
            "nesma": [],
            "cosmic": [],
            "common": []
        }
        
        try:
            cursor = self.documents_collection.find({})
            async for doc in cursor:
                record = DocumentRecord.from_dict(doc)
                if record.category in records:
                    records[record.category].append(record)
            
            total_records = sum(len(recs) for recs in records.values())
            logger.info(f"âœ… ä»MongoDBè·å– {total_records} æ¡æ–‡æ¡£è®°å½•")
            
        except Exception as e:
            logger.error(f"âŒ è·å–MongoDBè®°å½•å¤±è´¥: {e}")
        
        return records
    
    async def compare_and_detect_changes(
        self,
        current_files: Dict[str, List[DocumentRecord]],
        mongo_records: Dict[str, List[DocumentRecord]]
    ) -> Dict[str, Any]:
        """æ¯”è¾ƒæ–‡ä»¶ç³»ç»Ÿä¸MongoDBè®°å½•ï¼Œæ£€æµ‹å˜åŒ–"""
        logger.info("ğŸ” æ£€æµ‹æ–‡ä»¶å˜åŒ–...")
        
        changes = {
            "new_files": [],      # æ–°å¢æ–‡ä»¶
            "modified_files": [], # ä¿®æ”¹æ–‡ä»¶ï¼ˆMD5å˜åŒ–ï¼‰
            "deleted_files": [],  # åˆ é™¤æ–‡ä»¶
            "unchanged_files": [] # æœªå˜åŒ–æ–‡ä»¶
        }
        
        # åˆ›å»ºMongoDBè®°å½•çš„å¿«é€ŸæŸ¥æ‰¾å­—å…¸
        mongo_lookup = {}
        for category, records in mongo_records.items():
            for record in records:
                mongo_lookup[record.file_path] = record
        
        # æ£€æŸ¥å½“å‰æ–‡ä»¶
        for category, files in current_files.items():
            for file_record in files:
                file_path = file_record.file_path
                
                if file_path not in mongo_lookup:
                    # æ–°æ–‡ä»¶
                    changes["new_files"].append(file_record)
                    logger.info(f"ğŸ†• æ–°æ–‡ä»¶: {file_record.file_name}")
                else:
                    mongo_record = mongo_lookup[file_path]
                    
                    if file_record.md5_hash != mongo_record.md5_hash:
                        # æ–‡ä»¶å·²ä¿®æ”¹
                        file_record.vector_ids = mongo_record.vector_ids  # ä¿ç•™å‘é‡IDç”¨äºåˆ é™¤
                        changes["modified_files"].append(file_record)
                        logger.info(f"ğŸ”„ æ–‡ä»¶å·²ä¿®æ”¹: {file_record.file_name}")
                    else:
                        # æ–‡ä»¶æœªå˜åŒ–
                        changes["unchanged_files"].append(file_record)
        
        # æ£€æŸ¥å·²åˆ é™¤çš„æ–‡ä»¶
        current_file_paths = set()
        for files in current_files.values():
            for file_record in files:
                current_file_paths.add(file_record.file_path)
        
        for file_path, mongo_record in mongo_lookup.items():
            if file_path not in current_file_paths:
                changes["deleted_files"].append(mongo_record)
                logger.info(f"ğŸ—‘ï¸ æ–‡ä»¶å·²åˆ é™¤: {mongo_record.file_name}")
        
        logger.info(
            f"âœ… å˜åŒ–æ£€æµ‹å®Œæˆ: "
            f"æ–°å¢{len(changes['new_files'])}, "
            f"ä¿®æ”¹{len(changes['modified_files'])}, "
            f"åˆ é™¤{len(changes['deleted_files'])}, "
            f"æœªå˜åŒ–{len(changes['unchanged_files'])}"
        )
        
        return changes
    
    async def process_deleted_files(self, deleted_files: List[DocumentRecord]):
        """å¤„ç†å·²åˆ é™¤çš„æ–‡ä»¶"""
        if not deleted_files:
            return
        
        logger.info(f"ğŸ—‘ï¸ å¤„ç† {len(deleted_files)} ä¸ªå·²åˆ é™¤æ–‡ä»¶...")
        
        for record in deleted_files:
            try:
                # ä»PgVectoråˆ é™¤å‘é‡æ•°æ®
                if record.vector_ids:
                    vector_store = self.vector_stores.get(record.category)
                    if vector_store:
                        # æ³¨æ„ï¼šLangChain PGVectorå¯èƒ½éœ€è¦ç‰¹æ®Šçš„åˆ é™¤æ–¹æ³•
                        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯èƒ½éœ€è¦æ ¹æ®å…·ä½“å®ç°è°ƒæ•´
                        logger.info(f"ğŸ—‘ï¸ åˆ é™¤å‘é‡æ•°æ®: {record.file_name} ({len(record.vector_ids)} ä¸ªå‘é‡)")
                
                # ä»MongoDBåˆ é™¤è®°å½•
                await self.documents_collection.delete_one({"file_path": record.file_path})
                
                logger.info(f"âœ… å·²åˆ é™¤: {record.file_name}")
                
            except Exception as e:
                logger.error(f"âŒ åˆ é™¤æ–‡ä»¶è®°å½•å¤±è´¥ {record.file_name}: {e}")
    
    async def process_modified_files(self, modified_files: List[DocumentRecord]):
        """å¤„ç†å·²ä¿®æ”¹çš„æ–‡ä»¶"""
        if not modified_files:
            return
        
        logger.info(f"ğŸ”„ å¤„ç† {len(modified_files)} ä¸ªå·²ä¿®æ”¹æ–‡ä»¶...")
        
        for record in modified_files:
            # å…ˆåˆ é™¤æ—§çš„å‘é‡æ•°æ®
            if record.vector_ids:
                vector_store = self.vector_stores.get(record.category)
                if vector_store:
                    logger.info(f"ğŸ—‘ï¸ åˆ é™¤æ—§å‘é‡æ•°æ®: {record.file_name}")
                    # TODO: å®ç°å‘é‡åˆ é™¤é€»è¾‘
            
            # é‡æ–°å¤„ç†æ–‡ä»¶ï¼Œ_process_single_fileå·²ç»åŒ…å«é”™è¯¯å¤„ç†
            await self._process_single_file(record)
    
    async def process_new_files(self, new_files: List[DocumentRecord]):
        """å¤„ç†æ–°æ–‡ä»¶"""
        if not new_files:
            return
        
        logger.info(f"ğŸ†• å¤„ç† {len(new_files)} ä¸ªæ–°æ–‡ä»¶...")
        
        for record in new_files:
            # _process_single_file å·²ç»åŒ…å«äº†é”™è¯¯å¤„ç†å’Œè®°å½•é€»è¾‘
            await self._process_single_file(record)
    
    async def _process_single_file(self, record: DocumentRecord):
        """å¤„ç†å•ä¸ªæ–‡ä»¶ - å…ˆæˆåŠŸå¤„ç†ï¼Œå†è®°å½•çŠ¶æ€"""
        logger.info(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {record.file_name}")
        
        try:
            # åŠ è½½æ–‡æ¡£
            documents = await self._load_document(Path(record.file_path))
            
            if not documents:
                record.status = "failed"
                record.error_message = "æ— æ³•åŠ è½½æ–‡æ¡£å†…å®¹"
                await self._save_record_to_mongodb(record)
                return
            
            # åˆ†å‰²æ–‡æ¡£
            split_docs = self.text_splitter.split_documents(documents)
            
            # æ·»åŠ å…ƒæ•°æ®
            for idx, doc in enumerate(split_docs):
                if not doc.metadata:
                    doc.metadata = {}
                
                doc.metadata.update({
                    "source_category": record.category,
                    "file_name": record.file_name,
                    "file_path": record.file_path,
                    "chunk_index": idx,
                    "total_chunks": len(split_docs),
                    "file_md5": record.md5_hash,
                    "processed_at": datetime.utcnow().isoformat()
                })
            
            # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ - ä½¿ç”¨LangChainæ ‡å‡†åŒæ­¥API
            vector_store = self.vector_stores[record.category]
            try:
                # å¤„ç†æ‰¹æ¬¡å¤§å°é™åˆ¶ - BGE-M3 APIé™åˆ¶ä¸º64ä¸ªæ–‡æ¡£/æ‰¹æ¬¡
                max_batch_size = 32  # ä¿å®ˆèµ·è§è®¾ç½®ä¸º32
                vector_ids = []
                
                for i in range(0, len(split_docs), max_batch_size):
                    batch = split_docs[i:i + max_batch_size]
                    
                    try:
                        # ä½¿ç”¨LangChain PGVectoræ ‡å‡†åŒæ­¥API
                        batch_ids = vector_store.add_documents(batch)
                        if batch_ids:
                            vector_ids.extend(batch_ids)
                        logger.info(f"ğŸ“¦ æ‰¹æ¬¡ {i//max_batch_size + 1}: æ·»åŠ äº† {len(batch)} ä¸ªæ–‡æ¡£å—")
                        
                    except Exception as batch_error:
                        logger.warning(f"âš ï¸ æ‰¹æ¬¡å¤„ç†å¤±è´¥ï¼Œå°è¯•é€ä¸ªå¤„ç†: {batch_error}")
                        # å¦‚æœæ‰¹æ¬¡å¤±è´¥ï¼Œå°è¯•é€ä¸ªæ·»åŠ 
                        for doc in batch:
                            try:
                                single_ids = vector_store.add_documents([doc])
                                if single_ids:
                                    vector_ids.extend(single_ids)
                            except Exception as sync_error:
                                logger.error(f"âŒ å•ä¸ªæ–‡æ¡£æ·»åŠ å¤±è´¥: {sync_error}")
                                continue
                            
            except Exception as e:
                logger.error(f"âŒ å‘é‡å­˜å‚¨å¤±è´¥ {record.file_name}: {e}")
                # æœ€åçš„åŒæ­¥å›é€€
                try:
                    vector_ids = vector_store.add_documents(split_docs)
                except Exception as e2:
                    logger.error(f"âŒ åŒæ­¥å‘é‡å­˜å‚¨ä¹Ÿå¤±è´¥ {record.file_name}: {e2}")
                    raise e2
            
            # ğŸ¯ å…³é”®ä¿®å¤ï¼šåªæœ‰å¤„ç†å®Œå…¨æˆåŠŸåæ‰è®°å½•åˆ°MongoDB
            record.status = "completed"
            record.chunk_count = len(split_docs)
            record.vector_ids = vector_ids if isinstance(vector_ids, list) else []
            record.processed_at = datetime.utcnow()
            record.error_message = None
            
            await self._save_record_to_mongodb(record)
            
            logger.info(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {record.file_name} ({len(split_docs)} ä¸ªå—)")
            
        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥ {record.file_name}: {e}")
            # åªåœ¨ç¡®å®å¤±è´¥æ—¶æ‰è®°å½•å¤±è´¥çŠ¶æ€
            record.status = "failed"
            record.error_message = str(e)
            record.processed_at = datetime.utcnow()
            await self._save_record_to_mongodb(record)
            # ä¸å†æŠ›å‡ºå¼‚å¸¸ï¼Œè®©æµç¨‹ç»§ç»­å¤„ç†å…¶ä»–æ–‡ä»¶
    
    async def _load_document(self, file_path: Path) -> List[Document]:
        """åŠ è½½å•ä¸ªæ–‡æ¡£"""
        try:
            if file_path.suffix.lower() == '.pdf':
                # ä½¿ç”¨PyPDFLoaderæ ‡å‡†åŒæ­¥API
                try:
                    loader = PyPDFLoader(str(file_path))
                    documents = loader.load()
                    return documents
                except ImportError as e:
                    if "pdfminer" in str(e):
                        logger.error(f"âŒ ç¼ºå°‘PDFå¤„ç†ä¾èµ– {file_path.name}: {e}")
                        logger.info("ğŸ’¡ è¯·å®‰è£…: pip install pdfminer.six")
                        return []
                    raise e
                except Exception as e:
                    logger.error(f"âŒ PDFåŠ è½½å¤±è´¥ {file_path}: {e}")
                    return []
                    
            elif file_path.suffix.lower() == '.pptx':
                try:
                    loader = UnstructuredPowerPointLoader(str(file_path))
                    documents = loader.load()
                    return documents
                except Exception as e:
                    logger.error(f"âŒ PowerPointåŠ è½½å¤±è´¥ {file_path}: {e}")
                    return []
                
            elif file_path.suffix.lower() in ['.md', '.txt']:
                content = file_path.read_text(encoding='utf-8')
                return [Document(
                    page_content=content,
                    metadata={"source": str(file_path)}
                )]
            else:
                logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path.suffix}")
                return []
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ–‡æ¡£å¤±è´¥ {file_path}: {e}")
            return []
    
    async def _save_record_to_mongodb(self, record: DocumentRecord):
        """ä¿å­˜è®°å½•åˆ°MongoDB"""
        try:
            record_dict = record.to_dict()
            record_dict["updated_at"] = datetime.utcnow()
            
            await self.documents_collection.update_one(
                {"file_path": record.file_path},
                {"$set": record_dict},
                upsert=True
            )
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜MongoDBè®°å½•å¤±è´¥: {e}")
            raise
    
    async def auto_update_knowledge_base(self) -> Dict[str, Any]:
        """è‡ªåŠ¨æ›´æ–°çŸ¥è¯†åº“çš„ä¸»è¦æ–¹æ³•"""
        logger.info("ğŸš€ å¯åŠ¨å¢é‡çŸ¥è¯†åº“æ›´æ–°...")
        start_time = time.time()
        
        try:
            # åˆå§‹åŒ–
            await self.initialize()
            
            # æ‰«ææ–‡ä»¶ç³»ç»Ÿ
            current_files = await self.scan_file_system()
            
            # è·å–MongoDBè®°å½•
            mongo_records = await self.get_mongodb_records()
            
            # æ£€æµ‹å˜åŒ–
            changes = await self.compare_and_detect_changes(current_files, mongo_records)
            
            # å¤„ç†å˜åŒ–
            await self.process_deleted_files(changes["deleted_files"])
            await self.process_modified_files(changes["modified_files"])
            await self.process_new_files(changes["new_files"])
            
            duration = time.time() - start_time
            
            result = {
                "status": "success",
                "message": "çŸ¥è¯†åº“æ›´æ–°å®Œæˆ",
                "duration": duration,
                "changes": {
                    "new_files": len(changes["new_files"]),
                    "modified_files": len(changes["modified_files"]),
                    "deleted_files": len(changes["deleted_files"]),
                    "unchanged_files": len(changes["unchanged_files"])
                },
                "total_processed": len(changes["new_files"]) + len(changes["modified_files"]),
                "categories": list(self.supported_categories)
            }
            
            logger.info(f"âœ… çŸ¥è¯†åº“æ›´æ–°å®Œæˆï¼Œè€—æ—¶ {duration:.2f} ç§’")
            return result
            
        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†åº“æ›´æ–°å¤±è´¥: {e}")
            return {
                "status": "error",
                "message": f"çŸ¥è¯†åº“æ›´æ–°å¤±è´¥: {str(e)}",
                "duration": time.time() - start_time
            }
    
    async def close(self):
        """å…³é—­è¿æ¥"""
        if self.mongo_client:
            self.mongo_client.close()
        logger.info("âœ… è¿æ¥å·²å…³é—­")


# å…¨å±€å‡½æ•°
async def ensure_knowledge_base_ready() -> bool:
    """ç¡®ä¿çŸ¥è¯†åº“å°±ç»ª"""
    manager = IncrementalKnowledgeBaseManager()
    try:
        result = await manager.auto_update_knowledge_base()
        return result["status"] == "success"
    finally:
        await manager.close()


# å‘åå…¼å®¹çš„ç±»å
class AutoKnowledgeBaseSetup:
    """å‘åå…¼å®¹çš„ç±»å"""
    
    def __init__(self):
        self.manager = IncrementalKnowledgeBaseManager()
    
    async def auto_initialize_if_needed(self):
        """å‘åå…¼å®¹çš„æ–¹æ³•"""
        return await self.manager.auto_update_knowledge_base()
    
    async def setup_documents(self):
        """å‘åå…¼å®¹çš„æ–¹æ³•"""
        result = await self.manager.auto_update_knowledge_base()
        return result.get("total_processed", 0)


if __name__ == "__main__":
    async def main():
        """æµ‹è¯•å¢é‡æ›´æ–°åŠŸèƒ½"""
        manager = IncrementalKnowledgeBaseManager()
        try:
            result = await manager.auto_update_knowledge_base()
            print(f"æ›´æ–°ç»“æœ: {result}")
        finally:
            await manager.close()
    
    asyncio.run(main())
