#!/usr/bin/env python3
"""
è‡ªåŠ¨ç”Ÿæˆçš„çŸ¥è¯†åº“è®¾ç½®è„šæœ¬
ç”Ÿæˆæ—¶é—´: 2025-06-26 12:08:14
"""

import asyncio
from pathlib import Path
from langchain_community.document_loaders import (
    PyPDFLoader,
    UnstructuredPowerPointLoader
)
from langchain_text_splitters import RecursiveCharacterTextSplitter

class AutoKnowledgeBaseSetup:
    def __init__(self):
        self.base_dir = Path("knowledge_base")
        self.processing_order = [{'category': 'nesma', 'file': {'filename': 'NESMA_FPA_Method_v2.3.pdf', 'size': '0.5MB', 'metadata': {'title': 'NESMAåŠŸèƒ½ç‚¹åˆ†ææ–¹æ³•v2.3', 'type': 'official_standard', 'language': 'è‹±æ–‡', 'priority': 'high', 'description': 'NESMAå®˜æ–¹åŠŸèƒ½ç‚¹ä¼°ç®—æ ‡å‡†æ–‡æ¡£'}}, 'processing_order': 1}, {'category': 'cosmic', 'file': {'filename': 'COSMICåº¦é‡æ‰‹å†ŒV5.0-part-1-åŸåˆ™ã€å®šä¹‰ä¸è§„åˆ™.pdf', 'size': '0.3MB', 'metadata': {'title': 'COSMICåº¦é‡æ‰‹å†Œ-åŸåˆ™ä¸è§„åˆ™', 'type': 'official_standard', 'language': 'ä¸­æ–‡', 'priority': 'high', 'description': 'COSMIC v5.0æ ¸å¿ƒç†è®ºå’Œå®šä¹‰'}}, 'processing_order': 1}, {'category': 'cosmic', 'file': {'filename': 'COSMICåº¦é‡æ‰‹å†ŒV5.0-part-2-æŒ‡å—.pdf', 'size': '0.4MB', 'metadata': {'title': 'COSMICåº¦é‡æ‰‹å†Œ-å®æ–½æŒ‡å—', 'type': 'implementation_guide', 'language': 'ä¸­æ–‡', 'priority': 'high', 'description': 'COSMIC v5.0å®æ–½æ“ä½œæŒ‡å—'}}, 'processing_order': 1}, {'category': 'cosmic', 'file': {'filename': 'COSMICåº¦é‡æ‰‹å†ŒV5.0-part-3-æ¡ˆä¾‹.pdf', 'size': '0.7MB', 'metadata': {'title': 'COSMICåº¦é‡æ‰‹å†Œ-æ¡ˆä¾‹é›†', 'type': 'case_studies', 'language': 'ä¸­æ–‡', 'priority': 'medium', 'description': 'COSMIC v5.0å®é™…åº”ç”¨æ¡ˆä¾‹'}}, 'processing_order': 2}, {'category': 'cosmic', 'file': {'filename': 'COSMICæ—©æœŸè½¯ä»¶è§„æ¨¡åº¦é‡æŒ‡å—-å®è·µçº§-Early-Software-Sizingï¼ˆPractitioners.pdf', 'size': '0.7MB', 'metadata': {'title': 'COSMICæ—©æœŸä¼°ç®—-å®è·µçº§', 'type': 'early_sizing_guide', 'language': 'ä¸­æ–‡', 'priority': 'medium', 'description': 'æ—©æœŸé˜¶æ®µåŠŸèƒ½ç‚¹ä¼°ç®—å®è·µæŒ‡å—'}}, 'processing_order': 2}, {'category': 'cosmic', 'file': {'filename': 'COSMICæ—©æœŸè½¯ä»¶è§„æ¨¡åº¦é‡æŒ‡å—-â€“-ä¸“å®¶çº§V2-Early-Software-Sizingï¼ˆExperts.pdf', 'size': '1.3MB', 'metadata': {'title': 'COSMICæ—©æœŸä¼°ç®—-ä¸“å®¶çº§', 'type': 'advanced_guide', 'language': 'ä¸­æ–‡', 'priority': 'medium', 'description': 'é«˜çº§æ—©æœŸåŠŸèƒ½ç‚¹ä¼°ç®—æŒ‡å—'}}, 'processing_order': 2}, {'category': 'common', 'file': {'filename': 'NESMA_FPA_Method_v2.3.pdf', 'size': '0.5MB', 'metadata': {'title': 'NESMAå‚è€ƒæ–‡æ¡£', 'type': 'reference', 'language': 'è‹±æ–‡', 'priority': 'medium', 'description': 'é€šç”¨NESMAå‚è€ƒæ–‡æ¡£'}}, 'processing_order': 2}, {'category': 'common', 'file': {'filename': 'å·¥ä½œé‡æ‹†åˆ†è®²è§£V2.pptx', 'size': '11.3MB', 'metadata': {'title': 'å·¥ä½œé‡æ‹†åˆ†åŸ¹è®­', 'type': 'training_material', 'language': 'ä¸­æ–‡', 'priority': 'medium', 'description': 'åŠŸèƒ½ç‚¹å·¥ä½œé‡æ‹†åˆ†åŸ¹è®­ææ–™'}}, 'processing_order': 2}]
    
    async def setup_documents(self):
        """æŒ‰è®¡åˆ’å¤„ç†æ–‡æ¡£"""
        
        # ä¸­è‹±æ–‡ä¼˜åŒ–çš„åˆ†è¯å™¨
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", "ã€‚", ".", "!", "?", "ï¼›", ";"]
        )
        
        processed_docs = []
        
        for item in self.processing_order:
            category = item["category"]
            file_info = item["file"]
            
            file_path = self.base_dir / "documents" / category / file_info["filename"]
            
            if not file_path.exists():
                continue
            
            print(f"ğŸ”„ å¤„ç†æ–‡æ¡£: {file_info['metadata']['title']}")
            
            # é€‰æ‹©åˆé€‚çš„åŠ è½½å™¨
            if file_path.suffix.lower() == '.pdf':
                # ä½¿ç”¨PyPDFLoaderï¼Œæ›´ç®€å•å¯é ï¼Œæ— éœ€é¢å¤–ä¾èµ–
                loader = PyPDFLoader(str(file_path))
            elif file_path.suffix.lower() == '.pptx':
                loader = UnstructuredPowerPointLoader(str(file_path))
            else:
                continue
            
            # åŠ è½½å’Œåˆ†å—
            docs = await loader.aload()
            split_docs = text_splitter.split_documents(docs)
            
            # æ·»åŠ å…ƒæ•°æ®
            for doc in split_docs:
                doc.metadata.update({
                    "source_category": category,
                    "document_type": file_info["metadata"]["type"],
                    "language": file_info["metadata"]["language"],
                    "priority": file_info["metadata"]["priority"],
                    "title": file_info["metadata"]["title"]
                })
            
            processed_docs.extend(split_docs)
            print(f"âœ… å®Œæˆ: {len(split_docs)} ä¸ªæ–‡æ¡£å—")
        
        return processed_docs

if __name__ == "__main__":
    setup = AutoKnowledgeBaseSetup()
    asyncio.run(setup.setup_documents())
