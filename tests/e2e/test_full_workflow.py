"""
é‡å­æ™ºèƒ½åŒ–åŠŸèƒ½ç‚¹ä¼°ç®—ç³»ç»Ÿ - ç«¯åˆ°ç«¯æµ‹è¯•

æµ‹è¯•å®Œæ•´çš„å·¥ä½œæµç¨‹ä»è¾“å…¥åˆ°è¾“å‡ºçš„æ•´ä¸ªè¿‡ç¨‹
"""

import pytest
import asyncio
import time
import traceback
from unittest.mock import Mock, AsyncMock, patch, MagicMock
from typing import Dict, Any, List
import psutil
import os

# å·¥ä½œæµå’Œæ™ºèƒ½ä½“å¯¼å…¥
from graph.workflow_graph import FPEstimationWorkflow
from models.project_models import (
    ProjectInfo, TechnologyStack, BusinessDomain, 
    EstimationStrategy, EstimationStandard
)
from models.nesma_models import NESMAFunctionType, NESMAComplexityLevel
from models.cosmic_models import COSMICDataMovementType
from models.common_models import ConfidenceLevel


class TestFullWorkflow:
    """å®Œæ•´å·¥ä½œæµæµ‹è¯•"""
    
    @pytest.fixture
    def workflow_instance(self):
        """åˆ›å»ºå·¥ä½œæµå®ä¾‹"""
        workflow = FPEstimationWorkflow()
        return workflow
    
    @pytest.fixture
    def sample_projects(self):
        """æ ·æœ¬é¡¹ç›®æ•°æ®"""
        return {
            "small": ProjectInfo(
                name="å°å‹CRMç³»ç»Ÿ",
                description="å®¢æˆ·å…³ç³»ç®¡ç†ç³»ç»Ÿï¼ŒåŒ…å«å®¢æˆ·ä¿¡æ¯ç®¡ç†ã€é”€å”®è·Ÿè¸ªã€åˆåŒç®¡ç†ç­‰åŸºç¡€åŠŸèƒ½æ¨¡å—ï¼Œæ”¯æŒç”¨æˆ·æ³¨å†Œç™»å½•å’ŒåŸºæœ¬çš„æ•°æ®ç»´æŠ¤æ“ä½œ",
                technology_stack=[TechnologyStack.JAVA, TechnologyStack.MYSQL],
                business_domain=BusinessDomain.RETAIL
            ),
            "medium": ProjectInfo(
                name="ä¸­å‹ç”µå•†å¹³å°",
                description="å®Œæ•´çš„ç”µå•†å¹³å°ç³»ç»Ÿï¼ŒåŒ…å«ç”¨æˆ·ç®¡ç†ã€å•†å“ç®¡ç†ã€è®¢å•å¤„ç†ã€æ”¯ä»˜ç³»ç»Ÿã€åº“å­˜ç®¡ç†ã€ç‰©æµè·Ÿè¸ªã€å®¢æœç³»ç»Ÿç­‰æ ¸å¿ƒåŠŸèƒ½æ¨¡å—ï¼Œæ”¯æŒå¤šç”¨æˆ·è§’è‰²å’Œå¤æ‚çš„ä¸šåŠ¡æµç¨‹å¤„ç†",
                technology_stack=[TechnologyStack.JAVA, TechnologyStack.MYSQL, TechnologyStack.REDIS],
                business_domain=BusinessDomain.ECOMMERCE
            ),
            "large": ProjectInfo(
                name="å¤§å‹ä¼ä¸šç®¡ç†ç³»ç»Ÿ",
                description="å¤§å‹ä¼ä¸šèµ„æºè§„åˆ’ç³»ç»Ÿï¼ŒåŒ…å«äººåŠ›èµ„æºç®¡ç†ã€è´¢åŠ¡ç®¡ç†ã€ä¾›åº”é“¾ç®¡ç†ã€ç”Ÿäº§è®¡åˆ’ã€é¡¹ç›®ç®¡ç†ã€å®¢æˆ·æœåŠ¡ã€æ•°æ®åˆ†æç­‰å¤šä¸ªå¤æ‚ä¸šåŠ¡æ¨¡å—ï¼Œæ”¯æŒå¤šç»„ç»‡æ¶æ„ã€å·¥ä½œæµå®¡æ‰¹ã€æƒé™æ§åˆ¶ç­‰é«˜çº§åŠŸèƒ½ç‰¹æ€§",
                technology_stack=[TechnologyStack.JAVA, TechnologyStack.PYTHON, TechnologyStack.MYSQL, TechnologyStack.REDIS],
                business_domain=BusinessDomain.MANUFACTURING
            )
        }
    
    def _create_nesma_mock_responses(self):
        """åˆ›å»ºNESMAç›¸å…³çš„mockå“åº”"""
        return {
            "standard_recommendation": {
                "recommended_standard": "NESMA",
                "confidence_score": 0.85,
                "reasoning": "ä¼ ç»Ÿä¼ä¸šåº”ç”¨é€‚åˆNESMAæ ‡å‡†",
                "alternative_standards": ["COSMIC"]
            },
            "requirement_parsing": {
                "functional_modules": [
                    {"name": "ç”¨æˆ·ç®¡ç†", "description": "ç”¨æˆ·æ³¨å†Œç™»å½•"},
                    {"name": "å•†å“ç®¡ç†", "description": "å•†å“ä¿¡æ¯ç»´æŠ¤"}
                ],
                "business_entities": {"ç”¨æˆ·è§’è‰²": ["ç®¡ç†å‘˜", "æ™®é€šç”¨æˆ·"]},
                "business_processes": []
            },
            "process_identification": [
                {
                    "id": "proc_1",
                    "name": "ç”¨æˆ·æ³¨å†Œ",
                    "description": "ç”¨æˆ·æ³¨å†Œæµç¨‹",
                    "data_groups": ["ç”¨æˆ·ä¿¡æ¯"],
                    "dependencies": []
                }
            ],
            "nesma_classification": {
                "function_type": NESMAFunctionType.EI,
                "confidence_score": 0.9,
                "justification": "æ•°æ®è¾“å…¥åŠŸèƒ½"
            },
            "nesma_complexity": {
                "complexity": NESMAComplexityLevel.AVERAGE,
                "det_count": 8,
                "ret_count": 2
            },
            "nesma_ufp": {
                "total_ufp": 42,
                "function_breakdown": []
            }
        }
    
    def _create_cosmic_mock_responses(self):
        """åˆ›å»ºCOSMICç›¸å…³çš„mockå“åº”"""
        return {
            "cosmic_functional_users": [
                {
                    "id": "user_1",
                    "name": "ç³»ç»Ÿç”¨æˆ·",
                    "description": "ä½¿ç”¨ç³»ç»Ÿçš„ç”¨æˆ·",
                    "identification_confidence": 0.9
                }
            ],
            "cosmic_data_movements": [
                {
                    "type": COSMICDataMovementType.ENTRY,
                    "source": "ç”¨æˆ·ç•Œé¢",
                    "target": "åº”ç”¨å±‚",
                    "data_group": "ç”¨æˆ·ä¿¡æ¯",
                    "confidence_score": 0.85
                }
            ],
            "cosmic_cfp": {
                "total_cfp": 15,
                "movement_breakdown": []
            }
        }
    
    @pytest.mark.asyncio
    async def test_nesma_only_workflow(self, workflow_instance, sample_projects):
        """æµ‹è¯•çº¯NESMAä¼°ç®—å·¥ä½œæµ"""
        project = sample_projects["medium"]
        
        # åˆå§‹åŒ–å·¥ä½œæµ
        session_id = await workflow_instance.initialize(
            project_info=project,
            strategy=EstimationStrategy.NESMA_ONLY,
            requirements="æµ‹è¯•NESMAä¼°ç®—å·¥ä½œæµ"
        )
        
        # Mockå„ä¸ªæ™ºèƒ½ä½“çš„è¿”å›ç»“æœ
        mock_responses = self._create_nesma_mock_responses()
        
        with patch('agents.standards.nesma.function_classifier.NESMAFunctionClassifierAgent.execute', new_callable=AsyncMock) as mock_classifier, \
             patch('agents.standards.nesma.complexity_calculator.NESMAComplexityCalculatorAgent.execute', new_callable=AsyncMock) as mock_complexity, \
             patch('agents.standards.nesma.ufp_calculator.NESMAUFPCalculatorAgent.execute', new_callable=AsyncMock) as mock_ufp, \
             patch('agents.standards.standard_recommender.StandardRecommenderAgent.recommend_standards', new_callable=AsyncMock) as mock_recommender, \
             patch('agents.analysis.requirement_parser.RequirementParserAgent.parse_requirements', new_callable=AsyncMock) as mock_parser, \
             patch('agents.analysis.process_identifier.ProcessIdentifierAgent.identify_processes', new_callable=AsyncMock) as mock_identifier:
            
            # é…ç½®mockè¿”å›å€¼
            mock_recommender.return_value = mock_responses["standard_recommendation"]
            mock_parser.return_value = mock_responses["requirement_parsing"]
            mock_identifier.return_value = mock_responses["process_identification"]
            mock_classifier.return_value = mock_responses["nesma_classification"]
            mock_complexity.return_value = mock_responses["nesma_complexity"]
            mock_ufp.return_value = mock_responses["nesma_ufp"]
            
            # æ‰§è¡Œå·¥ä½œæµ
            result = await workflow_instance.execute()
            
            # éªŒè¯ç»“æœ
            assert result is not None
            assert session_id == workflow_instance.get_session_id()
            
            # éªŒè¯è°ƒç”¨æ¬¡æ•°
            assert mock_recommender.called
            assert mock_parser.called
    
    @pytest.mark.asyncio
    async def test_cosmic_only_workflow(self, workflow_instance, sample_projects):
        """æµ‹è¯•çº¯COSMICä¼°ç®—å·¥ä½œæµ"""
        project = sample_projects["medium"]
        
        # åˆå§‹åŒ–å·¥ä½œæµ
        session_id = await workflow_instance.initialize(
            project_info=project,
            strategy=EstimationStrategy.COSMIC_ONLY,
            requirements="æµ‹è¯•COSMICä¼°ç®—å·¥ä½œæµ"
        )
        
        # Mockå„ä¸ªæ™ºèƒ½ä½“çš„è¿”å›ç»“æœ
        mock_responses = self._create_cosmic_mock_responses()
        
        with patch('agents.standards.cosmic.functional_user_agent.COSMICFunctionalUserAgent.execute', new_callable=AsyncMock) as mock_functional_user, \
             patch('agents.standards.cosmic.boundary_analyzer.COSMICBoundaryAnalyzerAgent.execute', new_callable=AsyncMock) as mock_boundary, \
             patch('agents.standards.cosmic.data_movement_classifier.COSMICDataMovementClassifierAgent.execute', new_callable=AsyncMock) as mock_data_movement, \
             patch('agents.standards.cosmic.cfp_calculator.COSMICCFPCalculatorAgent.execute', new_callable=AsyncMock) as mock_cfp, \
             patch('agents.standards.standard_recommender.StandardRecommenderAgent.recommend_standards', new_callable=AsyncMock) as mock_recommender, \
             patch('agents.analysis.requirement_parser.RequirementParserAgent.parse_requirements', new_callable=AsyncMock) as mock_parser, \
             patch('agents.analysis.process_identifier.ProcessIdentifierAgent.identify_processes', new_callable=AsyncMock) as mock_identifier:
            
            # é…ç½®mockè¿”å›å€¼
            mock_recommender.return_value = {
                "recommended_standard": "COSMIC",
                "confidence_score": 0.9,
                "reasoning": "ç°ä»£åº”ç”¨é€‚åˆCOSMICæ ‡å‡†",
                "alternative_standards": ["NESMA"]
            }
            mock_parser.return_value = {
                "functional_modules": [{"name": "APIæœåŠ¡", "description": "RESTful API"}],
                "business_entities": {"åŠŸèƒ½ç”¨æˆ·": ["å®¢æˆ·ç«¯åº”ç”¨"]},
                "business_processes": []
            }
            mock_identifier.return_value = [
                {
                    "id": "proc_1", 
                    "name": "æ•°æ®å¤„ç†",
                    "description": "æ•°æ®å¤„ç†æµç¨‹",
                    "data_groups": ["ä¸šåŠ¡æ•°æ®"],
                    "dependencies": []
                }
            ]
            mock_functional_user.return_value = mock_responses["cosmic_functional_users"]
            mock_boundary.return_value = {"boundary_analysis": "completed"}
            mock_data_movement.return_value = mock_responses["cosmic_data_movements"]
            mock_cfp.return_value = mock_responses["cosmic_cfp"]
            
            # æ‰§è¡Œå·¥ä½œæµ
            result = await workflow_instance.execute()
            
            # éªŒè¯ç»“æœ
            assert result is not None
            assert session_id == workflow_instance.get_session_id()
            
            # éªŒè¯è°ƒç”¨æ¬¡æ•°
            assert mock_recommender.called
            assert mock_functional_user.called
    
    @pytest.mark.asyncio
    async def test_dual_standard_workflow(self, workflow_instance, sample_projects):
        """æµ‹è¯•åŒæ ‡å‡†å¯¹æ¯”ä¼°ç®—å·¥ä½œæµ"""
        project = sample_projects["large"]
        
        # åˆå§‹åŒ–å·¥ä½œæµ
        session_id = await workflow_instance.initialize(
            project_info=project,
            strategy=EstimationStrategy.DUAL_PARALLEL,
            requirements="æµ‹è¯•åŒæ ‡å‡†å¯¹æ¯”ä¼°ç®—å·¥ä½œæµ"
        )
        
        # Mockä¸¤å¥—æ ‡å‡†çš„è¿”å›ç»“æœ
        nesma_mock = self._create_nesma_mock_responses()
        cosmic_mock = self._create_cosmic_mock_responses()
        
        with patch('agents.standards.nesma.function_classifier.NESMAFunctionClassifierAgent.execute', new_callable=AsyncMock) as mock_nesma_classifier, \
             patch('agents.standards.nesma.complexity_calculator.NESMAComplexityCalculatorAgent.execute', new_callable=AsyncMock) as mock_nesma_complexity, \
             patch('agents.standards.nesma.ufp_calculator.NESMAUFPCalculatorAgent.execute', new_callable=AsyncMock) as mock_nesma_ufp, \
             patch('agents.standards.cosmic.functional_user_agent.COSMICFunctionalUserAgent.execute', new_callable=AsyncMock) as mock_cosmic_functional_user, \
             patch('agents.standards.cosmic.boundary_analyzer.COSMICBoundaryAnalyzerAgent.execute', new_callable=AsyncMock) as mock_cosmic_boundary, \
             patch('agents.standards.cosmic.data_movement_classifier.COSMICDataMovementClassifierAgent.execute', new_callable=AsyncMock) as mock_cosmic_data_movement, \
             patch('agents.standards.cosmic.cfp_calculator.COSMICCFPCalculatorAgent.execute', new_callable=AsyncMock) as mock_cosmic_cfp, \
             patch('agents.standards.standard_recommender.StandardRecommenderAgent.recommend_standards', new_callable=AsyncMock) as mock_recommender, \
             patch('agents.analysis.requirement_parser.RequirementParserAgent.parse_requirements', new_callable=AsyncMock) as mock_parser, \
             patch('agents.analysis.process_identifier.ProcessIdentifierAgent.identify_processes', new_callable=AsyncMock) as mock_identifier:
            
            # é…ç½®mockè¿”å›å€¼
            mock_recommender.return_value = {
                "recommended_standard": "NESMA+COSMIC",
                "confidence_score": 0.85,
                "reasoning": "å¤æ‚é¡¹ç›®å»ºè®®åŒæ ‡å‡†å¯¹æ¯”",
                "alternative_standards": ["NESMA", "COSMIC"]
            }
            mock_parser.return_value = nesma_mock["requirement_parsing"]
            mock_identifier.return_value = nesma_mock["process_identification"]
            
            # NESMA mocké…ç½®
            mock_nesma_classifier.return_value = nesma_mock["nesma_classification"]
            mock_nesma_complexity.return_value = nesma_mock["nesma_complexity"]
            mock_nesma_ufp.return_value = nesma_mock["nesma_ufp"]
            
            # COSMIC mocké…ç½®
            mock_cosmic_functional_user.return_value = cosmic_mock["cosmic_functional_users"]
            mock_cosmic_boundary.return_value = {"boundary_analysis": "completed"}
            mock_cosmic_data_movement.return_value = cosmic_mock["cosmic_data_movements"]
            mock_cosmic_cfp.return_value = cosmic_mock["cosmic_cfp"]
            
            # æ‰§è¡Œå·¥ä½œæµ
            result = await workflow_instance.execute()
            
            # éªŒè¯ç»“æœ
            assert result is not None
            assert session_id == workflow_instance.get_session_id()
            
            # éªŒè¯è°ƒç”¨æ¬¡æ•°
            assert mock_recommender.called
            assert mock_nesma_classifier.called
            assert mock_cosmic_functional_user.called
    
    @pytest.mark.asyncio
    async def test_error_handling_and_retry(self, workflow_instance, sample_projects):
        """æµ‹è¯•é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶"""
        project = sample_projects["small"]
        
        # åˆå§‹åŒ–å·¥ä½œæµ
        session_id = await workflow_instance.initialize(
            project_info=project,
            strategy=EstimationStrategy.NESMA_ONLY,
            requirements="æµ‹è¯•é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶"
        )
        
        # æ¨¡æ‹Ÿç¬¬ä¸€æ¬¡å¤±è´¥ï¼Œç¬¬äºŒæ¬¡æˆåŠŸçš„åœºæ™¯
        call_count = 0
        
        def failing_then_success(*args, **kwargs):
            nonlocal call_count
            call_count += 1
            if call_count == 1:
                raise Exception("æ¨¡æ‹ŸLLMè°ƒç”¨å¤±è´¥")
            return {
                "function_type": "EI",
                "confidence": 0.9,
                "reasoning": "é‡è¯•åæˆåŠŸ"
            }
        
        with patch('agents.standards.nesma.function_classifier.NESMAFunctionClassifierAgent.execute', 
                   side_effect=failing_then_success):
            
            # æ‰§è¡Œå·¥ä½œæµï¼ˆåº”è¯¥ä¼šè‡ªåŠ¨é‡è¯•ï¼‰
            try:
                result = await workflow_instance.execute()
                # éªŒè¯é‡è¯•æœºåˆ¶ç”Ÿæ•ˆ
                assert call_count >= 2  # è‡³å°‘è°ƒç”¨äº†2æ¬¡ï¼ˆç¬¬ä¸€æ¬¡å¤±è´¥ï¼Œç¬¬äºŒæ¬¡æˆåŠŸï¼‰
            except Exception:
                # å¦‚æœé‡è¯•ä»ç„¶å¤±è´¥ï¼Œè¿™ä¹Ÿæ˜¯å¯æ¥å—çš„
                assert call_count >= 1
    
    @pytest.mark.asyncio
    async def test_performance_benchmark(self, workflow_instance, sample_projects):
        """æµ‹è¯•æ€§èƒ½åŸºå‡†"""
        performance_results = {}
        
        for project_size, project in sample_projects.items():
            # æ‰§è¡Œå¤šæ¬¡å–å¹³å‡å€¼
            execution_times = []
            memory_usages = []
            
            for run in range(2):  # è¿è¡Œ2æ¬¡å‡å°‘æµ‹è¯•æ—¶é—´
                # åˆå§‹åŒ–å·¥ä½œæµ
                session_id = await workflow_instance.initialize(
                    project_info=project,
                    strategy=EstimationStrategy.NESMA_ONLY,
                    requirements=f"æ€§èƒ½æµ‹è¯• - {project_size} é¡¹ç›® - ç¬¬{run+1}æ¬¡"
                )
                
                # Mockç®€åŒ–çš„å“åº”ä»¥ç¡®ä¿ç¨³å®šæ€§èƒ½
                with patch.multiple(
                    'agents.standards.nesma.function_classifier.NESMAFunctionClassifierAgent',
                    execute=AsyncMock(return_value={"function_type": "EI", "confidence": 0.9}),
                ), patch.multiple(
                    'agents.standards.nesma.ufp_calculator.NESMAUFPCalculatorAgent',
                    execute=AsyncMock(return_value={"total_ufp": 25, "function_breakdown": []}),
                ):
                    
                    # è®°å½•å¼€å§‹æ—¶é—´å’Œå†…å­˜
                    start_time = time.time()
                    start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
                    
                    # æ‰§è¡Œå·¥ä½œæµ
                    await workflow_instance.execute()
                    
                    # è®°å½•ç»“æŸæ—¶é—´å’Œå†…å­˜
                    end_time = time.time()
                    end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
                    
                    execution_times.append(end_time - start_time)
                    memory_usages.append(end_memory - start_memory)
            
            # è®¡ç®—å¹³å‡å€¼
            avg_time = sum(execution_times) / len(execution_times)
            avg_memory = sum(memory_usages) / len(memory_usages)
            
            performance_results[project_size] = {
                "average_execution_time": avg_time,
                "average_memory_usage": avg_memory,
                "execution_times": execution_times,
                "memory_usages": memory_usages
            }
        
        # æ€§èƒ½æ–­è¨€
        assert performance_results["small"]["average_execution_time"] < 10.0  # å°é¡¹ç›®10ç§’å†…
        assert performance_results["medium"]["average_execution_time"] < 15.0  # ä¸­é¡¹ç›®15ç§’å†…
        assert performance_results["large"]["average_execution_time"] < 30.0  # å¤§é¡¹ç›®30ç§’å†…
        
        # å†…å­˜ä½¿ç”¨åº”è¯¥åˆç†ï¼ˆå°äº200MBå¢é•¿ï¼‰
        for size, results in performance_results.items():
            assert abs(results["average_memory_usage"]) < 200.0
        
        print(f"\nğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ:")
        for size, results in performance_results.items():
            print(f"  {size}: å¹³å‡è€—æ—¶ {results['average_execution_time']:.2f}s, "
                  f"å¹³å‡å†…å­˜å˜åŒ– {results['average_memory_usage']:.2f}MB")
    
    @pytest.mark.asyncio
    async def test_concurrent_execution(self, workflow_instance, sample_projects):
        """æµ‹è¯•å¹¶å‘æ‰§è¡Œèƒ½åŠ›"""
        # å‡†å¤‡å¤šä¸ªå¹¶å‘ä»»åŠ¡
        tasks = []
        for i in range(3):
            project = sample_projects["small"]
            
            # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„å·¥ä½œæµå®ä¾‹
            workflow = FPEstimationWorkflow()
            session_id = await workflow.initialize(
                project_info=project,
                strategy=EstimationStrategy.NESMA_ONLY,
                requirements=f"å¹¶å‘æµ‹è¯•ä»»åŠ¡ {i+1}"
            )
            
            # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„mock
            with patch('agents.standards.nesma.function_classifier.NESMAFunctionClassifierAgent.execute', 
                       new_callable=AsyncMock) as mock_classifier:
                mock_classifier.return_value = {"function_type": "EI", "confidence": 0.9}
                
                task = workflow.execute()
                tasks.append(task)
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        start_time = time.time()
        results = await asyncio.gather(*tasks, return_exceptions=True)
        end_time = time.time()
        
        # éªŒè¯ç»“æœ
        assert len(results) == 3
        successful_results = [r for r in results if not isinstance(r, Exception)]
        assert len(successful_results) >= 1  # è‡³å°‘1ä¸ªæˆåŠŸ
        
        # å¹¶å‘æ‰§è¡Œåº”è¯¥æ¯”ä¸²è¡Œæ‰§è¡Œå¿«
        concurrent_time = end_time - start_time
        print(f"\nâš¡ å¹¶å‘æ‰§è¡Œè€—æ—¶: {concurrent_time:.2f}s")
        assert concurrent_time < 30.0  # 3ä¸ªä»»åŠ¡å¹¶å‘æ‰§è¡Œåº”åœ¨30ç§’å†…å®Œæˆ


class TestConcurrentWorkflows:
    """å¹¶å‘å·¥ä½œæµæµ‹è¯•"""
    
    @pytest.mark.asyncio
    async def test_multiple_concurrent_estimations(self):
        """æµ‹è¯•å¤šä¸ªå¹¶å‘ä¼°ç®—"""
        # åˆ›å»ºå¤šä¸ªå·¥ä½œæµå®ä¾‹å¹¶å‘æ‰§è¡Œ
        workflows = [FPEstimationWorkflow() for _ in range(3)]
        sample_project = ProjectInfo(
            name="æµ‹è¯•é¡¹ç›®",
            description="å¹¶å‘æµ‹è¯•é¡¹ç›®çš„è¯¦ç»†æè¿°ä¿¡æ¯ï¼Œç”¨äºéªŒè¯ç³»ç»Ÿçš„å¹¶å‘å¤„ç†èƒ½åŠ›å’Œç¨³å®šæ€§è¡¨ç°",
            technology_stack=[TechnologyStack.PYTHON],
            business_domain=BusinessDomain.OTHER
        )
        
        # å‡†å¤‡å¹¶å‘ä»»åŠ¡
        tasks = []
        for i, workflow in enumerate(workflows):
            session_id = await workflow.initialize(
                project_info=sample_project,
                strategy=EstimationStrategy.NESMA_ONLY,
                requirements=f"å¹¶å‘ä¼°ç®—æµ‹è¯• {i+1}"
            )
            
            with patch('agents.standards.nesma.function_classifier.NESMAFunctionClassifierAgent.execute',
                       new_callable=AsyncMock) as mock_classifier:
                mock_classifier.return_value = {"function_type": "EI", "confidence": 0.8}
                
                task = workflow.execute()
                tasks.append(task)
        
        # æ‰§è¡Œå¹¶å‘æµ‹è¯•
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # éªŒè¯ç»“æœ
        assert len(results) == 3
        successful_results = [r for r in results if not isinstance(r, Exception)]
        assert len(successful_results) >= 1  # è‡³å°‘1ä¸ªæˆåŠŸ


@pytest.mark.asyncio
async def test_full_system_integration():
    """å®Œæ•´ç³»ç»Ÿé›†æˆæµ‹è¯•"""
    # è¿™æ˜¯ä¸€ä¸ªç»¼åˆæ€§æµ‹è¯•ï¼ŒéªŒè¯æ•´ä¸ªç³»ç»Ÿçš„ç«¯åˆ°ç«¯åŠŸèƒ½
    print("\nğŸš€ å¼€å§‹å®Œæ•´ç³»ç»Ÿé›†æˆæµ‹è¯•...")
    
    # æµ‹è¯•é¡¹ç›®
    test_project = ProjectInfo(
        name="ç»¼åˆæµ‹è¯•ç”µå•†å¹³å°",
        description="""
        å®Œæ•´çš„ç”µå•†å¹³å°åŠŸèƒ½ï¼š
        1. ç”¨æˆ·ç®¡ç†ï¼šæ³¨å†Œã€ç™»å½•ã€èµ„æ–™ç®¡ç†
        2. å•†å“ç®¡ç†ï¼šå•†å“å½•å…¥ã€åˆ†ç±»ã€åº“å­˜ç®¡ç†
        3. è®¢å•ç®¡ç†ï¼šä¸‹å•ã€æ”¯ä»˜ã€å‘è´§ã€é€€è´§
        4. å®¢æœç³»ç»Ÿï¼šåœ¨çº¿å’¨è¯¢ã€å·¥å•å¤„ç†
        5. æ•°æ®åˆ†æï¼šé”€å”®ç»Ÿè®¡ã€ç”¨æˆ·è¡Œä¸ºåˆ†æ
        """,
        technology_stack=[TechnologyStack.JAVA, TechnologyStack.MYSQL, TechnologyStack.REDIS],
        business_domain=BusinessDomain.ECOMMERCE
    )
    
    # åˆ›å»ºå·¥ä½œæµå®ä¾‹
    workflow = FPEstimationWorkflow()
    
    # ä½¿ç”¨å®é™…çš„æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œå®Œæ•´æµ‹è¯•
    with patch('agents.standards.standard_recommender.StandardRecommenderAgent.recommend_standards', new_callable=AsyncMock) as mock_recommender, \
         patch('agents.analysis.requirement_parser.RequirementParserAgent.parse_requirements', new_callable=AsyncMock) as mock_parser, \
         patch('agents.analysis.process_identifier.ProcessIdentifierAgent.identify_processes', new_callable=AsyncMock) as mock_identifier, \
         patch('agents.output.report_generator.ReportGeneratorAgent.execute', new_callable=AsyncMock) as mock_report:
        
        # é…ç½®å¤æ‚çš„mockæ•°æ®
        mock_recommender.return_value = {
            "recommended_standards": [EstimationStandard.NESMA, EstimationStandard.COSMIC],
            "strategy": EstimationStrategy.DUAL_PARALLEL,
            "confidence_score": 0.9,
            "reasoning": "ç”µå•†å¹³å°é€‚åˆåŒæ ‡å‡†å¯¹æ¯”ä¼°ç®—"
        }
        
        mock_parser.return_value = {
            "functional_modules": [
                {"name": "ç”¨æˆ·ç®¡ç†", "description": "ç”¨æˆ·æ³¨å†Œç™»å½•ç®¡ç†"},
                {"name": "å•†å“ç®¡ç†", "description": "å•†å“ä¿¡æ¯ç»´æŠ¤"},
                {"name": "è®¢å•ç®¡ç†", "description": "è®¢å•å¤„ç†æµç¨‹"},
                {"name": "æ”¯ä»˜ç³»ç»Ÿ", "description": "æ”¯ä»˜å¤„ç†"},
                {"name": "å®¢æœç³»ç»Ÿ", "description": "å®¢æˆ·æœåŠ¡"}
            ],
            "business_entities": {
                "ç”¨æˆ·è§’è‰²": ["ç®¡ç†å‘˜", "å•†å®¶", "ä¹°å®¶"],
                "ä¸šåŠ¡å¯¹è±¡": ["ç”¨æˆ·", "å•†å“", "è®¢å•", "æ”¯ä»˜"],
                "åŠŸèƒ½æ“ä½œ": ["æ³¨å†Œ", "ç™»å½•", "ä¸‹å•", "æ”¯ä»˜", "é€€è´§"]
            },
            "business_processes": []
        }
        
        mock_identifier.return_value = [
            {
                "id": "proc_1",
                "name": "ç”¨æˆ·æ³¨å†Œ",
                "description": "ç”¨æˆ·æ³¨å†Œæµç¨‹",
                "data_groups": ["ç”¨æˆ·ä¿¡æ¯"],
                "dependencies": []
            },
            {
                "id": "proc_2", 
                "name": "å•†å“å‘å¸ƒ",
                "description": "å•†å“å‘å¸ƒæµç¨‹",
                "data_groups": ["å•†å“ä¿¡æ¯"],
                "dependencies": ["proc_1"]
            },
            {
                "id": "proc_3",
                "name": "è®¢å•å¤„ç†",
                "description": "è®¢å•å¤„ç†æµç¨‹", 
                "data_groups": ["è®¢å•ä¿¡æ¯"],
                "dependencies": ["proc_1", "proc_2"]
            }
        ]
        
        mock_report.return_value = {
            "report_content": "ç³»ç»Ÿé›†æˆæµ‹è¯•æŠ¥å‘Š",
            "charts": [],
            "summary": "æµ‹è¯•å®Œæˆ"
        }
        
        # åˆå§‹åŒ–å·¥ä½œæµ
        session_id = await workflow.initialize(
            project_info=test_project,
            strategy=EstimationStrategy.DUAL_PARALLEL,
            requirements="å®Œæ•´ç³»ç»Ÿé›†æˆæµ‹è¯•"
        )
        
        # æ‰§è¡Œå®Œæ•´å·¥ä½œæµ
        result = await workflow.execute()
        
        # éªŒè¯ç³»ç»Ÿé›†æˆç»“æœ
        assert result is not None
        assert session_id == workflow.get_session_id()
        print("âœ… å®Œæ•´ç³»ç»Ÿé›†æˆæµ‹è¯•é€šè¿‡!")


@pytest.mark.asyncio 
async def test_stress_testing():
    """å‹åŠ›æµ‹è¯•"""
    print("\nğŸ”¥ å¼€å§‹å‹åŠ›æµ‹è¯•...")
    
    # åˆ›å»ºå¤šä¸ªå¤æ‚é¡¹ç›®è¿›è¡Œå‹åŠ›æµ‹è¯•
    stress_projects = []
    for i in range(3):  # å‡å°‘æ•°é‡ä»¥é¿å…è¶…æ—¶
        project = ProjectInfo(
            name=f"å‹åŠ›æµ‹è¯•é¡¹ç›®{i+1}",
            description=f"å‹åŠ›æµ‹è¯•é¡¹ç›®{i+1}çš„è¯¦ç»†æè¿°ï¼ŒåŒ…å«å¤šä¸ªå¤æ‚ä¸šåŠ¡æ¨¡å—å’ŒåŠŸèƒ½éœ€æ±‚ï¼Œç”¨äºéªŒè¯ç³»ç»Ÿåœ¨é«˜è´Ÿè½½æƒ…å†µä¸‹çš„ç¨³å®šæ€§å’Œæ€§èƒ½è¡¨ç°",
            technology_stack=[TechnologyStack.JAVA, TechnologyStack.MYSQL],
            business_domain=BusinessDomain.MANUFACTURING
        )
        stress_projects.append(project)
    
    # å¹¶å‘æ‰§è¡Œå‹åŠ›æµ‹è¯•
    workflows = [FPEstimationWorkflow() for _ in range(len(stress_projects))]
    
    # å‡†å¤‡å‹åŠ›æµ‹è¯•ä»»åŠ¡
    stress_tasks = []
    for i, (workflow, project) in enumerate(zip(workflows, stress_projects)):
        session_id = await workflow.initialize(
            project_info=project,
            strategy=EstimationStrategy.NESMA_ONLY,
            requirements=f"å‹åŠ›æµ‹è¯• {i+1}"
        )
        
        task = workflow.execute()
        stress_tasks.append(task)
    
    # æ‰§è¡Œå‹åŠ›æµ‹è¯•
    start_time = time.time()
    stress_results = await asyncio.gather(*stress_tasks, return_exceptions=True)
    end_time = time.time()
    
    # åˆ†æå‹åŠ›æµ‹è¯•ç»“æœ
    successful_count = sum(1 for r in stress_results if not isinstance(r, Exception))
    failed_count = len(stress_results) - successful_count
    
    total_time = end_time - start_time
    
    print(f"ğŸ“Š å‹åŠ›æµ‹è¯•ç»“æœ:")
    print(f"  æ€»ä»»åŠ¡æ•°: {len(stress_results)}")
    print(f"  æˆåŠŸæ•°: {successful_count}")
    print(f"  å¤±è´¥æ•°: {failed_count}")
    print(f"  æ€»è€—æ—¶: {total_time:.2f}s")
    print(f"  å¹³å‡æ¯ä»»åŠ¡è€—æ—¶: {total_time/len(stress_results):.2f}s")
    
    # å‹åŠ›æµ‹è¯•éªŒè¯
    assert successful_count >= len(stress_results) * 0.5  # è‡³å°‘50%æˆåŠŸç‡
    assert total_time < 90.0  # æ€»è€—æ—¶ä¸è¶…è¿‡90ç§’
    
    print("âœ… å‹åŠ›æµ‹è¯•é€šè¿‡!")


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    pytest.main([__file__, "-v", "--tb=short"]) 